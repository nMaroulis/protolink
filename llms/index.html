
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Agent-to-agent communication framework for interoperable agent systems">
      
      
      
      
        <link rel="prev" href="../agents/">
      
      
        <link rel="next" href="../models/">
      
      
        
      
      
      <link rel="icon" href="../assets/logo_sm.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.0">
    
    
      
        <title>LLMs - protolink</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.618322db.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#llms" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="protolink" class="md-header__button md-logo" aria-label="protolink" data-md-component="logo">
      
  <img src="../assets/logo_sm.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            protolink
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              LLMs
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/nMaroulis/protolink" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    nMaroulis/protolink
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="protolink" class="md-nav__button md-logo" aria-label="protolink" data-md-component="logo">
      
  <img src="../assets/logo_sm.png" alt="logo">

    </a>
    protolink
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/nMaroulis/protolink" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    nMaroulis/protolink
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_1" >
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            
  
    Overview
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../getting-started/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Getting Started
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    API Reference
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    API Reference
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../agents/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Agents
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    LLMs
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    LLMs
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#llm-types" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLM Types
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#configuration" class="md-nav__link">
    <span class="md-ellipsis">
      
        Configuration
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Models
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../registry/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Registry
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tools/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Tools
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../transports/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Transports
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../types/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Types
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Examples
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Examples
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Examples
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#llm-types" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLM Types
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#configuration" class="md-nav__link">
    <span class="md-ellipsis">
      
        Configuration
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="llms">LLMs<a class="headerlink" href="#llms" title="Permanent link">&para;</a></h1>
<p>Protolink integrates with various LLM backends.</p>
<h2 id="llm-types">LLM Types<a class="headerlink" href="#llm-types" title="Permanent link">&para;</a></h2>
<p>Protolink groups LLM backends into three broad categories:</p>
<div align="center" style="font-family: 'Courier New', monospace; color:#888; font-size:14px; margin-bottom:12px;">
  [ API ]   [ Server ]   [ Local ]
</div>
<div align="center" style="
  display:flex;
  flex-wrap:wrap;
  justify-content:center;
  gap:30px;
">
  <img src="https://raw.githubusercontent.com/pheralb/svgl/42f8f2de1987d83a7c6ad9d5dc2576377aa5110b/static/library/openai.svg" width="55" class="hover-icon" />
  <img src="https://raw.githubusercontent.com/pheralb/svgl/42f8f2de1987d83a7c6ad9d5dc2576377aa5110b/static/library/anthropic_black.svg" width="55" class="hover-icon" />
  <img src="https://raw.githubusercontent.com/pheralb/svgl/42f8f2de1987d83a7c6ad9d5dc2576377aa5110b/static/library/gemini.svg" width="55" class="hover-icon" />
  <img src="https://raw.githubusercontent.com/pheralb/svgl/42f8f2de1987d83a7c6ad9d5dc2576377aa5110b/static/library/deepseek.svg" width="55" class="hover-icon" />
  <img src="https://raw.githubusercontent.com/pheralb/svgl/42f8f2de1987d83a7c6ad9d5dc2576377aa5110b/static/library/ollama_light.svg" width="55" class="hover-icon" />
  <img src="https://raw.githubusercontent.com/abetlen/llama-cpp-python/main/docs/icon.svg" width="55" class="hover-icon" />
</div>
<style>
  .hover-icon{ transition: transform .15s ease; }
  .hover-icon:hover{ transform: translateY(-4px) scale(1.08); }
</style>

<ul>
<li>
<p><strong>API</strong> — calls a remote API and requires an API key:</p>
<ul>
<li><code>OpenAILLM</code>: uses the <strong>OpenAI API</strong> for sync &amp; async requests.</li>
<li><code>AnthropicLLM</code>: uses the <strong>Anthropic API</strong> for sync &amp; async requests.</li>
</ul>
</li>
<li>
<p><strong>Server</strong> — connects to an LLM server, locally or remotely:</p>
<ul>
<li><code>OllamaLLM</code>: connects to an <strong>Ollama</strong> server for sync &amp; async requests.</li>
</ul>
</li>
<li>
<p><strong>Local</strong> — runs the model directly in your runtime:</p>
<ul>
<li><code>LlamaCPPLLM</code>: uses a local <strong>llama.cpp</strong> runtime for sync &amp; async requests.</li>
</ul>
</li>
</ul>
<p>You can also use other LLM clients directly without going through Protolink’s <code>LLM</code> wrappers if you prefer.</p>
<h2 id="configuration">Configuration<a class="headerlink" href="#configuration" title="Permanent link">&para;</a></h2>
<p>Configuration depends on the specific backend, but the general pattern is:</p>
<ol>
<li><strong>Install the relevant extras</strong> (from the README):</li>
</ol>
<div class="highlight"><pre><span></span><code><span class="c1"># All supported LLM backends</span>
uv<span class="w"> </span>add<span class="w"> </span><span class="s2">&quot;protolink[llms]&quot;</span>
</code></pre></div>
<div class="admonition info">
<p class="admonition-title">Choosing LLM extras</p>
<p>If you only need a subset of LLMs (e.g. OpenAI API), it is advised to <strong>install them manually</strong> instead of using the <code>llms</code> extra, which will intall all the supported libraries.</p>
</div>
<ol>
<li><strong>Instantiate the LLM</strong> with the desired model and credentials:</li>
</ol>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">protolink.llms.api</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAILLM</span>


<span class="n">llm</span> <span class="o">=</span> <span class="n">OpenAILLM</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-5.2&quot;</span><span class="p">,</span>
    <span class="c1"># api_key is typically read from the environment, e.g. OPENAI_API_KEY</span>
<span class="p">)</span>
</code></pre></div>
<div class="admonition warning">
<p class="admonition-title">API keys</p>
<p>Never commit API keys to version control. Read them from environment variables or a secure secrets manager.</p>
</div>
<ol>
<li><strong>Pass the LLM to your Agent</strong>:</li>
</ol>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">protolink.agents</span><span class="w"> </span><span class="kn">import</span> <span class="n">Agent</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">protolink.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">AgentCard</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">protolink.transport</span><span class="w"> </span><span class="kn">import</span> <span class="n">HTTPTransport</span>


<span class="n">agent_card</span> <span class="o">=</span> <span class="n">AgentCard</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;llm_agent&quot;</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Agent backed by an LLM&quot;</span><span class="p">)</span>
<span class="n">transport</span> <span class="o">=</span> <span class="n">HTTPTransport</span><span class="p">()</span>

<span class="n">agent</span> <span class="o">=</span> <span class="n">Agent</span><span class="p">(</span><span class="n">agent_card</span><span class="p">,</span> <span class="n">transport</span><span class="p">,</span> <span class="n">llm</span><span class="p">)</span>
</code></pre></div>
<p>For local and server‑style LLMs (<code>LlamaCPPLLM</code>, <code>OllamaLLM</code>), configuration additionally includes paths to model files or server URLs. Refer to the corresponding example scripts in <code>examples/llms.py</code> for concrete usage patterns.</p>
<hr />
<h1 id="llm-api-reference">LLM API Reference<a class="headerlink" href="#llm-api-reference" title="Permanent link">&para;</a></h1>
<p>This section provides a detailed API reference for all LLM classes in Protolink. All LLM implementations inherit from the base <code>LLM</code> class and provide a consistent interface for generating responses.</p>
<div class="admonition success">
<p class="admonition-title">Unified LLM Interface</p>
<p><strong>Protolink provides a single, consistent API for all LLM providers.</strong> Whether you're using OpenAI, Anthropic, Ollama, or local models, you interact with them through the same methods: <code>generate_response()</code>, <code>generate_stream_response()</code>, and configuration helpers. This unified approach means you can swap LLM providers without changing your application code - just update the initialization and you're done!</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Why Use Protolink's LLM Wrappers?</p>
<ul>
<li><strong>Provider Agnostic</strong>: Switch between OpenAI, Anthropic, Ollama, and future providers with minimal code changes</li>
<li><strong>Consistent Interface</strong>: Same method signatures and behavior across all implementations</li>
<li><strong>Built-in Features</strong>: Connection validation, parameter validation, and error handling out of the box</li>
<li><strong>Extensible</strong>: Easy to add new LLM providers while maintaining compatibility</li>
<li><strong>Production Ready</strong>: Robust error handling and logging for real-world applications</li>
</ul>
</div>
<div class="admonition example">
<p class="admonition-title">Provider Switching in Action</p>
<div class="highlight"><pre><span></span><code><span class="c1"># The same code works with ANY LLM provider</span>

<span class="c1"># Choose your provider - just change the import and initialization!</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">protolink.llms.api</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAILLM</span>    <span class="c1"># or AnthropicLLM</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">protolink.llms.server</span><span class="w"> </span><span class="kn">import</span> <span class="n">OllamaLLM</span>  <span class="c1"># or any other provider</span>

<span class="c1"># Initialize your chosen LLM</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">OpenAILLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4&quot;</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="c1"># llm = AnthropicLLM(model=&quot;claude-3-sonnet&quot;, temperature=0.7)</span>
<span class="c1"># llm = OllamaLLM(model=&quot;llama3&quot;, temperature=0.7)</span>

<span class="c1"># The rest of your code stays EXACTLY the same!</span>
<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">Message</span><span class="p">(</span><span class="n">role</span><span class="o">=</span><span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="n">content</span><span class="o">=</span><span class="s2">&quot;Hello!&quot;</span><span class="p">)]</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate_response</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>

<span class="c1"># Streaming also works identically</span>
<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate_stream_response</span><span class="p">(</span><span class="n">messages</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">content</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
</div>
<div class="admonition info">
<p class="admonition-title">LLM Hierarchy</p>
<ul>
<li><strong><code>LLM</code></strong> - abstract base class</li>
<li><strong><code>APILLM</code></strong> - base for API-based LLMs</li>
<li><strong><code>ServerLLM</code></strong> - base for server-based LLMs</li>
<li><strong><code>LocalLLM</code></strong> - base for local runtime LLMs</li>
<li><strong>Concrete implementations</strong>: <code>OpenAILLM</code>, <code>AnthropicLLM</code>, <code>OllamaLLM</code>, etc.</li>
</ul>
</div>
<h2 id="base-llm-class">Base LLM Class<a class="headerlink" href="#base-llm-class" title="Permanent link">&para;</a></h2>
<p>The <code>LLM</code> class defines the common interface that all LLM implementations must follow.</p>
<h3 id="attributes">Attributes<a class="headerlink" href="#attributes" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>model_type</code></td>
<td><code>LLMType</code></td>
<td>The type of LLM (<code>"api"</code>, <code>"local"</code>, or <code>"server"</code>).</td>
</tr>
<tr>
<td><code>provider</code></td>
<td><code>LLMProvider</code></td>
<td>The provider name (<code>"openai"</code>, <code>"anthropic"</code>, <code>"ollama"</code>, etc.).</td>
</tr>
<tr>
<td><code>model</code></td>
<td><code>str</code></td>
<td>The model name/identifier.</td>
</tr>
<tr>
<td><code>model_params</code></td>
<td><code>dict[str, Any]</code></td>
<td>Model-specific parameters (temperature, max_tokens, etc.).</td>
</tr>
<tr>
<td><code>system_prompt</code></td>
<td><code>str</code></td>
<td>Default system prompt for the model.</td>
</tr>
</tbody>
</table>
<h3 id="core-methods">Core Methods<a class="headerlink" href="#core-methods" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Name</th>
<th>Parameters</th>
<th>Returns</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>generate_response()</code></td>
<td><code>messages: list[Message]</code></td>
<td><code>Message</code></td>
<td>Generate a single response from the model.</td>
</tr>
<tr>
<td><code>generate_stream_response()</code></td>
<td><code>messages: list[Message]</code></td>
<td><code>Iterable[Message]</code></td>
<td>Generate a streaming response, yielding messages as they're generated.</td>
</tr>
<tr>
<td><code>set_model_params()</code></td>
<td><code>model_params: dict[str, Any]</code></td>
<td><code>None</code></td>
<td>Update model parameters.</td>
</tr>
<tr>
<td><code>set_system_prompt()</code></td>
<td><code>system_prompt: str</code></td>
<td><code>None</code></td>
<td>Set the system prompt for the model.</td>
</tr>
<tr>
<td><code>validate_connection()</code></td>
<td>—</td>
<td><code>bool</code></td>
<td>Validate that the LLM connection is working.</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Abstract Methods</p>
<p>The <code>LLM</code> base class is abstract. You should use one of the concrete implementations like <code>OpenAILLM</code> or <code>AnthropicLLM</code>.</p>
</div>
<h2 id="api-based-llms">API-based LLMs<a class="headerlink" href="#api-based-llms" title="Permanent link">&para;</a></h2>
<p>API-based LLMs connect to external services and require API keys or authentication.</p>
<h3 id="apillm-base-class">APILLM Base Class<a class="headerlink" href="#apillm-base-class" title="Permanent link">&para;</a></h3>
<p>Base class for all API-based LLM implementations.</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Parameters</th>
<th>Returns</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>set_model_params()</code></td>
<td><code>model_params: dict[str, Any]</code></td>
<td><code>None</code></td>
<td>Update existing model parameters, ignoring invalid keys.</td>
</tr>
<tr>
<td><code>set_system_prompt()</code></td>
<td><code>system_prompt: str</code></td>
<td><code>None</code></td>
<td>Set the system prompt for the model.</td>
</tr>
<tr>
<td><code>validate_connection()</code></td>
<td>—</td>
<td><code>bool</code></td>
<td><strong>Abstract.</strong> Validate API connection (implemented by subclasses).</td>
</tr>
</tbody>
</table>
<h3 id="openaillm">OpenAILLM<a class="headerlink" href="#openaillm" title="Permanent link">&para;</a></h3>
<p>OpenAI API implementation using the official OpenAI client.</p>
<h4 id="constructor">Constructor<a class="headerlink" href="#constructor" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>api_key</code></td>
<td><code>str \| None</code></td>
<td><code>None</code></td>
<td>OpenAI API key. If not provided, uses <code>OPENAI_API_KEY</code> environment variable.</td>
</tr>
<tr>
<td><code>model</code></td>
<td><code>str \| None</code></td>
<td><code>"gpt-5"</code></td>
<td>OpenAI model name.</td>
</tr>
<tr>
<td><code>model_params</code></td>
<td><code>dict[str, Any] \| None</code></td>
<td><code>None</code></td>
<td>Model parameters (temperature, max_tokens, etc.).</td>
</tr>
<tr>
<td><code>base_url</code></td>
<td><code>str \| None</code></td>
<td><code>None</code></td>
<td>Custom base URL for OpenAI-compatible APIs.</td>
</tr>
</tbody>
</table>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">protolink.llms.api</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAILLM</span>

<span class="c1"># Basic usage</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">OpenAILLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4&quot;</span><span class="p">)</span>

<span class="c1"># With custom parameters</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">OpenAILLM</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4-turbo&quot;</span><span class="p">,</span>
    <span class="n">model_params</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">0.7</span><span class="p">,</span>
        <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>
        <span class="s2">&quot;top_p&quot;</span><span class="p">:</span> <span class="mf">0.9</span>
    <span class="p">}</span>
<span class="p">)</span>

<span class="c1"># With custom base URL (for OpenAI-compatible APIs)</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">OpenAILLM</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;custom-model&quot;</span><span class="p">,</span>
    <span class="n">base_url</span><span class="o">=</span><span class="s2">&quot;https://api.custom-provider.com/v1&quot;</span><span class="p">,</span>
    <span class="n">api_key</span><span class="o">=</span><span class="s2">&quot;your-api-key&quot;</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="default-model-parameters">Default Model Parameters<a class="headerlink" href="#default-model-parameters" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Type</th>
<th>Default</th>
<th>Range/Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>temperature</code></td>
<td><code>float</code></td>
<td><code>1.0</code></td>
<td><code>0.0</code> to <code>2.0</code> - Controls randomness</td>
</tr>
<tr>
<td><code>top_p</code></td>
<td><code>float</code></td>
<td><code>1.0</code></td>
<td>Nucleus sampling parameter</td>
</tr>
<tr>
<td><code>n</code></td>
<td><code>int</code></td>
<td><code>1</code></td>
<td>Number of completions to generate</td>
</tr>
<tr>
<td><code>stream</code></td>
<td><code>bool</code></td>
<td><code>False</code></td>
<td>Whether to stream responses</td>
</tr>
<tr>
<td><code>stop</code></td>
<td><code>str \| list[str] \| None</code></td>
<td><code>None</code></td>
<td>Stop sequences</td>
</tr>
<tr>
<td><code>max_tokens</code></td>
<td><code>int \| None</code></td>
<td><code>None</code></td>
<td>Maximum tokens to generate</td>
</tr>
<tr>
<td><code>presence_penalty</code></td>
<td><code>float</code></td>
<td><code>0.0</code></td>
<td><code>-2.0</code> to <code>2.0</code> - Presence penalty</td>
</tr>
<tr>
<td><code>frequency_penalty</code></td>
<td><code>float</code></td>
<td><code>0.0</code></td>
<td><code>-2.0</code> to <code>2.0</code> - Frequency penalty</td>
</tr>
<tr>
<td><code>logit_bias</code></td>
<td><code>dict \| None</code></td>
<td><code>None</code></td>
<td>Token bias dictionary</td>
</tr>
</tbody>
</table>
<h4 id="methods">Methods<a class="headerlink" href="#methods" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Parameters</th>
<th>Returns</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>generate_response()</code></td>
<td><code>messages: list[Message]</code></td>
<td><code>Message</code></td>
<td>Generate a single response using OpenAI's API.</td>
</tr>
<tr>
<td><code>generate_stream_response()</code></td>
<td><code>messages: list[Message]</code></td>
<td><code>Iterable[Message]</code></td>
<td>Generate streaming response, yielding partial messages.</td>
</tr>
<tr>
<td><code>validate_connection()</code></td>
<td>—</td>
<td><code>bool</code></td>
<td>Check if the model is available and API key is valid.</td>
</tr>
</tbody>
</table>
<div class="admonition warning">
<p class="admonition-title">API Key Required</p>
<p>OpenAI requires a valid API key. Set the <code>OPENAI_API_KEY</code> environment variable or pass the <code>api_key</code> parameter.</p>
</div>
<h3 id="anthropicllm">AnthropicLLM<a class="headerlink" href="#anthropicllm" title="Permanent link">&para;</a></h3>
<p>Anthropic Claude API implementation using the official Anthropic client.</p>
<h4 id="constructor_1">Constructor<a class="headerlink" href="#constructor_1" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>api_key</code></td>
<td><code>str \| None</code></td>
<td><code>None</code></td>
<td>Anthropic API key. If not provided, uses <code>ANTHROPIC_API_KEY</code> environment variable.</td>
</tr>
<tr>
<td><code>model</code></td>
<td><code>str \| None</code></td>
<td><code>"claude-sonnet-4-20250514"</code></td>
<td>Claude model name.</td>
</tr>
<tr>
<td><code>model_params</code></td>
<td><code>dict[str, Any] \| None</code></td>
<td><code>None</code></td>
<td>Model parameters (temperature, max_tokens, etc.).</td>
</tr>
<tr>
<td><code>base_url</code></td>
<td><code>str \| None</code></td>
<td><code>None</code></td>
<td>Custom base URL for Anthropic-compatible APIs.</td>
</tr>
</tbody>
</table>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">protolink.llms.api</span><span class="w"> </span><span class="kn">import</span> <span class="n">AnthropicLLM</span>

<span class="c1"># Basic usage</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">AnthropicLLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;claude-3-5-sonnet-20241022&quot;</span><span class="p">)</span>

<span class="c1"># With custom parameters</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">AnthropicLLM</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;claude-3-5-haiku-20241022&quot;</span><span class="p">,</span>
    <span class="n">model_params</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
        <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="mi">2000</span><span class="p">,</span>
        <span class="s2">&quot;top_p&quot;</span><span class="p">:</span> <span class="mf">0.8</span>
    <span class="p">}</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="default-model-parameters_1">Default Model Parameters<a class="headerlink" href="#default-model-parameters_1" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Type</th>
<th>Default</th>
<th>Range/Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>max_tokens</code></td>
<td><code>int</code></td>
<td><code>4096</code></td>
<td>Maximum tokens to generate</td>
</tr>
<tr>
<td><code>temperature</code></td>
<td><code>float</code></td>
<td><code>1.0</code></td>
<td><code>0.0</code> to <code>1.0</code> - Controls randomness</td>
</tr>
<tr>
<td><code>top_p</code></td>
<td><code>float</code></td>
<td><code>1.0</code></td>
<td>Nucleus sampling parameter</td>
</tr>
<tr>
<td><code>top_k</code></td>
<td><code>int \| None</code></td>
<td><code>None</code></td>
<td>Top-k sampling parameter</td>
</tr>
<tr>
<td><code>stop_sequences</code></td>
<td><code>list[str] \| None</code></td>
<td><code>None</code></td>
<td>Stop sequences</td>
</tr>
<tr>
<td><code>metadata</code></td>
<td><code>dict \| None</code></td>
<td><code>None</code></td>
<td>Additional metadata</td>
</tr>
</tbody>
</table>
<h4 id="methods_1">Methods<a class="headerlink" href="#methods_1" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Parameters</th>
<th>Returns</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>generate_response()</code></td>
<td><code>messages: list[Message]</code></td>
<td><code>Message</code></td>
<td>Generate a single response using Anthropic's API.</td>
</tr>
<tr>
<td><code>generate_stream_response()</code></td>
<td><code>messages: list[Message]</code></td>
<td><code>Iterable[Message]</code></td>
<td>Generate streaming response, yielding partial messages.</td>
</tr>
<tr>
<td><code>validate_connection()</code></td>
<td>—</td>
<td><code>bool</code></td>
<td>Check if the model is available and API key is valid.</td>
</tr>
</tbody>
</table>
<div class="admonition warning">
<p class="admonition-title">API Key Required</p>
<p>Anthropic requires a valid API key. Set the <code>ANTHROPIC_API_KEY</code> environment variable or pass the <code>api_key</code> parameter.</p>
</div>
<h2 id="server-based-llms">Server-based LLMs<a class="headerlink" href="#server-based-llms" title="Permanent link">&para;</a></h2>
<p>Server-based LLMs connect to local or remote LLM servers.</p>
<h3 id="serverllm-base-class">ServerLLM Base Class<a class="headerlink" href="#serverllm-base-class" title="Permanent link">&para;</a></h3>
<p>Base class for all server-based LLM implementations.</p>
<h4 id="constructor_2">Constructor<a class="headerlink" href="#constructor_2" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>base_url</code></td>
<td><code>str</code></td>
<td>—</td>
<td><strong>Required.</strong> URL of the LLM server.</td>
</tr>
</tbody>
</table>
<h4 id="methods_2">Methods<a class="headerlink" href="#methods_2" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Parameters</th>
<th>Returns</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>set_model_params()</code></td>
<td><code>model_params: dict[str, Any]</code></td>
<td><code>None</code></td>
<td>Update existing model parameters, ignoring invalid keys.</td>
</tr>
<tr>
<td><code>set_system_prompt()</code></td>
<td><code>system_prompt: str</code></td>
<td><code>None</code></td>
<td>Set the system prompt for the model.</td>
</tr>
<tr>
<td><code>validate_connection()</code></td>
<td>—</td>
<td><code>bool</code></td>
<td>Validate that the server is reachable.</td>
</tr>
</tbody>
</table>
<h3 id="ollamallm">OllamaLLM<a class="headerlink" href="#ollamallm" title="Permanent link">&para;</a></h3>
<p>Ollama server implementation for connecting to local or remote Ollama instances.</p>
<h4 id="constructor_3">Constructor<a class="headerlink" href="#constructor_3" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>base_url</code></td>
<td><code>str \| None</code></td>
<td><code>None</code></td>
<td>Ollama server URL. If not provided, uses <code>OLLAMA_HOST</code> environment variable.</td>
</tr>
<tr>
<td><code>headers</code></td>
<td><code>dict[str, str] \| None</code></td>
<td><code>None</code></td>
<td>Additional HTTP headers (including auth).</td>
</tr>
<tr>
<td><code>model</code></td>
<td><code>str \| None</code></td>
<td><code>"gemma3"</code></td>
<td>Ollama model name.</td>
</tr>
<tr>
<td><code>model_params</code></td>
<td><code>dict[str, Any] \| None</code></td>
<td><code>None</code></td>
<td>Model parameters (temperature, etc.).</td>
</tr>
</tbody>
</table>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">protolink.llms.server</span><span class="w"> </span><span class="kn">import</span> <span class="n">OllamaLLM</span>

<span class="c1"># Local Ollama server</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">OllamaLLM</span><span class="p">(</span>
    <span class="n">base_url</span><span class="o">=</span><span class="s2">&quot;http://localhost:11434&quot;</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;llama3&quot;</span>
<span class="p">)</span>

<span class="c1"># Remote Ollama with authentication</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">OllamaLLM</span><span class="p">(</span>
    <span class="n">base_url</span><span class="o">=</span><span class="s2">&quot;https://ollama.example.com&quot;</span><span class="p">,</span>
    <span class="n">headers</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;Authorization&quot;</span><span class="p">:</span> <span class="s2">&quot;Bearer your-token&quot;</span><span class="p">},</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;codellama&quot;</span>
<span class="p">)</span>

<span class="c1"># Using environment variables</span>
<span class="c1"># Set OLLAMA_HOST=http://localhost:11434</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">OllamaLLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;mistral&quot;</span><span class="p">)</span>
</code></pre></div>
<h4 id="default-model-parameters_2">Default Model Parameters<a class="headerlink" href="#default-model-parameters_2" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>temperature</code></td>
<td><code>float</code></td>
<td><code>1.0</code></td>
<td>Controls randomness (range depends on model).</td>
</tr>
</tbody>
</table>
<h4 id="methods_3">Methods<a class="headerlink" href="#methods_3" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Parameters</th>
<th>Returns</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>generate_response()</code></td>
<td><code>messages: list[Message]</code></td>
<td><code>Message</code></td>
<td>Generate a single response using Ollama's API.</td>
</tr>
<tr>
<td><code>generate_stream_response()</code></td>
<td><code>messages: list[Message]</code></td>
<td><code>Iterable[Message]</code></td>
<td>Generate streaming response, yielding partial messages.</td>
</tr>
<tr>
<td><code>validate_connection()</code></td>
<td>—</td>
<td><code>bool</code></td>
<td>Check if Ollama server is reachable and has models available.</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Ollama Server Required</p>
<p>OllamaLLM requires a running Ollama server. Install Ollama and start it with <code>ollama serve</code>.</p>
</div>
<h2 id="usage-examples">Usage Examples<a class="headerlink" href="#usage-examples" title="Permanent link">&para;</a></h2>
<h3 id="basic-llm-usage">Basic LLM Usage<a class="headerlink" href="#basic-llm-usage" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">protolink.llms.api</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAILLM</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">protolink.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">Message</span>

<span class="c1"># Initialize LLM</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">OpenAILLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4&quot;</span><span class="p">)</span>

<span class="c1"># Create messages</span>
<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">Message</span><span class="p">(</span><span class="n">role</span><span class="o">=</span><span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="n">content</span><span class="o">=</span><span class="s2">&quot;Hello, how are you?&quot;</span><span class="p">)</span>
<span class="p">]</span>

<span class="c1"># Generate response</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate_response</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</code></pre></div>
<h3 id="streaming-responses">Streaming Responses<a class="headerlink" href="#streaming-responses" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Generate streaming response</span>
<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate_stream_response</span><span class="p">(</span><span class="n">messages</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">content</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<h3 id="updating-parameters">Updating Parameters<a class="headerlink" href="#updating-parameters" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Update model parameters</span>
<span class="n">llm</span><span class="o">.</span><span class="n">set_model_params</span><span class="p">({</span>
    <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">0.7</span><span class="p">,</span>
    <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="mi">500</span>
<span class="p">})</span>

<span class="c1"># Update system prompt</span>
<span class="n">llm</span><span class="o">.</span><span class="n">set_system_prompt</span><span class="p">(</span><span class="s2">&quot;You are a helpful coding assistant.&quot;</span><span class="p">)</span>
</code></pre></div>
<h3 id="connection-validation">Connection Validation<a class="headerlink" href="#connection-validation" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Validate connection before use</span>
<span class="k">if</span> <span class="n">llm</span><span class="o">.</span><span class="n">validate_connection</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;LLM is ready!&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;LLM connection failed.&quot;</span><span class="p">)</span>
</code></pre></div>
<h2 id="error-handling">Error Handling<a class="headerlink" href="#error-handling" title="Permanent link">&para;</a></h2>
<p>All LLM implementations include error handling for common issues:</p>
<ul>
<li><strong>Authentication Errors</strong>: Missing or invalid API keys</li>
<li><strong>Connection Errors</strong>: Network issues or unavailable servers</li>
<li><strong>Model Errors</strong>: Invalid model names or unavailable models</li>
<li><strong>Parameter Errors</strong>: Invalid parameter values</li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Connection Validation</p>
<p>Always call <code>validate_connection()</code> before using an LLM to ensure it's properly configured and reachable.</p>
</div>
<h2 id="type-aliases">Type Aliases<a class="headerlink" href="#type-aliases" title="Permanent link">&para;</a></h2>
<p>The LLM module defines several type aliases for clarity:</p>
<div class="highlight"><pre><span></span><code><span class="n">LLMType</span><span class="p">:</span> <span class="n">TypeAlias</span> <span class="o">=</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;api&quot;</span><span class="p">,</span> <span class="s2">&quot;local&quot;</span><span class="p">,</span> <span class="s2">&quot;server&quot;</span><span class="p">]</span>
<span class="n">LLMProvider</span><span class="p">:</span> <span class="n">TypeAlias</span> <span class="o">=</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;openai&quot;</span><span class="p">,</span> <span class="s2">&quot;anthropic&quot;</span><span class="p">,</span> <span class="s2">&quot;google&quot;</span><span class="p">,</span> <span class="s2">&quot;llama.cpp&quot;</span><span class="p">,</span> <span class="s2">&quot;ollama&quot;</span><span class="p">]</span>
</code></pre></div>
<p>These are used throughout the LLM implementations to ensure type safety and clarity.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../agents/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Agents">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Agents
              </div>
            </div>
          </a>
        
        
          
          <a href="../models/" class="md-footer__link md-footer__link--next" aria-label="Next: Models">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Models
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/nMaroulis/protolink" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://pypi.org/project/protolink/" target="_blank" rel="noopener" title="pypi.org" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M384 512H96c-53 0-96-43-96-96V96C0 43 43 0 96 0h304c26.5 0 48 21.5 48 48v288c0 20.9-13.4 38.7-32 45.3V448c17.7 0 32 14.3 32 32s-14.3 32-32 32zM96 384c-17.7 0-32 14.3-32 32s14.3 32 32 32h256v-64zm32-232c0 13.3 10.7 24 24 24h176c13.3 0 24-10.7 24-24s-10.7-24-24-24H152c-13.3 0-24 10.7-24 24m24 72c-13.3 0-24 10.7-24 24s10.7 24 24 24h176c13.3 0 24-10.7 24-24s-10.7-24-24-24z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "..", "features": ["navigation.instant", "navigation.tracking", "content.code.copy", "navigation.footer", "navigation.expand", "navigation.top"], "search": "../assets/javascripts/workers/search.7a47a382.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.e71a0d61.min.js"></script>
      
    
  </body>
</html>