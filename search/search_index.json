{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Protolink","text":"<p>A lightweight, production-ready framework for agent-to-agent communication, built on and extending Google's A2A protocol.</p> <p>Get Started View on GitHub</p> <p>Welcome to the Protolink documentation.</p> <p>This site provides an overview of the framework, its concepts, and how to use it in your projects.</p> <p>Current release: see protolink on PyPI. </p>"},{"location":"#contents","title":"Contents","text":"<ul> <li>Getting Started</li> <li>Agents</li> <li>Transports</li> <li>LLMs</li> <li>Tools</li> <li>Examples</li> </ul>"},{"location":"#what-you-can-do-with-protolink","title":"What you can do with Protolink","text":"<ul> <li> <p>Build agents quickly   See Getting Started and Agents for the core concepts and basic setup.</p> </li> <li> <p>Choose your transport   Explore Transports to switch between HTTP, WebSocket, runtime, and future transports with minimal code changes.</p> </li> <li> <p>Plug in LLMs &amp; tools   Use LLMs and Tools to wire in language models and both native &amp; MCP tools as agent modules.</p> </li> </ul>"},{"location":"#overview","title":"Overview","text":"<p>Protolink is a lightweight, production\u2011ready framework for agent\u2011to\u2011agent communication, built around and extending Google\u2019s A2A protocol.</p> <p>Key ideas:</p> <ul> <li>Unified Agent model: a single <code>Agent</code> instance handles both client and server responsibilities.</li> <li>Flexible transports: HTTP, WebSocket, in\u2011process runtime, and planned JSON\u2011RPC / gRPC transports.</li> <li>LLM\u2011ready architecture: first\u2011class integration with API, local, and server\u2011hosted LLMs.</li> <li>Tools as modules: native Python tools and MCP tools plugged directly into agents.</li> </ul> <p>Use this documentation to:</p> <ul> <li>Install Protolink and run your first agent.</li> <li>Understand how agents, transports, LLMs, and tools fit together.</li> <li>Explore practical examples you can adapt to your own systems.</li> </ul> <p>Protolink is open source under the MIT license. Contributions are welcome \u2013 see the repository\u2019s Contributing section on GitHub.</p>"},{"location":"agents/","title":"Agents","text":"<p>Agents are the core building blocks in Protolink.</p>"},{"location":"agents/#concepts","title":"Concepts","text":"<p>An Agent in Protolink is a unified component that can act as both client and server. This is different from the original A2A spec, which separates client and server concerns.</p> <p>High\u2011level ideas:</p> <ul> <li>Unified model: a single <code>Agent</code> instance can send and receive messages.</li> <li>AgentCard: a small model describing the agent (name, description, metadata).</li> <li>Modules:</li> <li>LLMs (e.g. <code>OpenAILLM</code>, <code>AnthropicLLM</code>, <code>LlamaCPPLLM</code>, <code>OllamaLLM</code>).</li> <li>Tools (native Python functions or MCP\u2011backed tools).</li> <li>Transport abstraction: agents communicate over transports such as HTTP, WebSocket, gRPC, or the in\u2011process runtime transport.</li> </ul>"},{"location":"agents/#creating-an-agent","title":"Creating an Agent","text":"<p>A minimal agent consists of three pieces:</p> <ol> <li>An <code>AgentCard</code> describing the agent.</li> <li>A <code>Transport</code> implementation.</li> <li>An optional LLM and tools.</li> </ol> <p>Example:</p> <pre><code>from protolink.agents import Agent\nfrom protolink.models import AgentCard\nfrom protolink.transport import HTTPTransport\nfrom protolink.llms.api import OpenAILLM\n\n\nagent_card = AgentCard(\n    name=\"example_agent\",\n    description=\"A dummy agent\",\n)\n\ntransport = HTTPTransport()\nllm = OpenAILLM(model=\"gpt-5.1\")\n\nagent = Agent(agent_card, transport, llm)\n</code></pre> <p>You can then attach tools and start the agent.</p>"},{"location":"agents/#agent-to-agent-communication","title":"Agent-to-Agent Communication","text":"<p>Agents communicate over a chosen transport.</p> <p>Common patterns:</p> <ul> <li>RuntimeTransport: multiple agents in the same process share an in\u2011memory transport, which is ideal for local testing and composition.</li> <li>HTTPTransport / WebSocketTransport: agents expose HTTP or WebSocket endpoints so that other agents (or external clients) can send requests.</li> <li>gRPC / JSON\u2011RPC (planned): additional transports for more structured or high\u2011performance communication.</li> </ul> <p>From the framework\u2019s perspective, all of these are implementations of the same transport interface, so you can swap them with minimal code changes.</p>"},{"location":"agents/#agent-api-reference","title":"Agent API Reference","text":"<p>This section provides a detailed API reference for the <code>Agent</code> base class in <code>protolink.agents.base</code>. The <code>Agent</code> class is the core component for creating A2A-compatible agents, serving as both client and server.</p> <p>Unified Agent Model</p> <p>Unlike the original A2A specification, Protolink's <code>Agent</code> combines client and server functionality in a single class. You can send tasks/messages to other agents while also serving incoming requests.</p>"},{"location":"agents/#constructor","title":"Constructor","text":"Parameter Type Default Description <code>card</code> <code>AgentCard</code> \u2014 Required. The agent's metadata card containing name, description, and other identifying information. <code>llm</code> <code>LLM \\| None</code> <code>None</code> Optional language model instance for AI-powered task processing. <code>transport</code> <code>Transport \\| None</code> <code>None</code> Optional transport for communication. If not provided, you must set one later via <code>set_transport()</code>. <code>auth_provider</code> <code>AuthProvider \\| None</code> <code>None</code> Optional authentication provider for securing agent communications. <pre><code>from protolink.agents import Agent\nfrom protolink.models import AgentCard\nfrom protolink.transport import HTTPTransport\nfrom protolink.llms.api import OpenAILLM\n\ncard = AgentCard(name=\"my_agent\", description=\"Example agent\")\nllm = OpenAILLM(model=\"gpt-4\")\ntransport = HTTPTransport()\n\nagent = Agent(card=card, llm=llm, transport=transport)\n</code></pre>"},{"location":"agents/#lifecycle-methods","title":"Lifecycle Methods","text":"<p>These methods control the agent's server component lifecycle.</p> Name Parameters Returns Description <code>start()</code> \u2014 <code>None</code> Starts the agent's server component if a transport is configured. <code>stop()</code> \u2014 <code>None</code> Stops the agent's server component and cleans up resources. <p>Transport Required</p> <p>The <code>start()</code> method requires a transport to be configured. If no transport was provided during construction, call <code>set_transport()</code> first.</p>"},{"location":"agents/#transport-management","title":"Transport Management","text":"Name Parameters Returns Description <code>set_transport()</code> <code>transport: Transport</code> <code>None</code> Sets or updates the transport used by this agent. <code>client</code> (property) \u2014 <code>AgentClient</code> Returns the client instance for sending requests to other agents. <code>server</code> (property) \u2014 <code>Server \\| None</code> Returns the server instance if one is available via the transport."},{"location":"agents/#task-and-message-handling","title":"Task and Message Handling","text":""},{"location":"agents/#core-task-processing","title":"Core Task Processing","text":"Name Parameters Returns Description <code>handle_task()</code> <code>task: Task</code> <code>Task</code> Abstract method. Subclasses must implement this to define how tasks are processed. <code>handle_task_streaming()</code> <code>task: Task</code> <code>AsyncIterator[Task]</code> Optional streaming handler for real-time task updates. Default raises <code>NotImplementedError</code>."},{"location":"agents/#communication-methods","title":"Communication Methods","text":"Name Parameters Returns Description <code>send_task_to()</code> <code>agent_url: str</code>, <code>task: Task</code>, <code>skill: str \\| None = None</code> <code>Task</code> Sends a task to another agent and returns the processed result. <code>send_message_to()</code> <code>agent_url: str</code>, <code>message: Message</code> <code>Message</code> Sends a message to another agent and returns the response. <p>Authentication</p> <p>All outgoing requests are automatically signed if an <code>auth_provider</code> is configured. Incoming requests are verified against the same provider.</p>"},{"location":"agents/#tool-management","title":"Tool Management","text":"<p>Tools allow agents to execute external functions and APIs.</p> Name Parameters Returns Description <code>add_tool()</code> <code>tool: BaseTool</code> <code>None</code> Registers a tool with the agent. <code>tool()</code> <code>name: str</code>, <code>description: str</code> <code>decorator</code> Decorator for registering Python functions as tools. <code>call_tool()</code> <code>tool_name: str</code>, <code>**kwargs</code> <code>Any</code> Executes a registered tool by name with provided arguments. <pre><code># Using the decorator approach\n@agent.tool(\"calculate\", \"Performs basic calculations\")\ndef calculate(operation: str, a: float, b: float) -&gt; float:\n    if operation == \"add\":\n        return a + b\n    elif operation == \"multiply\":\n        return a * b\n    else:\n        raise ValueError(f\"Unsupported operation: {operation}\")\n\n# Direct tool registration\nfrom protolink.tools import BaseTool\n\nclass WeatherTool(BaseTool):\n    def call(self, location: str) -&gt; dict:\n        # Weather API logic here\n        return {\"temperature\": 72, \"conditions\": \"sunny\"}\n\nagent.add_tool(WeatherTool())\n</code></pre>"},{"location":"agents/#utility-methods","title":"Utility Methods","text":"Name Parameters Returns Description <code>get_agent_card()</code> \u2014 <code>AgentCard</code> Returns the agent's metadata card. <code>set_llm()</code> <code>llm: LLM</code> <code>None</code> Updates the agent's language model instance. <code>verify_auth()</code> <code>request: Request</code> <code>bool</code> Verifies authentication for incoming requests if an auth provider is configured."},{"location":"agents/#abstract-methods","title":"Abstract Methods","text":"<p>Subclasses of <code>Agent</code> must implement the following methods:</p> <ul> <li><code>handle_task(task: Task) -&gt; Task</code>: Defines the core logic for processing incoming tasks.</li> </ul> <p>Minimal Agent Implementation</p> <pre><code>from protolink.agents import Agent\nfrom protolink.models import AgentCard, Task, Message\n\nclass EchoAgent(Agent):\n    async def handle_task(self, task: Task) -&gt; Task:\n        # Echo back all messages\n        response_messages = []\n        for message in task.messages:\n            response_messages.append(\n                Message(\n                    content=f\"Echo: {message.content}\",\n                    role=\"assistant\"\n                )\n            )\n\n        return Task(\n            messages=response_messages,\n            parent_task_id=task.id\n        )\n</code></pre>"},{"location":"agents/#error-handling","title":"Error Handling","text":"<p>The <code>Agent</code> class includes several error handling patterns:</p> <ul> <li>Missing Transport: Raises <code>ValueError</code> if trying to start without a transport.</li> <li>Authentication Failures: Returns <code>401</code> or <code>403</code> responses for invalid auth.</li> <li>Tool Errors: Tool execution errors are propagated to the caller.</li> <li>Task Processing: Errors in <code>handle_task()</code> are caught and returned as error messages to the sender.</li> </ul>"},{"location":"agents/#authentication-integration","title":"Authentication Integration","text":"<p>When an <code>auth_provider</code> is configured, the agent automatically:</p> <ol> <li>Signs outgoing requests with appropriate authentication headers</li> <li>Verifies incoming requests using the same auth mechanism</li> <li>Returns appropriate HTTP status codes for auth failures (401, 403)</li> </ol> <p>Supported auth providers include API key authentication, OAuth, and custom implementations.</p>"},{"location":"examples/","title":"Examples","text":"<p>This section links to example projects and code snippets in the repository.</p>"},{"location":"examples/#http-agents","title":"HTTP Agents","text":"<p>The repository includes several examples under the <code>examples/</code> directory. For HTTP\u2011based agents:</p> <ul> <li><code>examples/http_agents.py</code> \u2014 basic HTTP transport example showing how to spin up an HTTP\u2011enabled agent.</li> <li><code>examples/http_math_agents.py</code> \u2014 example of delegating between agents over HTTP (e.g. a question agent calling a math agent).</li> </ul>"},{"location":"examples/#other-examples","title":"Other Examples","text":"<p>Additional examples illustrate other capabilities:</p> <ul> <li><code>examples/basic_agent.py</code> \u2014 minimal agent setup focused on core concepts.</li> <li><code>examples/llms.py</code> \u2014 examples of wiring different LLM backends into agents.</li> <li><code>examples/runtime_agents.py</code> \u2014 demonstrates using <code>RuntimeTransport</code> for in\u2011process agent communication.</li> <li><code>examples/streaming_agent.py</code> \u2014 shows streaming behaviour (e.g. via WebSocket or other streaming\u2011capable transports).</li> <li><code>examples/oauth_agent.py</code> \u2014 demonstrates OAuth 2.0 and API\u2011key based security in front of agents.</li> </ul> <p>You can run and adapt these scripts as starting points for your own agent systems.</p> <p>New here?</p> <p>Start with <code>examples/basic_agent.py</code> to understand the core concepts, then move on to <code>examples/http_agents.py</code> for HTTP-based setups.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide shows how to install and start using Protolink.</p>"},{"location":"getting-started/#installation","title":"Installation","text":"<p>Protolink is published on PyPI and can be installed with either <code>uv</code> (recommended) or <code>pip</code>.</p>"},{"location":"getting-started/#basic-installation","title":"Basic Installation","text":"<p>This installs the base package without optional extras:</p> <pre><code># Using uv (recommended)\nuv add protolink\n\n# Using pip\npip install protolink\n</code></pre>"},{"location":"getting-started/#optional-dependencies","title":"Optional Dependencies","text":"<p>Protolink exposes several extras to enable additional functionality:</p> <pre><code># Install with all optional dependencies\nuv add \"protolink[all]\"\n\n# HTTP support (for web-based agents)\nuv add \"protolink[http]\"\n\n# All supported LLM libraries\nuv add \"protolink[llms]\"\n\n# Development (all extras + testing tools)\nuv add \"protolink[dev]\"\n</code></pre> <p>Optional extras</p> <p>You usually only need the extras that match your use case. For example, <code>protolink[http]</code> for web transports. The <code>protolink[llms]</code> will install all the supported LLM libraries (OpenAI, Anthropic, Ollama etc.) so it is advised to install manually the libraries that are needed for your project.</p> <p>For development from source:</p> <pre><code>git clone https://github.com/nmaroulis/protolink.git\ncd protolink\nuv pip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/#basic-example","title":"Basic Example","text":"<p>Below is a minimal example that wires together an agent, HTTP transport, an OpenAI LLM, and both native and MCP tools:</p> <pre><code>from protolink.agents import Agent\nfrom protolink.models import AgentCard\nfrom protolink.transport import HTTPTransport\nfrom protolink.tools import MCPToolAdapter\nfrom protolink.llms.api import OpenAILLM\n\n\n# Define the agent card\nagent_card = AgentCard(\n    name=\"example_agent\",\n    description=\"A dummy agent\",\n)\n\n\n# Initialize the transport\ntransport = HTTPTransport()\n\n\n# OpenAI API LLM\nllm = OpenAILLM(model=\"gpt-5.1\")\n\n\n# Initialize the agent\nagent = Agent(agent_card, transport, llm)\n\n\n# Add Native tool\n@agent.tool(name=\"add\", description=\"Add two numbers\")\nasync def add_numbers(a: int, b: int):\n    return a + b\n\n\n# Add MCP tool\nmcp_tool = MCPToolAdapter(mcp_client, \"multiply\")\nagent.add_tool(mcp_tool)\n\n\n# Start the agent\nagent.start()\n</code></pre> <p>This example demonstrates the core pieces of Protolink:</p> <ul> <li>AgentCard to describe the agent.</li> <li>Transport (here <code>HTTPTransport</code>) to handle communication.</li> <li>LLM backend (<code>OpenAILLM</code>).</li> <li>Native tools (Python functions decorated with <code>@agent.tool</code>).</li> <li>MCP tools registered via <code>MCPToolAdapter</code>.</li> </ul>"},{"location":"llms/","title":"LLMs","text":"<p>Protolink integrates with various LLM backends.</p>"},{"location":"llms/#llm-types","title":"LLM Types","text":"<p>Protolink groups LLM backends into three broad categories:</p> <ul> <li>API \u2014 calls a remote API and requires an API key:</li> <li><code>OpenAILLM</code>: uses the OpenAI API for sync &amp; async requests.</li> <li> <p><code>AnthropicLLM</code>: uses the Anthropic API for sync &amp; async requests.</p> </li> <li> <p>Local \u2014 runs the model directly in your runtime:</p> </li> <li> <p><code>LlamaCPPLLM</code>: uses a local llama.cpp runtime for sync &amp; async requests.</p> </li> <li> <p>Server \u2014 connects to an LLM server, locally or remotely:</p> </li> <li><code>OllamaLLM</code>: connects to an Ollama server for sync &amp; async requests.</li> </ul> <p>You can also use other LLM clients directly without going through Protolink\u2019s <code>LLM</code> wrappers if you prefer.</p>"},{"location":"llms/#configuration","title":"Configuration","text":"<p>Configuration depends on the specific backend, but the general pattern is:</p> <ol> <li>Install the relevant extras (from the README):</li> </ol> <pre><code># All supported LLM backends\nuv add \"protolink[llms]\"\n</code></pre> <p>!!! info \"Choosing LLM extras\"        If you only need a subset of backends, you can install more targeted extras once they are exposed (for example, only OpenAI or only local backends).</p> <ol> <li>Instantiate the LLM with the desired model and credentials:</li> </ol> <pre><code>from protolink.llms.api import OpenAILLM\n\n\nllm = OpenAILLM(\n    model=\"gpt-5.1\",\n    # api_key is typically read from the environment, e.g. OPENAI_API_KEY\n)\n</code></pre> <p>!!! warning \"API keys\"        Never commit API keys to version control. Read them from environment variables or a secure secrets manager.</p> <ol> <li>Pass the LLM to your Agent:</li> </ol> <pre><code>from protolink.agents import Agent\nfrom protolink.models import AgentCard\nfrom protolink.transport import HTTPTransport\n\n\nagent_card = AgentCard(name=\"llm_agent\", description=\"Agent backed by an LLM\")\ntransport = HTTPTransport()\n\nagent = Agent(agent_card, transport, llm)\n</code></pre> <p>For local and server\u2011style LLMs (<code>LlamaCPPLLM</code>, <code>OllamaLLM</code>), configuration additionally includes paths to model files or server URLs. Refer to the corresponding example scripts in <code>examples/llms.py</code> for concrete usage patterns.</p>"},{"location":"tools/","title":"Tools","text":"<p>Tools extend agent capabilities with additional functions.</p>"},{"location":"tools/#native-tools","title":"Native Tools","text":"<p>Native tools are regular Python callables that you register on an agent. They are exposed over the transport so that other agents (or clients) can invoke them.</p> <p>To register a native tool, decorate an async function with <code>@agent.tool</code>:</p> <pre><code>from protolink.agents import Agent\n\n\n@agent.tool(name=\"add\", description=\"Add two numbers\")\nasync def add_numbers(a: int, b: int) -&gt; int:\n    return a + b\n</code></pre> <p>Native tools are ideal for business logic, data access, or any capability you want your agents to expose.</p>"},{"location":"tools/#mcp-tools","title":"MCP Tools","text":"<p>Protolink can also expose tools from an MCP (Model Context Protocol) server using the <code>MCPToolAdapter</code>.</p> <p>High\u2011level flow:</p> <ol> <li>Connect to an MCP server using an MCP client (not shown here).</li> <li>Wrap an MCP tool with <code>MCPToolAdapter</code>.</li> <li>Register it on the agent.</li> </ol> <p>Example pattern:</p> <pre><code>from protolink.tools import MCPToolAdapter\n\n\nmcp_tool = MCPToolAdapter(mcp_client, \"multiply\")\nagent.add_tool(mcp_tool)\n</code></pre> <p>The MCP adapter lets you reuse existing MCP tools as if they were native tools, keeping a consistent interface on the agent side.</p>"},{"location":"transports/","title":"Transports","text":"<p>Protolink supports multiple transports for agent communication. A transport is responsible for delivering <code>Task</code> and <code>Message</code> objects between agents and for exposing an API surface (HTTP, WebSocket, in\u2011memory, etc.).</p> <p>At a high level, all transports implement the same conceptual operations:</p> <ul> <li>Send work: send a <code>Task</code> or <code>Message</code> to another agent.</li> <li>Receive work: expose an endpoint / callback to handle incoming tasks.</li> <li>Lifecycle: start and stop the underlying server or runtime.</li> </ul> <p>The sections below describe the available transports in more detail, with an emphasis on <code>HTTPTransport</code>.</p>"},{"location":"transports/#supported-transports","title":"Supported Transports","text":"<ul> <li>HTTPTransport</li> <li>Uses HTTP/HTTPS for synchronous request/response.</li> <li>Backed by ASGI frameworks:<ul> <li><code>Starlette</code> + <code>httpx</code> + <code>uvicorn</code> (lightweight default backend).</li> <li><code>FastAPI</code> + <code>pydantic</code> + <code>uvicorn</code> (with optional request validation).</li> </ul> </li> <li> <p>Great default choice for web\u2011based agents, simple deployments, and interoperable APIs.</p> </li> <li> <p>WebSocketTransport</p> </li> <li>Uses WebSocket for streaming requests and responses.</li> <li>Built on top of libraries like <code>websockets</code> (and <code>httpx</code> for HTTP parts where applicable).</li> <li> <p>Useful for real\u2011time, bidirectional communication or token\u2011level streaming.</p> </li> <li> <p>JSONRPCTransport (TBD)</p> </li> <li>Planned JSON\u2011RPC based transport.</li> <li> <p>Intended for structured, RPC\u2011style interactions.</p> </li> <li> <p>GRPCTransport (TBD)</p> </li> <li>Planned gRPC transport.</li> <li> <p>Intended for high\u2011performance, strongly\u2011typed communication.</p> </li> <li> <p>RuntimeTransport</p> </li> <li>Simple in\u2011process, in\u2011memory transport.</li> <li>Allows multiple agents to communicate within the same Python process.</li> <li>Ideal for local development, testing, and tightly\u2011coupled agent systems.</li> </ul>"},{"location":"transports/#choosing-a-transport","title":"Choosing a Transport","text":"<p>Some rough guidelines:</p> <ul> <li>Use RuntimeTransport for local experiments, tests, or when all agents live in the same process.</li> <li>Use HTTPTransport when you want a simple, interoperable API surface (e.g. calling agents from other services or frontends).</li> <li>Use WebSocketTransport when you need streaming and interactive sessions.</li> <li>Plan for JSONRPCTransport or GRPCTransport if you need stricter schemas or higher performance across services.</li> </ul> <p>The rest of this page dives into the API of each transport in more detail.</p>"},{"location":"transports/#httptransport","title":"HTTPTransport","text":"<p><code>HTTPTransport</code> is the main network transport in Protolink. It exposes a simple JSON HTTP API compatible with the rest of the framework.</p>"},{"location":"transports/#overview","title":"Overview","text":"<ul> <li>Client side</li> <li>Uses <code>httpx.AsyncClient</code> to send JSON requests.</li> <li> <p>Provides helpers to send a full <code>Task</code> or a single <code>Message</code>.</p> </li> <li> <p>Server side</p> </li> <li>Uses an ASGI app (Starlette or FastAPI) to expose:<ul> <li><code>POST /tasks/</code> \u2014 submit a <code>Task</code> to the agent.</li> <li><code>GET /.well-known/agent.json</code> \u2014 served by the agent itself, not the transport, but typically used together.</li> </ul> </li> <li>Uses a backend implementation of <code>BackendInterface</code> to manage the ASGI app and <code>uvicorn</code> server.</li> </ul>"},{"location":"transports/#backends-starlette-vs-fastapi","title":"Backends: Starlette vs FastAPI","text":"<p><code>HTTPTransport</code> delegates server behavior to a backend implementing <code>BackendInterface</code>:</p> <ul> <li>StarletteBackend (default)</li> <li>Minimal Starlette app with a single <code>POST /tasks/</code> route.</li> <li>No extra request validation beyond what <code>Task.from_dict()</code> does.</li> <li> <p>Best when you want low overhead and trust callers to send valid payloads.</p> </li> <li> <p>FastAPIBackend</p> </li> <li>FastAPI app with optional Pydantic models mirroring the <code>Task</code>/<code>Message</code>/<code>Artifact</code> structures.</li> <li>When <code>validate_schema=True</code>, incoming requests are validated against these models before being converted with <code>Task.from_dict()</code>.</li> <li>Best when you want schema validation and better generated OpenAPI / docs.</li> </ul> <p>Backend and validation are selected via the <code>HTTPTransport</code> constructor:</p> <pre><code>from protolink.transport.http_transport import HTTPTransport\n\n# Starlette backend (default)\ntransport = HTTPTransport()\n\n# Explicit Starlette backend\ntransport = HTTPTransport(backend=\"starlette\")\n\n# FastAPI backend without schema validation\ntransport = HTTPTransport(backend=\"fastapi\", validate_schema=False)\n\n# FastAPI backend with full schema validation\ntransport = HTTPTransport(backend=\"fastapi\", validate_schema=True)\n</code></pre>"},{"location":"transports/#wire-format","title":"Wire Format","text":"<p><code>HTTPTransport</code> sends and receives JSON payloads that match the core models' <code>to_dict()</code> methods. A typical <code>Task</code> request body looks like this:</p> <pre><code>{\n  \"id\": \"8c1e93b3-9f72-4a37-8c4c-3d2d8a9c4f7c\",\n  \"state\": \"submitted\",\n  \"messages\": [\n    {\n      \"id\": \"f0e4c2f7-5d3b-4b0a-b6e0-6a7f2d9c1b2a\",\n      \"role\": \"user\",\n      \"parts\": [\n        {\"type\": \"text\", \"content\": \"Hi Bob, how are you?\"}\n      ],\n      \"timestamp\": \"2025-01-01T12:00:00Z\"\n    }\n  ],\n  \"artifacts\": [],\n  \"metadata\": {},\n  \"created_at\": \"2025-01-01T12:00:00Z\"\n}\n</code></pre> <p>The tables below document each object type.</p>"},{"location":"transports/#task","title":"Task","text":"Field Type Description <code>id</code> <code>str</code> Unique task identifier. <code>state</code> <code>str</code> One of <code>\"submitted\"</code>, <code>\"working\"</code>, <code>\"completed\"</code>, etc. <code>messages</code> <code>list[Message]</code> Conversation history for this task. <code>artifacts</code> <code>list[Artifact]</code> Outputs produced by the task. <code>metadata</code> <code>dict[str, Any]</code> Arbitrary metadata attached to the task. <code>created_at</code> <code>str</code> ISO\u20118601 timestamp (UTC)."},{"location":"transports/#message","title":"Message","text":"<pre><code>{\n  \"id\": \"f0e4c2f7-5d3b-4b0a-b6e0-6a7f2d9c1b2a\",\n  \"role\": \"user\",\n  \"parts\": [\n    {\"type\": \"text\", \"content\": \"Hi Bob, how are you?\"}\n  ],\n  \"timestamp\": \"2025-01-01T12:00:00Z\"\n}\n</code></pre> Field Type Description <code>id</code> <code>str</code> Unique message identifier. <code>role</code> <code>\"user\" \\| \"agent\" \\| \"system\"</code> Sender role. <code>parts</code> <code>list[Part]</code> Content payloads. <code>timestamp</code> <code>str</code> ISO\u20118601 timestamp."},{"location":"transports/#part","title":"Part","text":"<pre><code>{\"type\": \"text\", \"content\": \"Hi Bob, how are you?\"}\n</code></pre> Field Type Description <code>type</code> <code>str</code> Content type (e.g. <code>\"text\"</code>). <code>content</code> <code>Any</code> The actual content payload."},{"location":"transports/#artifact","title":"Artifact","text":"<pre><code>{\n  \"artifact_id\": \"a1b2c3\",\n  \"parts\": [\n    {\"type\": \"text\", \"content\": \"final report\"}\n  ],\n  \"metadata\": {\"kind\": \"report\"},\n  \"created_at\": \"2025-01-01T12:00:00Z\"\n}\n</code></pre> Field Type Description <code>artifact_id</code> <code>str</code> Unique artifact identifier. <code>parts</code> <code>list[Part]</code> Artifact content. <code>metadata</code> <code>dict[str, Any]</code> Artifact metadata. <code>created_at</code> <code>str</code> ISO\u20118601 timestamp."},{"location":"transports/#typical-usage","title":"Typical Usage","text":""},{"location":"transports/#exposing-an-agent-over-http","title":"Exposing an agent over HTTP","text":"<pre><code>from protolink.agents import Agent\nfrom protolink.core.agent_card import AgentCard\nfrom protolink.core.message import Message\nfrom protolink.core.task import Task\nfrom protolink.transport.http_transport import HTTPTransport\n\n\nclass EchoAgent(Agent):\n    def __init__(self, port: int) -&gt; None:\n        card = AgentCard(\n            name=\"echo\", description=\"Echoes back the last user message\", url=f\"http://127.0.0.1:{port}\",\n        )\n        transport = HTTPTransport(host=\"127.0.0.1\", port=port)\n        super().__init__(card, transport=transport)\n\n    async def handle_task(self, task: Task) -&gt; Task:\n        last_msg = task.messages[-1]\n        reply = Message.agent(f\"echo: {last_msg.parts[0].content}\")\n        return Task(id=task.id, messages=task.messages + [reply])\n</code></pre> <p>Then run the agent and call it from another agent or client using <code>send_task</code> or <code>send_message</code>.</p>"},{"location":"transports/#calling-a-remote-agent","title":"Calling a remote agent","text":"<pre><code>from protolink.client.agent_client import AgentClient\nfrom protolink.core.message import Message\nfrom protolink.core.task import Task\nfrom protolink.transport.http_transport import HTTPTransport\n\n\nclient = AgentClient(HTTPTransport())\n\n\nasync def call_remote(url: str) -&gt; None:\n    hello = Task.create(Message.user(\"Hello over HTTP!\"))\n    result = await client.send_task(url, hello)\n    print(\"Response:\", result.messages[-1].parts[0].content)\n</code></pre>"},{"location":"transports/#httptransport-api-reference","title":"HTTPTransport API Reference","text":"<p>The most important public methods on <code>HTTPTransport</code> are summarized below.</p>"},{"location":"transports/#constructor-lifecycle","title":"Constructor &amp; lifecycle","text":"Name Parameters Returns Description <code>__init__</code> <code>host: str = \"0.0.0.0\"</code>, <code>port: int = 8000</code>, <code>timeout: float = 30.0</code>, <code>auth_provider: AuthProvider \\| None = None</code>, <code>backend: Literal[\"starlette\", \"fastapi\"] = \"starlette\"</code>, <code>validate_schema: bool = False</code> <code>None</code> Configure host/port, request timeout, optional authentication provider, backend implementation, and whether to enable FastAPI/Pydantic schema validation. <code>start</code> <code>self</code> <code>Awaitable[None]</code> Start the selected backend, register the <code>/tasks/</code> route and create the internal <code>httpx.AsyncClient</code>. Must be awaited before serving HTTP traffic. <code>stop</code> <code>self</code> <code>Awaitable[None]</code> Stop the backend server and close the internal HTTP client. Safe to call multiple times."},{"location":"transports/#sending-receiving","title":"Sending &amp; receiving","text":"Name Parameters Returns Description <code>on_task_received</code> <code>handler: Callable[[Task], Awaitable[Task]]</code> <code>None</code> Register the callback that will handle incoming tasks on <code>POST /tasks/</code>. This must be set before <code>start()</code> when running as a server. <code>send_task</code> <code>agent_url: str</code>, <code>task: Task</code>, <code>skill: str \\| None = None</code> <code>Awaitable[Task]</code> Send a <code>Task</code> to <code>POST {agent_url}/tasks/</code> and return the resulting <code>Task</code> from the remote agent. The optional <code>skill</code> is passed via headers and can be used by agents to route work. <code>send_message</code> <code>agent_url: str</code>, <code>message: Message</code> <code>Awaitable[Message]</code> Convenience wrapper that wraps a single <code>Message</code> in a new <code>Task</code>, calls <code>send_task</code>, and returns the last response message. Ideal for simple request/response interactions. <code>get_agent_card</code> <code>agent_url: str</code> <code>Awaitable[AgentCard]</code> Fetch the remote agent's <code>AgentCard</code> description from <code>GET {agent_url}/.well-known/agent.json</code>. Useful for discovery and capability inspection."},{"location":"transports/#auth-utilities","title":"Auth &amp; utilities","text":"Name Parameters Returns Description <code>authenticate</code> <code>credentials: str</code> <code>Awaitable[None]</code> Use the configured <code>AuthProvider</code> to obtain an auth context (for example, exchanging an API key for a bearer token). The resulting context is automatically injected into outgoing HTTP headers. <code>_build_headers</code> <code>skill: str \\| None = None</code> <code>dict[str, str]</code> Internal helper that constructs HTTP headers (including <code>Authorization</code> when an auth context is present). Exposed here for completeness; you normally do not need to call it directly. <code>validate_agent_url</code> <code>agent_url: str</code> <code>bool</code> Return <code>True</code> if the URL is considered local to this transport's host/port (e.g. for allow\u2011listing), <code>False</code> otherwise."},{"location":"transports/#runtimetransport","title":"RuntimeTransport","text":"<p><code>RuntimeTransport</code> is an in\u2011process, in\u2011memory transport used primarily for tests, local experimentation, and tightly\u2011coupled multi\u2011agent systems.</p> <p>Characteristics:</p> <ul> <li>No network hops, very low latency.</li> <li>Multiple agents share the same runtime transport instance.</li> <li>Ideal for composition and unit tests (see <code>tests/test_agent.py</code>).</li> </ul>"},{"location":"transports/#runtimetransport-api","title":"RuntimeTransport API","text":"Name Parameters Returns Description <code>__init__</code> <code>...</code> <code>None</code> Create an in\u2011memory transport registry for agents that live in the same Python process. <code>register</code> <code>agent</code> <code>None</code> Add an agent to the runtime transport so it can receive tasks from others. <code>unregister</code> <code>agent</code> <code>None</code> Remove an agent from the runtime transport. <code>send_task</code> <code>agent_id_or_url</code>, <code>task: Task</code> <code>Task \\| Awaitable[Task]</code> Dispatch a <code>Task</code> to another agent registered on the same runtime transport instance. Exact return type depends on the concrete implementation. <code>start</code> / <code>stop</code> <code>self</code> <code>None</code> Often no\u2011op or light\u2011weight setup/teardown. Provided for a consistent lifecycle API with other transports."},{"location":"transports/#websockettransport","title":"WebSocketTransport","text":"<p><code>WebSocketTransport</code> (when available) provides streaming, bidirectional communication between agents or between agents and external clients.</p> <p>Use it when:</p> <ul> <li>You need token\u2011level or chunk\u2011level streaming.</li> <li>You want long\u2011lived interactive sessions (chat UIs, dashboards, tools that stream output).</li> </ul>"},{"location":"transports/#websockettransport-api","title":"WebSocketTransport API","text":"Name Parameters Returns Description <code>__init__</code> <code>...</code> <code>None</code> Configure host/port and WebSocket settings for streaming connections. <code>send_task_stream</code> <code>...</code> <code>AsyncIterator[Task] \\| AsyncIterator[Message]</code> Send a <code>Task</code> and receive a stream of partial results or updates over a single WebSocket connection. (Exact type depends on implementation.) <code>start</code> / <code>stop</code> <code>self</code> <code>Awaitable[None]</code> Start or stop the WebSocket server."},{"location":"transports/#planned-transports","title":"Planned Transports","text":"<p>These transports are not implemented yet in the core library. The sections below describe the intended design so you can plan ahead, but there is currently no public API to import.</p> <p>Status: Design sketches only. Do not rely on these in production code.</p>"},{"location":"transports/#jsonrpctransport-planned","title":"JSONRPCTransport (planned)","text":"<ul> <li>JSON\u2011RPC 2.0 style envelope for structured requests and responses.</li> <li>Strong separation of methods, params, and results.</li> <li>Natural fit for RPC\u2011style integrations.</li> </ul>"},{"location":"transports/#jsonrpctransport-design","title":"JSONRPCTransport design","text":"Name Parameters Returns Description <code>send_request</code> <code>method: str</code>, <code>params: dict</code> <code>Awaitable[dict]</code> (Planned) Send a JSON\u2011RPC request and return the decoded result payload. <code>notify</code> <code>method: str</code>, <code>params: dict</code> <code>Awaitable[None]</code> (Planned) Fire\u2011and\u2011forget notification without a response. <code>start</code> / <code>stop</code> <code>self</code> <code>Awaitable[None]</code> (Planned) Start/stop the JSON\u2011RPC server."},{"location":"transports/#grpctransport-planned","title":"GRPCTransport (planned)","text":"<ul> <li>gRPC\u2011based transport with protobuf definitions for tasks and messages.</li> <li>High\u2011performance, strongly\u2011typed, streaming\u2011friendly.</li> </ul>"},{"location":"transports/#grpctransport-design","title":"GRPCTransport design","text":"Name Parameters Returns Description <code>send_task</code> <code>...</code> <code>Awaitable[...]</code> (Planned) Unary RPC for sending tasks. <code>send_task_stream</code> <code>...</code> <code>AsyncIterator[...]</code> (Planned) Streaming RPC for long\u2011running tasks and progress updates. <code>start</code> / <code>stop</code> <code>self</code> <code>Awaitable[None]</code> (Planned) Start/stop the gRPC server."}]}