{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p>A lightweight, production-ready framework for agent-to-agent communication, built on and extending Google's A2A protocol.</p> <p>Get Started View on GitHub</p> <p>Welcome to the Protolink documentation.</p> <p>This site provides an overview of the framework, its concepts, and how to use it in your projects.</p> <p>Current release: see protolink on PyPI. </p> <p> </p>"},{"location":"#contents","title":"Contents","text":"<ul> <li>Getting Started</li> <li>Agents</li> <li>Transports</li> <li>LLMs</li> <li>Tools</li> <li>Examples</li> </ul>"},{"location":"#what-you-can-do-with-protolink","title":"What you can do with Protolink","text":"<ul> <li> <p>Build agents quickly   See Getting Started and Agents for the core concepts and basic setup.</p> </li> <li> <p>Choose your transport   Explore Transports to switch between HTTP, WebSocket, runtime, and future transports with minimal code changes.</p> </li> <li> <p>Plug in LLMs &amp; tools   Use LLMs and Tools to wire in language models and both native &amp; MCP tools as agent modules.</p> </li> </ul>"},{"location":"#overview","title":"Overview","text":"<p>Protolink is a lightweight, production\u2011ready framework for agent\u2011to\u2011agent communication, built around and extending Google\u2019s A2A protocol.</p> <p>Key ideas:</p> <ul> <li>Unified Agent model: a single <code>Agent</code> instance handles both client and server responsibilities.</li> <li>Flexible transports: HTTP, WebSocket, in\u2011process runtime, and planned JSON\u2011RPC / gRPC transports.</li> <li>LLM\u2011ready architecture: first\u2011class integration with API, local, and server\u2011hosted LLMs.</li> <li>Tools as modules: native Python tools and MCP tools plugged directly into agents.</li> </ul> <p>Use this documentation to:</p> <ul> <li>Install Protolink and run your first agent.</li> <li>Understand how agents, transports, LLMs, and tools fit together.</li> <li>Explore practical examples you can adapt to your own systems.</li> </ul> <p>Protolink is open source under the MIT license. Contributions are welcome \u2013 see the repository\u2019s Contributing section on GitHub.</p>"},{"location":"agents/","title":"Agents","text":"<p>Agents are the core building blocks in Protolink.</p>"},{"location":"agents/#concepts","title":"Concepts","text":"<p>An Agent in Protolink is a unified component that can act as both client and server. This is different from the original A2A spec, which separates client and server concerns.</p> <p>High\u2011level ideas:</p> <ul> <li>Unified model: a single <code>Agent</code> instance can send and receive messages.</li> <li>AgentCard: a small model describing the agent (name, description, metadata).</li> <li>Modules:<ul> <li>LLMs (e.g. <code>OpenAILLM</code>, <code>AnthropicLLM</code>, <code>LlamaCPPLLM</code>, <code>OllamaLLM</code>).</li> <li>Tools (native Python functions or MCP\u2011backed tools).</li> </ul> </li> <li>Transport abstraction: agents communicate over transports such as HTTP, WebSocket, gRPC, or the in\u2011process runtime transport.</li> </ul>"},{"location":"agents/#creating-an-agent","title":"Creating an Agent","text":"<p>A minimal agent consists of three pieces:</p> <ol> <li>An <code>AgentCard</code> describing the agent.</li> <li>A <code>Transport</code> implementation.</li> <li>An optional LLM and tools.</li> </ol> <p>Example:</p> <pre><code>from protolink.agents import Agent\nfrom protolink.models import AgentCard\nfrom protolink.transport import HTTPTransport\nfrom protolink.llms.api import OpenAILLM\n\n\nagent_card = AgentCard(\n    name=\"example_agent\",\n    description=\"A dummy agent\",\n)\n\ntransport = HTTPTransport()\nllm = OpenAILLM(model=\"gpt-5.2\")\n\nagent = Agent(agent_card, transport, llm)\n</code></pre> <p>You can then attach tools and start the agent.</p>"},{"location":"agents/#agent-to-agent-communication","title":"Agent-to-Agent Communication","text":"<p>Agents communicate over a chosen transport.</p> <p>Common patterns:</p> <ul> <li>RuntimeTransport: multiple agents in the same process share an in\u2011memory transport, which is ideal for local testing and composition.</li> <li>HTTPTransport / WebSocketTransport: agents expose HTTP or WebSocket endpoints so that other agents (or external clients) can send requests.</li> <li>gRPC / JSON\u2011RPC (planned): additional transports for more structured or high\u2011performance communication.</li> </ul> <p>From the framework\u2019s perspective, all of these are implementations of the same transport interface, so you can swap them with minimal code changes.</p>"},{"location":"agents/#agent-api-reference","title":"Agent API Reference","text":"<p>This section provides a detailed API reference for the <code>Agent</code> base class in <code>protolink.agents.base</code>. The <code>Agent</code> class is the core component for creating A2A-compatible agents, serving as both client and server.</p> <p>Unified Agent Model</p> <p>Unlike the original A2A specification, Protolink's <code>Agent</code> combines client and server functionality in a single class. You can send tasks/messages to other agents while also serving incoming requests.</p>"},{"location":"agents/#constructor","title":"Constructor","text":"Parameter Type Default Description <code>card</code> <code>AgentCard</code> \u2014 Required. The agent's metadata card containing name, description, and other identifying information. <code>llm</code> <code>LLM | None</code> <code>None</code> Optional language model instance for AI-powered task processing. <code>transport</code> <code>Transport | None</code> <code>None</code> Optional transport for communication. If not provided, you must set one later via <code>set_transport()</code>. <code>auth_provider</code> <code>AuthProvider | None</code> <code>None</code> Optional authentication provider for securing agent communications. <code>skills</code> <code>Literal[\"auto\", \"fixed\"]</code> <code>\"auto\"</code> Skills mode - <code>\"auto\"</code> to automatically detect and add skills, <code>\"fixed\"</code> to use only the skills defined by the user in the AgentCard. <pre><code>from protolink.agents import Agent\nfrom protolink.models import AgentCard\nfrom protolink.transport import HTTPTransport\nfrom protolink.llms.api import OpenAILLM\n\ncard = AgentCard(name=\"my_agent\", description=\"Example agent\")\nllm = OpenAILLM(model=\"gpt-4\")\ntransport = HTTPTransport()\n\nagent = Agent(card=card, llm=llm, transport=transport)\n</code></pre>"},{"location":"agents/#lifecycle-methods","title":"Lifecycle Methods","text":"<p>These methods control the agent's server component lifecycle.</p> Name Parameters Returns Description <code>start()</code> \u2014 <code>None</code> Starts the agent's server component if a transport is configured. <code>stop()</code> \u2014 <code>None</code> Stops the agent's server component and cleans up resources. <p>Transport Required</p> <p>The <code>start()</code> method requires a transport to be configured. If no transport was provided during construction, call <code>set_transport()</code> first.</p>"},{"location":"agents/#transport-management","title":"Transport Management","text":"Name Parameters Returns Description <code>set_transport()</code> <code>transport: Transport</code> <code>None</code> Sets or updates the transport used by this agent. <code>client</code> (property) \u2014 <code>AgentClient</code> Returns the client instance for sending requests to other agents. <code>server</code> (property) \u2014 <code>Server \\| None</code> Returns the server instance if one is available via the transport."},{"location":"agents/#task-and-message-handling","title":"Task and Message Handling","text":""},{"location":"agents/#core-task-processing","title":"Core Task Processing","text":"Name Parameters Returns Description <code>handle_task()</code> <code>task: Task</code> <code>Task</code> Abstract method. Subclasses must implement this to define how tasks are processed. <code>handle_task_streaming()</code> <code>task: Task</code> <code>AsyncIterator[Task]</code> Optional streaming handler for real-time task updates. Default raises <code>NotImplementedError</code>."},{"location":"agents/#communication-methods","title":"Communication Methods","text":"Name Parameters Returns Description <code>send_task_to()</code> <code>agent_url: str</code>, <code>task: Task</code>, <code>skill: str \\| None = None</code> <code>Task</code> Sends a task to another agent and returns the processed result. <code>send_message_to()</code> <code>agent_url: str</code>, <code>message: Message</code> <code>Message</code> Sends a message to another agent and returns the response. <p>Authentication</p> <p>All outgoing requests are automatically signed if an <code>auth_provider</code> is configured. Incoming requests are verified against the same provider.</p>"},{"location":"agents/#skills-management","title":"Skills Management","text":"<p>Skills represent the capabilities that an agent can perform. Skills are stored in the <code>AgentCard</code> and can be automatically detected or added.</p>"},{"location":"agents/#skills-modes","title":"Skills Modes","text":"Mode Description <code>\"auto\"</code> Automatically detects skills from tools and public methods, and adds them to the AgentCard <code>\"fixed\"</code> Uses only the skills explicitly defined in the AgentCard"},{"location":"agents/#skill-detection","title":"Skill Detection","text":"<p>When using <code>\"auto\"</code> mode, the agent detects skills from:</p> <ol> <li>Tools: Each registered tool becomes a skill</li> <li>Public Methods: Optional detection of public methods (controlled by <code>include_public_methods</code> parameter)</li> </ol> <pre><code># Auto-detect skills from tools only\nagent = Agent(card, skills=\"auto\")\n\n# Use only skills defined in AgentCard\nagent = Agent(card, skills=\"fixed\")\n</code></pre>"},{"location":"agents/#skills-in-agentcard","title":"Skills in AgentCard","text":"<p>Skills are persisted in the AgentCard and serialized when the card is exported to JSON:</p> <pre><code>from protolink.models import AgentCard, AgentSkill\n\n# Create skills manually in AgentCard\ncard = AgentCard(\n    name=\"weather_agent\",\n    description=\"Weather information agent\",\n    skills=[\n        AgentSkill(\n            id=\"get_weather\",\n            description=\"Get current weather for a location\",\n            tags=[\"weather\", \"forecast\"],\n            examples=[\"What's the weather in New York?\"]\n        )\n    ]\n)\n\n# Use fixed mode to only use these skills\nagent = Agent(card, skills=\"fixed\")\n</code></pre>"},{"location":"agents/#tool-management","title":"Tool Management","text":"<p>Tools allow agents to execute external functions and APIs.</p> Name Parameters Returns Description <code>add_tool()</code> <code>tool: BaseTool</code> <code>None</code> Registers a tool with the agent and automatically adds it as a skill to the AgentCard. <code>tool()</code> <code>name: str</code>, <code>description: str</code> <code>decorator</code> Decorator for registering Python functions as tools (automatically adds as skills). <code>call_tool()</code> <code>tool_name: str</code>, <code>**kwargs</code> <code>Any</code> Executes a registered tool by name with provided arguments. <pre><code># Using the decorator approach\n@agent.tool(\"calculate\", \"Performs basic calculations\")\ndef calculate(operation: str, a: float, b: float) -&gt; float:\n    if operation == \"add\":\n        return a + b\n    elif operation == \"multiply\":\n        return a * b\n    else:\n        raise ValueError(f\"Unsupported operation: {operation}\")\n\n# Direct tool registration\nfrom protolink.tools import BaseTool\n\nclass WeatherTool(BaseTool):\n    def call(self, location: str) -&gt; dict:\n        # Weather API logic here\n        return {\"temperature\": 72, \"conditions\": \"sunny\"}\n\nagent.add_tool(WeatherTool())\n</code></pre>"},{"location":"agents/#utility-methods","title":"Utility Methods","text":"Name Parameters Returns Description <code>get_agent_card()</code> \u2014 <code>AgentCard</code> Returns the agent's metadata card. <code>set_llm()</code> <code>llm: LLM</code> <code>None</code> Updates the agent's language model instance. <code>verify_auth()</code> <code>request: Request</code> <code>bool</code> Verifies authentication for incoming requests if an auth provider is configured."},{"location":"agents/#abstract-methods","title":"Abstract Methods","text":"<p>Subclasses of <code>Agent</code> must implement the following methods:</p> <ul> <li><code>handle_task(task: Task) -&gt; Task</code>: Defines the core logic for processing incoming tasks.</li> </ul> <p>Minimal Agent Implementation</p> <pre><code>from protolink.agents import Agent\nfrom protolink.models import AgentCard, Task, Message\n\nclass EchoAgent(Agent):\n    async def handle_task(self, task: Task) -&gt; Task:\n        # Echo back all messages\n        response_messages = []\n        for message in task.messages:\n            response_messages.append(\n                Message(\n                    content=f\"Echo: {message.content}\",\n                    role=\"assistant\"\n                )\n            )\n\n        return Task(\n            messages=response_messages,\n            parent_task_id=task.id\n        )\n</code></pre>"},{"location":"agents/#error-handling","title":"Error Handling","text":"<p>The <code>Agent</code> class includes several error handling patterns:</p> <ul> <li>Missing Transport: Raises <code>ValueError</code> if trying to start without a transport.</li> <li>Authentication Failures: Returns <code>401</code> or <code>403</code> responses for invalid auth.</li> <li>Tool Errors: Tool execution errors are propagated to the caller.</li> <li>Task Processing: Errors in <code>handle_task()</code> are caught and returned as error messages to the sender.</li> </ul>"},{"location":"agents/#authentication-integration","title":"Authentication Integration","text":"<p>When an <code>auth_provider</code> is configured, the agent automatically:</p> <ol> <li>Signs outgoing requests with appropriate authentication headers</li> <li>Verifies incoming requests using the same auth mechanism</li> <li>Returns appropriate HTTP status codes for auth failures (401, 403)</li> </ol> <p>Supported auth providers include API key authentication, OAuth, and custom implementations.</p>"},{"location":"examples/","title":"Examples","text":"<p>This section links to example projects and code snippets in the repository.</p>"},{"location":"examples/#http-agents","title":"HTTP Agents","text":"<p>The repository includes several examples under the <code>examples/</code> directory. For HTTP\u2011based agents:</p> <ul> <li><code>examples/http_agents.py</code> \u2014 basic HTTP transport example showing how to spin up an HTTP\u2011enabled agent.</li> <li><code>examples/http_math_agents.py</code> \u2014 example of delegating between agents over HTTP (e.g. a question agent calling a math agent).</li> </ul>"},{"location":"examples/#other-examples","title":"Other Examples","text":"<p>Additional examples illustrate other capabilities:</p> <ul> <li><code>examples/basic_agent.py</code> \u2014 minimal agent setup focused on core concepts.</li> <li><code>examples/llms.py</code> \u2014 examples of wiring different LLM backends into agents.</li> <li><code>examples/runtime_agents.py</code> \u2014 demonstrates using <code>RuntimeTransport</code> for in\u2011process agent communication.</li> <li><code>examples/streaming_agent.py</code> \u2014 shows streaming behaviour (e.g. via WebSocket or other streaming\u2011capable transports).</li> <li><code>examples/oauth_agent.py</code> \u2014 demonstrates OAuth 2.0 and API\u2011key based security in front of agents.</li> </ul> <p>You can run and adapt these scripts as starting points for your own agent systems.</p> <p>New here?</p> <p>Start with <code>examples/basic_agent.py</code> to understand the core concepts, then move on to <code>examples/http_agents.py</code> for HTTP-based setups.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide shows how to install and start using Protolink.</p>"},{"location":"getting-started/#installation","title":"Installation","text":"<p>Protolink is published on PyPI and can be installed with either <code>uv</code> (recommended) or <code>pip</code>.</p>"},{"location":"getting-started/#basic-installation","title":"Basic Installation","text":"<p>This installs the base package without optional extras:</p> <pre><code># Using uv (recommended)\nuv add protolink\n\n# Using pip\npip install protolink\n</code></pre>"},{"location":"getting-started/#optional-dependencies","title":"Optional Dependencies","text":"<p>Protolink exposes several extras to enable additional functionality:</p> <pre><code># Install with all optional dependencies\nuv add \"protolink[all]\"\n\n# HTTP support (for web-based agents)\nuv add \"protolink[http]\"\n\n# All supported LLM libraries\nuv add \"protolink[llms]\"\n\n# Development (all extras + testing tools)\nuv add \"protolink[dev]\"\n</code></pre> <p>Optional extras</p> <p>You usually only need the extras that match your use case. The <code>protolink[llms]</code> will install all the supported LLM libraries (OpenAI, Anthropic, Ollama etc.) so it is advised to install manually the libraries that are needed for your project.</p> <p>For development from source:</p> <pre><code>git clone https://github.com/nmaroulis/protolink.git\ncd protolink\nuv pip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/#basic-example","title":"Basic Example","text":"<p>Below is a minimal example that wires together an agent, HTTP transport, an OpenAI LLM, and both native and MCP tools:</p> <pre><code>from protolink.agents import Agent\nfrom protolink.models import AgentCard\nfrom protolink.transport import HTTPTransport\nfrom protolink.tools import MCPToolAdapter\nfrom protolink.llms.api import OpenAILLM\n\n\n# Define the agent card\nagent_card = AgentCard(\n    name=\"example_agent\",\n    description=\"A dummy agent\",\n)\n\n\n# Initialize the transport\ntransport = HTTPTransport()\n\n\n# OpenAI API LLM\nllm = OpenAILLM(model=\"gpt-5.2\")\n\n\n# Initialize the agent\nagent = Agent(agent_card, transport, llm)\n\n\n# Add Native tool\n@agent.tool(name=\"add\", description=\"Add two numbers\")\nasync def add_numbers(a: int, b: int):\n    return a + b\n\n\n# Add MCP tool\nmcp_tool = MCPToolAdapter(mcp_client, \"multiply\")\nagent.add_tool(mcp_tool)\n\n\n# Start the agent\nagent.start()\n</code></pre> <p>This example demonstrates the core pieces of Protolink:</p> <ul> <li>AgentCard to describe the agent.</li> <li>Transport (here <code>HTTPTransport</code>) to handle communication.</li> <li>LLM backend (<code>OpenAILLM</code>).</li> <li>Native tools (Python functions decorated with <code>@agent.tool</code>).</li> <li>MCP tools registered via <code>MCPToolAdapter</code>.</li> </ul>"},{"location":"llms/","title":"LLMs","text":"<p>Protolink integrates with various LLM backends.</p>"},{"location":"llms/#llm-types","title":"LLM Types","text":"<p>Protolink groups LLM backends into three broad categories:</p>    [ API ]   [ Server ]   [ Local ]  <ul> <li> <p>API \u2014 calls a remote API and requires an API key:</p> <ul> <li><code>OpenAILLM</code>: uses the OpenAI API for sync &amp; async requests.</li> <li><code>AnthropicLLM</code>: uses the Anthropic API for sync &amp; async requests.</li> </ul> </li> <li> <p>Server \u2014 connects to an LLM server, locally or remotely:</p> <ul> <li><code>OllamaLLM</code>: connects to an Ollama server for sync &amp; async requests.</li> </ul> </li> <li> <p>Local \u2014 runs the model directly in your runtime:</p> <ul> <li><code>LlamaCPPLLM</code>: uses a local llama.cpp runtime for sync &amp; async requests.</li> </ul> </li> </ul> <p>You can also use other LLM clients directly without going through Protolink\u2019s <code>LLM</code> wrappers if you prefer.</p>"},{"location":"llms/#configuration","title":"Configuration","text":"<p>Configuration depends on the specific backend, but the general pattern is:</p> <ol> <li>Install the relevant extras (from the README):</li> </ol> <pre><code># All supported LLM backends\nuv add \"protolink[llms]\"\n</code></pre> <p>Choosing LLM extras</p> <p>If you only need a subset of LLMs (e.g. OpenAI API), it is advised to install them manually instead of using the <code>llms</code> extra, which will intall all the supported libraries.</p> <ol> <li>Instantiate the LLM with the desired model and credentials:</li> </ol> <pre><code>from protolink.llms.api import OpenAILLM\n\n\nllm = OpenAILLM(\n    model=\"gpt-5.2\",\n    # api_key is typically read from the environment, e.g. OPENAI_API_KEY\n)\n</code></pre> <p>API keys</p> <p>Never commit API keys to version control. Read them from environment variables or a secure secrets manager.</p> <ol> <li>Pass the LLM to your Agent:</li> </ol> <pre><code>from protolink.agents import Agent\nfrom protolink.models import AgentCard\nfrom protolink.transport import HTTPTransport\n\n\nagent_card = AgentCard(name=\"llm_agent\", description=\"Agent backed by an LLM\")\ntransport = HTTPTransport()\n\nagent = Agent(agent_card, transport, llm)\n</code></pre> <p>For local and server\u2011style LLMs (<code>LlamaCPPLLM</code>, <code>OllamaLLM</code>), configuration additionally includes paths to model files or server URLs. Refer to the corresponding example scripts in <code>examples/llms.py</code> for concrete usage patterns.</p>"},{"location":"llms/#llm-api-reference","title":"LLM API Reference","text":"<p>This section provides a detailed API reference for all LLM classes in Protolink. All LLM implementations inherit from the base <code>LLM</code> class and provide a consistent interface for generating responses.</p> <p>Unified LLM Interface</p> <p>Protolink provides a single, consistent API for all LLM providers. Whether you're using OpenAI, Anthropic, Ollama, or local models, you interact with them through the same methods: <code>generate_response()</code>, <code>generate_stream_response()</code>, and configuration helpers. This unified approach means you can swap LLM providers without changing your application code - just update the initialization and you're done!</p> <p>Why Use Protolink's LLM Wrappers?</p> <ul> <li>Provider Agnostic: Switch between OpenAI, Anthropic, Ollama, and future providers with minimal code changes</li> <li>Consistent Interface: Same method signatures and behavior across all implementations</li> <li>Built-in Features: Connection validation, parameter validation, and error handling out of the box</li> <li>Extensible: Easy to add new LLM providers while maintaining compatibility</li> <li>Production Ready: Robust error handling and logging for real-world applications</li> </ul> <p>Provider Switching in Action</p> <pre><code># The same code works with ANY LLM provider\n\n# Choose your provider - just change the import and initialization!\nfrom protolink.llms.api import OpenAILLM    # or AnthropicLLM\nfrom protolink.llms.server import OllamaLLM  # or any other provider\n\n# Initialize your chosen LLM\nllm = OpenAILLM(model=\"gpt-4\", temperature=0.7)\n# llm = AnthropicLLM(model=\"claude-3-sonnet\", temperature=0.7)\n# llm = OllamaLLM(model=\"llama3\", temperature=0.7)\n\n# The rest of your code stays EXACTLY the same!\nmessages = [Message(role=\"user\", content=\"Hello!\")]\nresponse = llm.generate_response(messages)\nprint(response.content)\n\n# Streaming also works identically\nfor chunk in llm.generate_stream_response(messages):\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <p>LLM Hierarchy</p> <ul> <li><code>LLM</code> - abstract base class</li> <li><code>APILLM</code> - base for API-based LLMs</li> <li><code>ServerLLM</code> - base for server-based LLMs</li> <li><code>LocalLLM</code> - base for local runtime LLMs</li> <li>Concrete implementations: <code>OpenAILLM</code>, <code>AnthropicLLM</code>, <code>OllamaLLM</code>, etc.</li> </ul>"},{"location":"llms/#base-llm-class","title":"Base LLM Class","text":"<p>The <code>LLM</code> class defines the common interface that all LLM implementations must follow.</p>"},{"location":"llms/#attributes","title":"Attributes","text":"Attribute Type Description <code>model_type</code> <code>LLMType</code> The type of LLM (<code>\"api\"</code>, <code>\"local\"</code>, or <code>\"server\"</code>). <code>provider</code> <code>LLMProvider</code> The provider name (<code>\"openai\"</code>, <code>\"anthropic\"</code>, <code>\"ollama\"</code>, etc.). <code>model</code> <code>str</code> The model name/identifier. <code>model_params</code> <code>dict[str, Any]</code> Model-specific parameters (temperature, max_tokens, etc.). <code>system_prompt</code> <code>str</code> Default system prompt for the model."},{"location":"llms/#core-methods","title":"Core Methods","text":"Name Parameters Returns Description <code>generate_response()</code> <code>messages: list[Message]</code> <code>Message</code> Generate a single response from the model. <code>generate_stream_response()</code> <code>messages: list[Message]</code> <code>Iterable[Message]</code> Generate a streaming response, yielding messages as they're generated. <code>set_model_params()</code> <code>model_params: dict[str, Any]</code> <code>None</code> Update model parameters. <code>set_system_prompt()</code> <code>system_prompt: str</code> <code>None</code> Set the system prompt for the model. <code>validate_connection()</code> \u2014 <code>bool</code> Validate that the LLM connection is working. <p>Abstract Methods</p> <p>The <code>LLM</code> base class is abstract. You should use one of the concrete implementations like <code>OpenAILLM</code> or <code>AnthropicLLM</code>.</p>"},{"location":"llms/#api-based-llms","title":"API-based LLMs","text":"<p>API-based LLMs connect to external services and require API keys or authentication.</p>"},{"location":"llms/#apillm-base-class","title":"APILLM Base Class","text":"<p>Base class for all API-based LLM implementations.</p> Name Parameters Returns Description <code>set_model_params()</code> <code>model_params: dict[str, Any]</code> <code>None</code> Update existing model parameters, ignoring invalid keys. <code>set_system_prompt()</code> <code>system_prompt: str</code> <code>None</code> Set the system prompt for the model. <code>validate_connection()</code> \u2014 <code>bool</code> Abstract. Validate API connection (implemented by subclasses)."},{"location":"llms/#openaillm","title":"OpenAILLM","text":"<p>OpenAI API implementation using the official OpenAI client.</p>"},{"location":"llms/#constructor","title":"Constructor","text":"Parameter Type Default Description <code>api_key</code> <code>str \\| None</code> <code>None</code> OpenAI API key. If not provided, uses <code>OPENAI_API_KEY</code> environment variable. <code>model</code> <code>str \\| None</code> <code>\"gpt-5\"</code> OpenAI model name. <code>model_params</code> <code>dict[str, Any] \\| None</code> <code>None</code> Model parameters (temperature, max_tokens, etc.). <code>base_url</code> <code>str \\| None</code> <code>None</code> Custom base URL for OpenAI-compatible APIs. <pre><code>from protolink.llms.api import OpenAILLM\n\n# Basic usage\nllm = OpenAILLM(model=\"gpt-4\")\n\n# With custom parameters\nllm = OpenAILLM(\n    model=\"gpt-4-turbo\",\n    model_params={\n        \"temperature\": 0.7,\n        \"max_tokens\": 1000,\n        \"top_p\": 0.9\n    }\n)\n\n# With custom base URL (for OpenAI-compatible APIs)\nllm = OpenAILLM(\n    model=\"custom-model\",\n    base_url=\"https://api.custom-provider.com/v1\",\n    api_key=\"your-api-key\"\n)\n</code></pre>"},{"location":"llms/#default-model-parameters","title":"Default Model Parameters","text":"Parameter Type Default Range/Description <code>temperature</code> <code>float</code> <code>1.0</code> <code>0.0</code> to <code>2.0</code> - Controls randomness <code>top_p</code> <code>float</code> <code>1.0</code> Nucleus sampling parameter <code>n</code> <code>int</code> <code>1</code> Number of completions to generate <code>stream</code> <code>bool</code> <code>False</code> Whether to stream responses <code>stop</code> <code>str \\| list[str] \\| None</code> <code>None</code> Stop sequences <code>max_tokens</code> <code>int \\| None</code> <code>None</code> Maximum tokens to generate <code>presence_penalty</code> <code>float</code> <code>0.0</code> <code>-2.0</code> to <code>2.0</code> - Presence penalty <code>frequency_penalty</code> <code>float</code> <code>0.0</code> <code>-2.0</code> to <code>2.0</code> - Frequency penalty <code>logit_bias</code> <code>dict \\| None</code> <code>None</code> Token bias dictionary"},{"location":"llms/#methods","title":"Methods","text":"Name Parameters Returns Description <code>generate_response()</code> <code>messages: list[Message]</code> <code>Message</code> Generate a single response using OpenAI's API. <code>generate_stream_response()</code> <code>messages: list[Message]</code> <code>Iterable[Message]</code> Generate streaming response, yielding partial messages. <code>validate_connection()</code> \u2014 <code>bool</code> Check if the model is available and API key is valid. <p>API Key Required</p> <p>OpenAI requires a valid API key. Set the <code>OPENAI_API_KEY</code> environment variable or pass the <code>api_key</code> parameter.</p>"},{"location":"llms/#anthropicllm","title":"AnthropicLLM","text":"<p>Anthropic Claude API implementation using the official Anthropic client.</p>"},{"location":"llms/#constructor_1","title":"Constructor","text":"Parameter Type Default Description <code>api_key</code> <code>str \\| None</code> <code>None</code> Anthropic API key. If not provided, uses <code>ANTHROPIC_API_KEY</code> environment variable. <code>model</code> <code>str \\| None</code> <code>\"claude-sonnet-4-20250514\"</code> Claude model name. <code>model_params</code> <code>dict[str, Any] \\| None</code> <code>None</code> Model parameters (temperature, max_tokens, etc.). <code>base_url</code> <code>str \\| None</code> <code>None</code> Custom base URL for Anthropic-compatible APIs. <pre><code>from protolink.llms.api import AnthropicLLM\n\n# Basic usage\nllm = AnthropicLLM(model=\"claude-3-5-sonnet-20241022\")\n\n# With custom parameters\nllm = AnthropicLLM(\n    model=\"claude-3-5-haiku-20241022\",\n    model_params={\n        \"temperature\": 0.5,\n        \"max_tokens\": 2000,\n        \"top_p\": 0.8\n    }\n)\n</code></pre>"},{"location":"llms/#default-model-parameters_1","title":"Default Model Parameters","text":"Parameter Type Default Range/Description <code>max_tokens</code> <code>int</code> <code>4096</code> Maximum tokens to generate <code>temperature</code> <code>float</code> <code>1.0</code> <code>0.0</code> to <code>1.0</code> - Controls randomness <code>top_p</code> <code>float</code> <code>1.0</code> Nucleus sampling parameter <code>top_k</code> <code>int \\| None</code> <code>None</code> Top-k sampling parameter <code>stop_sequences</code> <code>list[str] \\| None</code> <code>None</code> Stop sequences <code>metadata</code> <code>dict \\| None</code> <code>None</code> Additional metadata"},{"location":"llms/#methods_1","title":"Methods","text":"Name Parameters Returns Description <code>generate_response()</code> <code>messages: list[Message]</code> <code>Message</code> Generate a single response using Anthropic's API. <code>generate_stream_response()</code> <code>messages: list[Message]</code> <code>Iterable[Message]</code> Generate streaming response, yielding partial messages. <code>validate_connection()</code> \u2014 <code>bool</code> Check if the model is available and API key is valid. <p>API Key Required</p> <p>Anthropic requires a valid API key. Set the <code>ANTHROPIC_API_KEY</code> environment variable or pass the <code>api_key</code> parameter.</p>"},{"location":"llms/#server-based-llms","title":"Server-based LLMs","text":"<p>Server-based LLMs connect to local or remote LLM servers.</p>"},{"location":"llms/#serverllm-base-class","title":"ServerLLM Base Class","text":"<p>Base class for all server-based LLM implementations.</p>"},{"location":"llms/#constructor_2","title":"Constructor","text":"Parameter Type Default Description <code>base_url</code> <code>str</code> \u2014 Required. URL of the LLM server."},{"location":"llms/#methods_2","title":"Methods","text":"Name Parameters Returns Description <code>set_model_params()</code> <code>model_params: dict[str, Any]</code> <code>None</code> Update existing model parameters, ignoring invalid keys. <code>set_system_prompt()</code> <code>system_prompt: str</code> <code>None</code> Set the system prompt for the model. <code>validate_connection()</code> \u2014 <code>bool</code> Validate that the server is reachable."},{"location":"llms/#ollamallm","title":"OllamaLLM","text":"<p>Ollama server implementation for connecting to local or remote Ollama instances.</p>"},{"location":"llms/#constructor_3","title":"Constructor","text":"Parameter Type Default Description <code>base_url</code> <code>str \\| None</code> <code>None</code> Ollama server URL. If not provided, uses <code>OLLAMA_HOST</code> environment variable. <code>headers</code> <code>dict[str, str] \\| None</code> <code>None</code> Additional HTTP headers (including auth). <code>model</code> <code>str \\| None</code> <code>\"gemma3\"</code> Ollama model name. <code>model_params</code> <code>dict[str, Any] \\| None</code> <code>None</code> Model parameters (temperature, etc.). <pre><code>from protolink.llms.server import OllamaLLM\n\n# Local Ollama server\nllm = OllamaLLM(\n    base_url=\"http://localhost:11434\",\n    model=\"llama3\"\n)\n\n# Remote Ollama with authentication\nllm = OllamaLLM(\n    base_url=\"https://ollama.example.com\",\n    headers={\"Authorization\": \"Bearer your-token\"},\n    model=\"codellama\"\n)\n\n# Using environment variables\n# Set OLLAMA_HOST=http://localhost:11434\nllm = OllamaLLM(model=\"mistral\")\n</code></pre>"},{"location":"llms/#default-model-parameters_2","title":"Default Model Parameters","text":"Parameter Type Default Description <code>temperature</code> <code>float</code> <code>1.0</code> Controls randomness (range depends on model)."},{"location":"llms/#methods_3","title":"Methods","text":"Name Parameters Returns Description <code>generate_response()</code> <code>messages: list[Message]</code> <code>Message</code> Generate a single response using Ollama's API. <code>generate_stream_response()</code> <code>messages: list[Message]</code> <code>Iterable[Message]</code> Generate streaming response, yielding partial messages. <code>validate_connection()</code> \u2014 <code>bool</code> Check if Ollama server is reachable and has models available. <p>Ollama Server Required</p> <p>OllamaLLM requires a running Ollama server. Install Ollama and start it with <code>ollama serve</code>.</p>"},{"location":"llms/#usage-examples","title":"Usage Examples","text":""},{"location":"llms/#basic-llm-usage","title":"Basic LLM Usage","text":"<pre><code>from protolink.llms.api import OpenAILLM\nfrom protolink.models import Message\n\n# Initialize LLM\nllm = OpenAILLM(model=\"gpt-4\")\n\n# Create messages\nmessages = [\n    Message(role=\"user\", content=\"Hello, how are you?\")\n]\n\n# Generate response\nresponse = llm.generate_response(messages)\nprint(response.content)\n</code></pre>"},{"location":"llms/#streaming-responses","title":"Streaming Responses","text":"<pre><code># Generate streaming response\nfor chunk in llm.generate_stream_response(messages):\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"llms/#updating-parameters","title":"Updating Parameters","text":"<pre><code># Update model parameters\nllm.set_model_params({\n    \"temperature\": 0.7,\n    \"max_tokens\": 500\n})\n\n# Update system prompt\nllm.set_system_prompt(\"You are a helpful coding assistant.\")\n</code></pre>"},{"location":"llms/#connection-validation","title":"Connection Validation","text":"<pre><code># Validate connection before use\nif llm.validate_connection():\n    print(\"LLM is ready!\")\nelse:\n    print(\"LLM connection failed.\")\n</code></pre>"},{"location":"llms/#error-handling","title":"Error Handling","text":"<p>All LLM implementations include error handling for common issues:</p> <ul> <li>Authentication Errors: Missing or invalid API keys</li> <li>Connection Errors: Network issues or unavailable servers</li> <li>Model Errors: Invalid model names or unavailable models</li> <li>Parameter Errors: Invalid parameter values</li> </ul> <p>Connection Validation</p> <p>Always call <code>validate_connection()</code> before using an LLM to ensure it's properly configured and reachable.</p>"},{"location":"llms/#type-aliases","title":"Type Aliases","text":"<p>The LLM module defines several type aliases for clarity:</p> <pre><code>LLMType: TypeAlias = Literal[\"api\", \"local\", \"server\"]\nLLMProvider: TypeAlias = Literal[\"openai\", \"anthropic\", \"google\", \"llama.cpp\", \"ollama\"]\n</code></pre> <p>These are used throughout the LLM implementations to ensure type safety and clarity.</p>"},{"location":"models/","title":"Models API Reference","text":"<p>This section provides detailed API documentation for the core data models in Protolink. These models represent the fundamental data structures used throughout the framework for agent communication, task management, and data exchange.</p>"},{"location":"models/#table-of-contents","title":"Table of Contents","text":"<ul> <li>AgentCard</li> <li>AgentCapabilities</li> <li>AgentSkill</li> <li>Task</li> <li>TaskState</li> <li>Message</li> <li>Part</li> <li>Artifact</li> <li>Context</li> </ul>"},{"location":"models/#agentcard","title":"AgentCard","text":"<pre><code>@dataclass\nclass AgentCard:\n    name: str\n    description: str\n    url: str\n    version: str = \"1.0.0\"\n    protocol_version: str = protolink_version\n    capabilities: AgentCapabilities = field(default_factory=AgentCapabilities)\n    skills: list[AgentSkill] = field(default_factory=list)\n    security_schemes: dict[str, dict[str, Any]] | None = field(default_factory=dict)\n    required_scopes: list[str] | None = field(default_factory=list)\n</code></pre> <p>Agent identity and capability declaration. This is the main metadata card that describes an agent's identity, capabilities, and security requirements.</p>"},{"location":"models/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>name</code> <code>str</code> \u2014 Required. Agent name <code>description</code> <code>str</code> \u2014 Required. Agent purpose/description <code>url</code> <code>str</code> \u2014 Required. Service endpoint URL <code>version</code> <code>str</code> <code>\"1.0.0\"</code> Agent version <code>protocol_version</code> <code>str</code> <code>protolink_version</code> Protolink Protocol version <code>capabilities</code> <code>AgentCapabilities</code> <code>AgentCapabilities()</code> Supported features <code>skills</code> <code>list[AgentSkill]</code> <code>[]</code> List of skills the agent can perform <code>security_schemes</code> <code>dict[str, dict[str, Any]] | None</code> <code>{}</code> Authentication schemes <code>required_scopes</code> <code>list[str] | None</code> <code>[]</code> Required OAuth scopes"},{"location":"models/#methods","title":"Methods","text":""},{"location":"models/#to_json-dictstr-any","title":"<code>to_json() -&gt; dict[str, Any]</code>","text":"<p>Convert the AgentCard to JSON format compatible with the A2A agent card specification.</p> <p>Returns: <pre><code>dict[str, Any]  # JSON dictionary representation\n</code></pre></p> <p>Example: <pre><code>card = AgentCard(name=\"weather_agent\", description=\"Weather service\")\njson_data = card.to_json()\nprint(json_data[\"name\"])  # \"weather_agent\"\n</code></pre></p>"},{"location":"models/#from_jsondata-dictstr-any-agentcard-classmethod","title":"<code>from_json(data: dict[str, Any]) -&gt; AgentCard</code> <code>classmethod</code>","text":"<p>Create an AgentCard from JSON data.</p> <p>Parameters: <pre><code>data: dict[str, Any]  # JSON dictionary containing agent card data\n</code></pre></p> <p>Returns: <pre><code>AgentCard  # New AgentCard instance\n</code></pre></p> <p>Example: <pre><code>json_data = {\n    \"name\": \"weather_agent\",\n    \"description\": \"Weather service\",\n    \"url\": \"https://api.example.com/weather\"\n}\ncard = AgentCard.from_json(json_data)\n</code></pre></p>"},{"location":"models/#example","title":"Example","text":"<pre><code>from protolink.models import AgentCard, AgentCapabilities\n\ncard = AgentCard(\n    name=\"weather_agent\",\n    description=\"Provides weather information\",\n    url=\"https://api.example.com/weather\",\n    version=\"1.2.0\",\n    capabilities=AgentCapabilities(\n        streaming=True,\n        tool_calling=True,\n        max_concurrency=5\n    )\n)\n\n# Convert to JSON\njson_data = card.to_json()\n</code></pre>"},{"location":"models/#agentcapabilities","title":"AgentCapabilities","text":"<pre><code>@dataclass\nclass AgentCapabilities:\n    streaming: bool = False\n    push_notifications: bool = False\n    state_transition_history: bool = False\n    max_concurrency: int = 1\n    message_batching: bool = False\n    tool_calling: bool = False\n    multi_step_reasoning: bool = False\n    timeout_support: bool = False\n    delegation: bool = False\n    rag_support: bool = False\n    code_execution: bool = False\n</code></pre> <p>Defines the capabilities and limitations of an agent. This extends the A2A specification with additional capability flags.</p>"},{"location":"models/#parameters_1","title":"Parameters","text":"Parameter Type Default Description <code>streaming</code> <code>bool</code> <code>False</code> Supports Server-Sent Events (SSE) streaming <code>push_notifications</code> <code>bool</code> <code>False</code> Supports push notifications (webhooks) <code>state_transition_history</code> <code>bool</code> <code>False</code> Provides detailed task state history <code>max_concurrency</code> <code>int</code> <code>1</code> Maximum concurrent tasks <code>message_batching</code> <code>bool</code> <code>False</code> Processes multiple messages per request <code>tool_calling</code> <code>bool</code> <code>False</code> Can call external tools/APIs <code>multi_step_reasoning</code> <code>bool</code> <code>False</code> Performs multi-step reasoning <code>timeout_support</code> <code>bool</code> <code>False</code> Respects operation timeouts <code>delegation</code> <code>bool</code> <code>False</code> Can delegate tasks to other agents <code>rag_support</code> <code>bool</code> <code>False</code> Supports Retrieval-Augmented Generation <code>code_execution</code> <code>bool</code> <code>False</code> Has access to safe execution sandbox"},{"location":"models/#agentskill","title":"AgentSkill","text":"<pre><code>@dataclass\nclass AgentSkill:\n    id: str\n    description: str = \"\"\n    tags: list[str] = field(default_factory=list)\n    examples: list[str] = field(default_factory=list)\n</code></pre> <p>Represents a task that an agent can perform. Skills are used to advertise specific capabilities to other agents.</p>"},{"location":"models/#parameters_2","title":"Parameters","text":"Parameter Type Default Description <code>id</code> <code>str</code> \u2014 Required. Unique Human-readable identifier for the task <code>description</code> <code>str</code> <code>\"\"</code> Optional Detailed description of what the task does <code>tags</code> <code>list[str]</code> <code>[]</code> Optional Tags for categorization <code>examples</code> <code>list[str]</code> <code>[]</code> Optional Example inputs or usage scenarios"},{"location":"models/#example_1","title":"Example","text":"<pre><code>skill = AgentSkill(\n    id=\"weather_forecast\",\n    description=\"Get weather forecast for any location\",\n    tags=[\"weather\", \"forecast\", \"location\"],\n    examples=[\n        \"What's the weather in New York?\",\n        \"Forecast for London tomorrow\",\n        \"Weather in 90210\"\n    ]\n)\n</code></pre>"},{"location":"models/#task","title":"Task","text":"<pre><code>@dataclass\nclass Task:\n    id: str = field(default_factory=lambda: str(uuid.uuid4()))\n    state: TaskState = TaskState.SUBMITTED\n    messages: list[Message] = field(default_factory=list)\n    artifacts: list[Artifact] = field(default_factory=list)\n    metadata: dict[str, Any] = field(default_factory=dict)\n    created_at: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat())\n</code></pre> <p>Unit of work exchanged between agents. Tasks encapsulate a complete unit of work including messages, state, and output artifacts.</p>"},{"location":"models/#parameters_3","title":"Parameters","text":"Parameter Type Default Description <code>id</code> <code>str</code> <code>uuid4()</code> Unique task identifier <code>state</code> <code>TaskState</code> <code>SUBMITTED</code> Current task state <code>messages</code> <code>list[Message]</code> <code>[]</code> Communication history <code>artifacts</code> <code>list[Artifact]</code> <code>[]</code> Output artifacts <code>metadata</code> <code>dict[str, Any]</code> <code>{}</code> Additional metadata <code>created_at</code> <code>str</code> <code>utc now</code> Creation timestamp"},{"location":"models/#methods_1","title":"Methods","text":""},{"location":"models/#task-add-message","title":"<code>add_message(message: Message) -&gt; Task</code>","text":"<p>Add a message to the task's communication history.</p> <p>Parameters: <pre><code>message: Message  # Message object to add to the task\n</code></pre></p> <p>Returns: <pre><code>Task  # Self for method chaining\n</code></pre></p> <p>Example: <pre><code>task = Task()\ntask.add_message(Message.user(\"What's the weather?\"))\ntask.add_message(Message.agent(\"It's sunny!\"))\n</code></pre></p>"},{"location":"models/#task-add-artifact","title":"<code>add_artifact(artifact: Artifact) -&gt; Task</code>","text":"<p>Add an output artifact to the task (v0.2.0+).</p> <p>Parameters: <pre><code>artifact: Artifact  # Artifact representing task output\n</code></pre></p> <p>Returns: <pre><code>Task  # Self for method chaining\n</code></pre></p> <p>Example: <pre><code>artifact = Artifact()\nartifact.add_text(\"Weather analysis complete\")\ntask.add_artifact(artifact)\n</code></pre></p>"},{"location":"models/#task-update-state","title":"<code>update_state(state: TaskState) -&gt; Task</code>","text":"<p>Update the task's current state.</p> <p>Parameters: <pre><code>state: TaskState  # New task state from TaskState enum\n</code></pre></p> <p>Returns: <pre><code>Task  # Self for method chaining\n</code></pre></p> <p>Example: <pre><code>task.update_state(TaskState.WORKING)\n# ... process task ...\ntask.update_state(TaskState.COMPLETED)\n</code></pre></p>"},{"location":"models/#task-complete","title":"<code>complete(response_text: str) -&gt; Task</code>","text":"<p>Mark the task as completed with a response message.</p> <p>Parameters: <pre><code>response_text: str  # Final response message text\n</code></pre></p> <p>Returns: <pre><code>Task  # Self for method chaining\n</code></pre></p> <p>Example: <pre><code>task.complete(\"The weather is sunny and 75\u00b0F.\")\nprint(task.state)  # TaskState.COMPLETED\n</code></pre></p>"},{"location":"models/#task-fail","title":"<code>fail(error_message: str) -&gt; Task</code>","text":"<p>Mark the task as failed with an error message.</p> <p>Parameters: <pre><code>error_message: str  # Description of the error\n</code></pre></p> <p>Returns: <pre><code>Task  # Self for method chaining\n</code></pre></p> <p>Example: <pre><code>task.fail(\"Weather API unavailable\")\nprint(task.state)  # TaskState.FAILED\nprint(task.metadata[\"error\"])  # \"Weather API unavailable\"\n</code></pre></p>"},{"location":"models/#task-to-dict","title":"<code>to_dict() -&gt; dict[str, Any]</code>","text":"<p>Convert the task to a dictionary for serialization.</p> <p>Returns: <pre><code>dict[str, Any]  # Dictionary representation of the task\n</code></pre></p> <p>Example: <pre><code>task_dict = task.to_dict()\nprint(task_dict[\"id\"])  # Task UUID\nprint(task_dict[\"state\"])  # Current state as string\n</code></pre></p>"},{"location":"models/#task-from-dict","title":"<code>from_dict(data: dict[str, Any]) -&gt; Task</code> <code>classmethod</code>","text":"<p>Create a task from a dictionary.</p> <p>Parameters: <pre><code>data: dict[str, Any]  # Dictionary containing task data\n</code></pre></p> <p>Returns: <pre><code>Task  # New Task instance\n</code></pre></p> <p>Example: <pre><code>task_dict = {\n    \"id\": \"123e4567-e89b-12d3-a456-426614174000\",\n    \"state\": \"completed\",\n    \"messages\": []\n}\ntask = Task.from_dict(task_dict)\n</code></pre></p>"},{"location":"models/#task-create","title":"<code>create(message: Message) -&gt; Task</code> <code>classmethod</code>","text":"<p>Create a new task with an initial message.</p> <p>Parameters: <pre><code>message: Message  # Initial message for the task\n</code></pre></p> <p>Returns: <pre><code>Task  # New Task instance with the message added\n</code></pre></p> <p>Example: <pre><code>task = Task.create(Message.user(\"Analyze this data\"))\nprint(len(task.messages))  # 1\nprint(task.state)  # TaskState.SUBMITTED\n</code></pre></p>"},{"location":"models/#example_2","title":"Example","text":"<pre><code>from protolink.models import Task, Message\n\n# Create task with initial message\ntask = Task.create(Message.user(\"What's the weather in New York?\"))\n\n# Add response and complete\ntask.add_message(Message.agent(\"It's 72\u00b0F and sunny in New York.\"))\ntask.complete(\"Weather forecast provided.\")\n\n# Or use convenience method\ntask = Task.create(Message.user(\"Hello\")).complete(\"Hi there!\")\n</code></pre>"},{"location":"models/#taskstate","title":"TaskState","text":"<pre><code>class TaskState(Enum):\n    SUBMITTED = \"submitted\"\n    WORKING = \"working\"\n    INPUT_REQUIRED = \"input-required\"\n    COMPLETED = \"completed\"\n    CANCELED = \"canceled\"\n    FAILED = \"failed\"\n    UNKNOWN = \"unknown\"\n</code></pre> <p>Enumeration of possible task states.</p>"},{"location":"models/#values","title":"Values","text":"Value Description <code>SUBMITTED</code> Task has been submitted to the agent <code>WORKING</code> Agent is actively processing the task <code>INPUT_REQUIRED</code> Agent needs additional input from user <code>COMPLETED</code> Task has been successfully completed <code>CANCELED</code> Task was canceled by user or agent <code>FAILED</code> Task failed due to an error <code>UNKNOWN</code> Task state is unknown"},{"location":"models/#message","title":"Message","text":"<pre><code>@dataclass\nclass Message:\n    id: str = field(default_factory=lambda: str(uuid.uuid4()))\n    role: RoleType = \"user\"\n    parts: list[Part] = field(default_factory=list)\n    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())\n</code></pre> <p>Single unit of communication between agents. Messages contain one or more parts and have a specific role.</p>"},{"location":"models/#parameters_4","title":"Parameters","text":"Parameter Type Default Description <code>id</code> <code>str</code> <code>uuid4()</code> Unique message identifier <code>role</code> <code>RoleType</code> <code>\"user\"</code> Sender role <code>parts</code> <code>list[Part]</code> <code>[]</code> Message content parts <code>timestamp</code> <code>str</code> <code>now</code> Creation timestamp"},{"location":"models/#role-types","title":"Role Types","text":"<ul> <li><code>\"user\"</code>: Message from a human user</li> <li><code>\"agent\"</code>: Message from an agent</li> <li><code>\"system\"</code>: System-level message</li> </ul>"},{"location":"models/#methods_2","title":"Methods","text":""},{"location":"models/#message-add-text","title":"<code>add_text(text: str) -&gt; Message</code>","text":"<p>Add a text part to the message.</p> <p>Parameters: <pre><code>text: str  # Text content to add\n</code></pre></p> <p>Returns: <pre><code>Message  # Self for method chaining\n</code></pre></p> <p>Example: <pre><code>msg = Message(role=\"user\")\nmsg.add_text(\"Hello, world!\")\nprint(len(msg.parts))  # 1\nprint(msg.parts[0].content)  # \"Hello, world!\"\n</code></pre></p>"},{"location":"models/#message-add-part","title":"<code>add_part(part: Part) -&gt; Message</code>","text":"<p>Add a content part to the message.</p> <p>Parameters: <pre><code>part: Part  # Part object to add (text, image, file, etc.)\n</code></pre></p> <p>Returns: <pre><code>Message  # Self for method chaining\n</code></pre></p> <p>Example: <pre><code>msg = Message(role=\"agent\")\nmsg.add_part(Part(\"text\", \"Here's the analysis:\"))\nmsg.add_part(Part(\"data\", {\"result\": \"success\"}))\n</code></pre></p>"},{"location":"models/#message-to-dict","title":"<code>to_dict() -&gt; dict[str, Any]</code>","text":"<p>Convert the message to a dictionary for serialization.</p> <p>Returns: <pre><code>dict[str, Any]  # Dictionary representation of the message\n</code></pre></p> <p>Example: <pre><code>msg = Message.user(\"Hello\")\nmsg_dict = msg.to_dict()\nprint(msg_dict[\"role\"])  # \"user\"\nprint(msg_dict[\"parts\"][0][\"content\"])  # \"Hello\"\n</code></pre></p>"},{"location":"models/#message-from-dict","title":"<code>from_dict(data: dict[str, Any]) -&gt; Message</code> <code>classmethod</code>","text":"<p>Create a message from a dictionary.</p> <p>Parameters: <pre><code>data: dict[str, Any]  # Dictionary containing message data\n</code></pre></p> <p>Returns: <pre><code>Message  # New Message instance\n</code></pre></p> <p>Example: <pre><code>msg_dict = {\n    \"id\": \"msg-123\",\n    \"role\": \"user\",\n    \"parts\": [{\"type\": \"text\", \"content\": \"Hello\"}],\n    \"timestamp\": \"2023-01-01T00:00:00Z\"\n}\nmsg = Message.from_dict(msg_dict)\n</code></pre></p>"},{"location":"models/#message-user","title":"<code>user(text: str) -&gt; Message</code> <code>classmethod</code>","text":"<p>Create a user message with text content (convenience method).</p> <p>Parameters: <pre><code>text: str  # Message text content\n</code></pre></p> <p>Returns: <pre><code>Message  # New Message instance with role \"user\"\n</code></pre></p> <p>Example: <pre><code>msg = Message.user(\"What's the weather?\")\nprint(msg.role)  # \"user\"\nprint(msg.parts[0].content)  # \"What's the weather?\"\n</code></pre></p>"},{"location":"models/#message-agent","title":"<code>agent(text: str) -&gt; Message</code> <code>classmethod</code>","text":"<p>Create an agent message with text content (convenience method).</p> <p>Parameters: <pre><code>text: str  # Message text content\n</code></pre></p> <p>Returns: <pre><code>Message  # New Message instance with role \"agent\"\n</code></pre></p> <p>Example: <pre><code>msg = Message.agent(\"It's sunny and 75\u00b0F.\")\nprint(msg.role)  # \"agent\"\nprint(msg.parts[0].content)  # \"It's sunny and 75\u00b0F.\"\n</code></pre></p>"},{"location":"models/#example_3","title":"Example","text":"<pre><code>from protolink.models import Message, Part\n\n# Create messages using convenience methods\nuser_msg = Message.user(\"What's the weather?\")\nagent_msg = Message.agent(\"It's sunny and 75\u00b0F.\")\n\n# Create message with multiple parts\nmsg = Message(role=\"user\")\nmsg.add_text(\"Here's an image:\")\nmsg.add_part(Part(\"image\", image_data))\n</code></pre>"},{"location":"models/#part","title":"Part","text":"<pre><code>@dataclass\nclass Part:\n    type: str\n    content: Any\n</code></pre> <p>Atomic content unit within a message. Parts represent individual pieces of content like text, images, or files.</p>"},{"location":"models/#parameters_5","title":"Parameters","text":"Parameter Type Description <code>type</code> <code>str</code> Content type (e.g., 'text', 'image', 'file') <code>content</code> <code>Any</code> The actual content data"},{"location":"models/#methods_3","title":"Methods","text":""},{"location":"models/#to_dict-dictstr-any","title":"<code>to_dict() -&gt; dict[str, Any]</code>","text":"<p>Convert to dictionary.</p> <p>Returns: Dictionary representation</p>"},{"location":"models/#from_dictdata-dictstr-any-part","title":"<code>from_dict(data: dict[str, Any]) -&gt; Part</code>","text":"<p>Create from dictionary.</p> <p>Parameters: - <code>data</code>: Dictionary data</p> <p>Returns: New Part instance</p>"},{"location":"models/#textcontent-str-part-classmethod","title":"<code>text(content: str) -&gt; Part</code> (classmethod)","text":"<p>Create a text part.</p> <p>Parameters: - <code>content</code>: Text content</p> <p>Returns: New Part instance with type \"text\"</p>"},{"location":"models/#example_4","title":"Example","text":"<pre><code>from protolink.models import Part\n\n# Create different types of parts\ntext_part = Part.text(\"Hello, world!\")\nimage_part = Part(\"image\", binary_image_data)\nfile_part = Part(\"file\", {\"filename\": \"report.pdf\", \"data\": pdf_data})\n</code></pre>"},{"location":"models/#artifact","title":"Artifact","text":"<pre><code>@dataclass\nclass Artifact:\n    artifact_id: str = field(default_factory=lambda: str(uuid.uuid4()))\n    parts: list[Part] = field(default_factory=list)\n    metadata: dict[str, Any] = field(default_factory=dict)\n    created_at: str = field(default_factory=lambda: datetime.utcnow().isoformat())\n</code></pre> <p>Output produced by a task (v0.2.0+). Artifacts represent results from task execution - files, structured data, analysis results, etc.</p>"},{"location":"models/#parameters_6","title":"Parameters","text":"Parameter Type Default Description <code>artifact_id</code> <code>str</code> <code>uuid4()</code> Unique artifact identifier <code>parts</code> <code>list[Part]</code> <code>[]</code> Content parts of the artifact <code>metadata</code> <code>dict[str, Any]</code> <code>{}</code> Artifact metadata <code>created_at</code> <code>str</code> <code>utc now</code> Creation timestamp"},{"location":"models/#methods_4","title":"Methods","text":""},{"location":"models/#add_partpart-part-artifact","title":"<code>add_part(part: Part) -&gt; Artifact</code>","text":"<p>Add content part to artifact.</p> <p>Parameters: - <code>part</code>: Part to add</p> <p>Returns: Self for method chaining</p>"},{"location":"models/#add_texttext-str-artifact","title":"<code>add_text(text: str) -&gt; Artifact</code>","text":"<p>Add text content (convenience method).</p> <p>Parameters: - <code>text</code>: Text content</p> <p>Returns: Self for method chaining</p>"},{"location":"models/#to_dict-dictstr-any_1","title":"<code>to_dict() -&gt; dict[str, Any]</code>","text":"<p>Convert to dictionary.</p> <p>Returns: Dictionary representation</p>"},{"location":"models/#from_dictdata-dictstr-any-artifact","title":"<code>from_dict(data: dict[str, Any]) -&gt; Artifact</code>","text":"<p>Create from dictionary.</p> <p>Parameters: - <code>data</code>: Dictionary data</p> <p>Returns: New Artifact instance</p>"},{"location":"models/#example_5","title":"Example","text":"<pre><code>from protolink.models import Artifact, Part\n\n# Create artifact with multiple parts\nartifact = Artifact()\nartifact.add_text(\"Analysis Results:\")\nartifact.add_part(Part(\"data\", {\"results\": [1, 2, 3]}))\nartifact.add_part(Part(\"chart\", chart_image_data))\n\n# Set metadata\nartifact.metadata[\"type\"] = \"analysis_report\"\nartifact.metadata[\"version\"] = \"1.0\"\n</code></pre>"},{"location":"models/#context","title":"Context","text":"<pre><code>@dataclass\nclass Context:\n    context_id: str = field(default_factory=lambda: str(uuid.uuid4()))\n    messages: list = field(default_factory=list)  # List[Message]\n    metadata: dict[str, Any] = field(default_factory=dict)\n    created_at: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat())\n    last_accessed: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat())\n</code></pre> <p>Represents a conversation context (session). Contexts group messages across multiple turns, enabling long-running conversations and session persistence.</p>"},{"location":"models/#parameters_7","title":"Parameters","text":"Parameter Type Default Description <code>context_id</code> <code>str</code> <code>uuid4()</code> Unique context identifier <code>messages</code> <code>list</code> <code>[]</code> All messages in this context <code>metadata</code> <code>dict[str, Any]</code> <code>{}</code> Custom context data <code>created_at</code> <code>str</code> <code>utc now</code> Creation timestamp <code>last_accessed</code> <code>str</code> <code>utc now</code> Last activity timestamp"},{"location":"models/#methods_5","title":"Methods","text":""},{"location":"models/#add_messagemessage-context","title":"<code>add_message(message) -&gt; Context</code>","text":"<p>Add a message to this context.</p> <p>Parameters: - <code>message</code>: Message object to add</p> <p>Returns: Self for method chaining</p>"},{"location":"models/#to_dict-dict","title":"<code>to_dict() -&gt; dict</code>","text":"<p>Convert context to dictionary.</p> <p>Returns: Dictionary representation</p>"},{"location":"models/#from_dictdata-dict-context","title":"<code>from_dict(data: dict) -&gt; Context</code>","text":"<p>Create context from dictionary.</p> <p>Parameters: - <code>data</code>: Dictionary data</p> <p>Returns: New Context instance</p>"},{"location":"models/#example_6","title":"Example","text":"<pre><code>from protolink.models import Context, Message\n\n# Create new context\ncontext = Context()\ncontext.metadata[\"user_id\"] = \"user123\"\ncontext.metadata[\"session_type\"] = \"weather_chat\"\n\n# Add messages\ncontext.add_message(Message.user(\"What's the weather?\"))\ncontext.add_message(Message.agent(\"It's sunny!\"))\n\n# Context maintains conversation history\nfor msg in context.messages:\n    print(f\"{msg.role}: {msg.parts[0].content}\")\n</code></pre>"},{"location":"models/#usage-patterns","title":"Usage Patterns","text":""},{"location":"models/#task-workflow","title":"Task Workflow","text":"<pre><code>from protolink.models import Task, Message, Artifact, TaskState\n\n# Create and submit task\ntask = Task.create(Message.user(\"Analyze this data\"))\n\n# Process task\ntask.update_state(TaskState.WORKING)\n\n# Add results\nartifact = Artifact()\nartifact.add_text(\"Analysis complete\")\nartifact.add_part(Part(\"data\", {\"result\": \"success\"}))\n\ntask.add_artifact(artifact)\ntask.add_message(Message.agent(\"Analysis complete\"))\ntask.update_state(TaskState.COMPLETED)\n</code></pre>"},{"location":"models/#context-management","title":"Context Management","text":"<pre><code>from protolink.models import Context, Message\n\n# Long-running conversation\ncontext = Context()\ncontext.add_message(Message.user(\"Hello\"))\ncontext.add_message(Message.agent(\"Hi! How can I help?\"))\n\n# Continue conversation later\ncontext.add_message(Message.user(\"What did we discuss?\"))\n# Context maintains full history\n</code></pre>"},{"location":"models/#serialization","title":"Serialization","text":"<p>All models support JSON/dict serialization:</p> <pre><code># Convert to dict\ntask_dict = task.to_dict()\n\n# Restore from dict\nrestored_task = Task.from_dict(task_dict)\n\n# Works for all models\ncontext_dict = context.to_dict()\nrestored_context = Context.from_dict(context_dict)\n</code></pre>"},{"location":"tools/","title":"Tools","text":"<p>Tools extend agent capabilities with additional functions.</p>"},{"location":"tools/#native-tools","title":"Native Tools","text":"<p>Native tools are regular Python callables that you register on an agent. They are exposed over the transport so that other agents (or clients) can invoke them.</p> <p>To register a native tool, decorate an async function with <code>@agent.tool</code>:</p> <pre><code>from protolink.agents import Agent\n\n\n@agent.tool(name=\"add\", description=\"Add two numbers\")\nasync def add_numbers(a: int, b: int) -&gt; int:\n    return a + b\n</code></pre> <p>Native tools are ideal for business logic, data access, or any capability you want your agents to expose.</p>"},{"location":"tools/#mcp-tools","title":"MCP Tools","text":"<p>Protolink can also expose tools from an MCP (Model Context Protocol) server using the <code>MCPToolAdapter</code>.</p> <p>High\u2011level flow:</p> <ol> <li>Connect to an MCP server using an MCP client (not shown here).</li> <li>Wrap an MCP tool with <code>MCPToolAdapter</code>.</li> <li>Register it on the agent.</li> </ol> <p>Example pattern:</p> <pre><code>from protolink.tools import MCPToolAdapter\n\n\nmcp_tool = MCPToolAdapter(mcp_client, \"multiply\")\nagent.add_tool(mcp_tool)\n</code></pre> <p>The MCP adapter lets you reuse existing MCP tools as if they were native tools, keeping a consistent interface on the agent side.</p>"},{"location":"transports/","title":"Transports","text":"<p>Protolink supports multiple transports for agent communication. A transport is responsible for delivering <code>Task</code> and <code>Message</code> objects between agents and for exposing an API surface (HTTP, WebSocket, in\u2011memory, etc.).</p> <p>At a high level, all transports implement the same conceptual operations:</p> <ul> <li>Send work: send a <code>Task</code> or <code>Message</code> to another agent.</li> <li>Receive work: expose an endpoint / callback to handle incoming tasks.</li> <li>Lifecycle: start and stop the underlying server or runtime.</li> </ul> <p>The sections below describe the available transports in more detail, with an emphasis on <code>HTTPTransport</code>.</p>"},{"location":"transports/#supported-transports","title":"Supported Transports","text":"<ul> <li>HTTPTransport</li> <li>Uses HTTP/HTTPS for synchronous request/response.</li> <li>Backed by ASGI frameworks:<ul> <li><code>Starlette</code> + <code>httpx</code> + <code>uvicorn</code> (lightweight default backend).</li> <li><code>FastAPI</code> + <code>pydantic</code> + <code>uvicorn</code> (with optional request validation).</li> </ul> </li> <li> <p>Great default choice for web\u2011based agents, simple deployments, and interoperable APIs.</p> </li> <li> <p>WebSocketTransport</p> </li> <li>Uses WebSocket for streaming requests and responses.</li> <li>Built on top of libraries like <code>websockets</code> (and <code>httpx</code> for HTTP parts where applicable).</li> <li> <p>Useful for real\u2011time, bidirectional communication or token\u2011level streaming.</p> </li> <li> <p>JSONRPCTransport (TBD)</p> </li> <li>Planned JSON\u2011RPC based transport.</li> <li> <p>Intended for structured, RPC\u2011style interactions.</p> </li> <li> <p>GRPCTransport (TBD)</p> </li> <li>Planned gRPC transport.</li> <li> <p>Intended for high\u2011performance, strongly\u2011typed communication.</p> </li> <li> <p>RuntimeTransport</p> </li> <li>Simple in\u2011process, in\u2011memory transport.</li> <li>Allows multiple agents to communicate within the same Python process.</li> <li>Ideal for local development, testing, and tightly\u2011coupled agent systems.</li> </ul>"},{"location":"transports/#choosing-a-transport","title":"Choosing a Transport","text":"<p>Some rough guidelines:</p> <ul> <li>Use RuntimeTransport for local experiments, tests, or when all agents live in the same process.</li> <li>Use HTTPTransport when you want a simple, interoperable API surface (e.g. calling agents from other services or frontends).</li> <li>Use WebSocketTransport when you need streaming and interactive sessions.</li> <li>Plan for JSONRPCTransport or GRPCTransport if you need stricter schemas or higher performance across services.</li> </ul> <p>The rest of this page dives into the API of each transport in more detail.</p>"},{"location":"transports/#httptransport","title":"HTTPTransport","text":"<p><code>HTTPTransport</code> is the main network transport in Protolink. It exposes a simple JSON HTTP API compatible with the rest of the framework.</p>"},{"location":"transports/#overview","title":"Overview","text":"<ul> <li>Client side</li> <li>Uses <code>httpx.AsyncClient</code> to send JSON requests.</li> <li> <p>Provides helpers to send a full <code>Task</code> or a single <code>Message</code>.</p> </li> <li> <p>Server side</p> </li> <li>Uses an ASGI app (Starlette or FastAPI) to expose:<ul> <li><code>POST /tasks/</code> \u2014 submit a <code>Task</code> to the agent.</li> <li><code>GET /.well-known/agent.json</code> \u2014 served by the agent itself, not the transport, but typically used together.</li> </ul> </li> <li>Uses a backend implementation of <code>BackendInterface</code> to manage the ASGI app and <code>uvicorn</code> server.</li> </ul>"},{"location":"transports/#backends-starlette-vs-fastapi","title":"Backends: Starlette vs FastAPI","text":"<p><code>HTTPTransport</code> delegates server behavior to a backend implementing <code>BackendInterface</code>:</p> <ul> <li>StarletteBackend (default)</li> <li>Minimal Starlette app with a single <code>POST /tasks/</code> route.</li> <li>No extra request validation beyond what <code>Task.from_dict()</code> does.</li> <li> <p>Best when you want low overhead and trust callers to send valid payloads.</p> </li> <li> <p>FastAPIBackend</p> </li> <li>FastAPI app with optional Pydantic models mirroring the <code>Task</code>/<code>Message</code>/<code>Artifact</code> structures.</li> <li>When <code>validate_schema=True</code>, incoming requests are validated against these models before being converted with <code>Task.from_dict()</code>.</li> <li>Best when you want schema validation and better generated OpenAPI / docs.</li> </ul> <p>Backend and validation are selected via the <code>HTTPTransport</code> constructor:</p> <pre><code>from protolink.transport.http_transport import HTTPTransport\n\n# Starlette backend (default)\ntransport = HTTPTransport()\n\n# Explicit Starlette backend\ntransport = HTTPTransport(backend=\"starlette\")\n\n# FastAPI backend without schema validation\ntransport = HTTPTransport(backend=\"fastapi\", validate_schema=False)\n\n# FastAPI backend with full schema validation\ntransport = HTTPTransport(backend=\"fastapi\", validate_schema=True)\n</code></pre>"},{"location":"transports/#wire-format","title":"Wire Format","text":"<p><code>HTTPTransport</code> sends and receives JSON payloads that match the core models' <code>to_dict()</code> methods. A typical <code>Task</code> request body looks like this:</p> <pre><code>{\n  \"id\": \"8c1e93b3-9f72-4a37-8c4c-3d2d8a9c4f7c\",\n  \"state\": \"submitted\",\n  \"messages\": [\n    {\n      \"id\": \"f0e4c2f7-5d3b-4b0a-b6e0-6a7f2d9c1b2a\",\n      \"role\": \"user\",\n      \"parts\": [\n        {\"type\": \"text\", \"content\": \"Hi Bob, how are you?\"}\n      ],\n      \"timestamp\": \"2025-01-01T12:00:00Z\"\n    }\n  ],\n  \"artifacts\": [],\n  \"metadata\": {},\n  \"created_at\": \"2025-01-01T12:00:00Z\"\n}\n</code></pre> <p>The tables below document each object type.</p>"},{"location":"transports/#task","title":"Task","text":"Field Type Description <code>id</code> <code>str</code> Unique task identifier. <code>state</code> <code>str</code> One of <code>\"submitted\"</code>, <code>\"working\"</code>, <code>\"completed\"</code>, etc. <code>messages</code> <code>list[Message]</code> Conversation history for this task. <code>artifacts</code> <code>list[Artifact]</code> Outputs produced by the task. <code>metadata</code> <code>dict[str, Any]</code> Arbitrary metadata attached to the task. <code>created_at</code> <code>str</code> ISO\u20118601 timestamp (UTC)."},{"location":"transports/#message","title":"Message","text":"<pre><code>{\n  \"id\": \"f0e4c2f7-5d3b-4b0a-b6e0-6a7f2d9c1b2a\",\n  \"role\": \"user\",\n  \"parts\": [\n    {\"type\": \"text\", \"content\": \"Hi Bob, how are you?\"}\n  ],\n  \"timestamp\": \"2025-01-01T12:00:00Z\"\n}\n</code></pre> Field Type Description <code>id</code> <code>str</code> Unique message identifier. <code>role</code> <code>\"user\" \\| \"agent\" \\| \"system\"</code> Sender role. <code>parts</code> <code>list[Part]</code> Content payloads. <code>timestamp</code> <code>str</code> ISO\u20118601 timestamp."},{"location":"transports/#part","title":"Part","text":"<pre><code>{\"type\": \"text\", \"content\": \"Hi Bob, how are you?\"}\n</code></pre> Field Type Description <code>type</code> <code>str</code> Content type (e.g. <code>\"text\"</code>). <code>content</code> <code>Any</code> The actual content payload."},{"location":"transports/#artifact","title":"Artifact","text":"<pre><code>{\n  \"artifact_id\": \"a1b2c3\",\n  \"parts\": [\n    {\"type\": \"text\", \"content\": \"final report\"}\n  ],\n  \"metadata\": {\"kind\": \"report\"},\n  \"created_at\": \"2025-01-01T12:00:00Z\"\n}\n</code></pre> Field Type Description <code>artifact_id</code> <code>str</code> Unique artifact identifier. <code>parts</code> <code>list[Part]</code> Artifact content. <code>metadata</code> <code>dict[str, Any]</code> Artifact metadata. <code>created_at</code> <code>str</code> ISO\u20118601 timestamp."},{"location":"transports/#typical-usage","title":"Typical Usage","text":""},{"location":"transports/#exposing-an-agent-over-http","title":"Exposing an agent over HTTP","text":"<pre><code>from protolink.agents import Agent\nfrom protolink.core.agent_card import AgentCard\nfrom protolink.core.message import Message\nfrom protolink.core.task import Task\nfrom protolink.transport.http_transport import HTTPTransport\n\n\nclass EchoAgent(Agent):\n    def __init__(self, port: int) -&gt; None:\n        card = AgentCard(\n            name=\"echo\", description=\"Echoes back the last user message\", url=f\"http://127.0.0.1:{port}\",\n        )\n        transport = HTTPTransport(host=\"127.0.0.1\", port=port)\n        super().__init__(card, transport=transport)\n\n    async def handle_task(self, task: Task) -&gt; Task:\n        last_msg = task.messages[-1]\n        reply = Message.agent(f\"echo: {last_msg.parts[0].content}\")\n        return Task(id=task.id, messages=task.messages + [reply])\n</code></pre> <p>Then run the agent and call it from another agent or client using <code>send_task</code> or <code>send_message</code>.</p>"},{"location":"transports/#calling-a-remote-agent","title":"Calling a remote agent","text":"<pre><code>from protolink.client.agent_client import AgentClient\nfrom protolink.core.message import Message\nfrom protolink.core.task import Task\nfrom protolink.transport.http_transport import HTTPTransport\n\n\nclient = AgentClient(HTTPTransport())\n\n\nasync def call_remote(url: str) -&gt; None:\n    hello = Task.create(Message.user(\"Hello over HTTP!\"))\n    result = await client.send_task(url, hello)\n    print(\"Response:\", result.messages[-1].parts[0].content)\n</code></pre>"},{"location":"transports/#httptransport-api-reference","title":"HTTPTransport API Reference","text":"<p>The most important public methods on <code>HTTPTransport</code> are summarized below.</p>"},{"location":"transports/#constructor-lifecycle","title":"Constructor &amp; lifecycle","text":"Name Parameters Returns Description <code>__init__</code> <code>host: str = \"0.0.0.0\"</code>, <code>port: int = 8000</code>, <code>timeout: float = 30.0</code>, <code>auth_provider: AuthProvider \\| None = None</code>, <code>backend: Literal[\"starlette\", \"fastapi\"] = \"starlette\"</code>, <code>validate_schema: bool = False</code> <code>None</code> Configure host/port, request timeout, optional authentication provider, backend implementation, and whether to enable FastAPI/Pydantic schema validation. <code>start</code> <code>self</code> <code>Awaitable[None]</code> Start the selected backend, register the <code>/tasks/</code> route and create the internal <code>httpx.AsyncClient</code>. Must be awaited before serving HTTP traffic. <code>stop</code> <code>self</code> <code>Awaitable[None]</code> Stop the backend server and close the internal HTTP client. Safe to call multiple times."},{"location":"transports/#sending-receiving","title":"Sending &amp; receiving","text":"Name Parameters Returns Description <code>on_task_received</code> <code>handler: Callable[[Task], Awaitable[Task]]</code> <code>None</code> Register the callback that will handle incoming tasks on <code>POST /tasks/</code>. This must be set before <code>start()</code> when running as a server. <code>send_task</code> <code>agent_url: str</code>, <code>task: Task</code>, <code>skill: str \\| None = None</code> <code>Awaitable[Task]</code> Send a <code>Task</code> to <code>POST {agent_url}/tasks/</code> and return the resulting <code>Task</code> from the remote agent. The optional <code>skill</code> is passed via headers and can be used by agents to route work. <code>send_message</code> <code>agent_url: str</code>, <code>message: Message</code> <code>Awaitable[Message]</code> Convenience wrapper that wraps a single <code>Message</code> in a new <code>Task</code>, calls <code>send_task</code>, and returns the last response message. Ideal for simple request/response interactions. <code>get_agent_card</code> <code>agent_url: str</code> <code>Awaitable[AgentCard]</code> Fetch the remote agent's <code>AgentCard</code> description from <code>GET {agent_url}/.well-known/agent.json</code>. Useful for discovery and capability inspection."},{"location":"transports/#auth-utilities","title":"Auth &amp; utilities","text":"Name Parameters Returns Description <code>authenticate</code> <code>credentials: str</code> <code>Awaitable[None]</code> Use the configured <code>AuthProvider</code> to obtain an auth context (for example, exchanging an API key for a bearer token). The resulting context is automatically injected into outgoing HTTP headers. <code>_build_headers</code> <code>skill: str \\| None = None</code> <code>dict[str, str]</code> Internal helper that constructs HTTP headers (including <code>Authorization</code> when an auth context is present). Exposed here for completeness; you normally do not need to call it directly. <code>validate_agent_url</code> <code>agent_url: str</code> <code>bool</code> Return <code>True</code> if the URL is considered local to this transport's host/port (e.g. for allow\u2011listing), <code>False</code> otherwise."},{"location":"transports/#runtimetransport","title":"RuntimeTransport","text":"<p><code>RuntimeTransport</code> is an in\u2011process, in\u2011memory transport used primarily for tests, local experimentation, and tightly\u2011coupled multi\u2011agent systems.</p> <p>Characteristics:</p> <ul> <li>No network hops, very low latency.</li> <li>Multiple agents share the same runtime transport instance.</li> <li>Ideal for composition and unit tests (see <code>tests/test_agent.py</code>).</li> </ul>"},{"location":"transports/#runtimetransport-api","title":"RuntimeTransport API","text":"Name Parameters Returns Description <code>__init__</code> <code>...</code> <code>None</code> Create an in\u2011memory transport registry for agents that live in the same Python process. <code>register</code> <code>agent</code> <code>None</code> Add an agent to the runtime transport so it can receive tasks from others. <code>unregister</code> <code>agent</code> <code>None</code> Remove an agent from the runtime transport. <code>send_task</code> <code>agent_id_or_url</code>, <code>task: Task</code> <code>Task \\| Awaitable[Task]</code> Dispatch a <code>Task</code> to another agent registered on the same runtime transport instance. Exact return type depends on the concrete implementation. <code>start</code> / <code>stop</code> <code>self</code> <code>None</code> Often no\u2011op or light\u2011weight setup/teardown. Provided for a consistent lifecycle API with other transports."},{"location":"transports/#websockettransport","title":"WebSocketTransport","text":"<p><code>WebSocketTransport</code> (when available) provides streaming, bidirectional communication between agents or between agents and external clients.</p> <p>Use it when:</p> <ul> <li>You need token\u2011level or chunk\u2011level streaming.</li> <li>You want long\u2011lived interactive sessions (chat UIs, dashboards, tools that stream output).</li> </ul>"},{"location":"transports/#websockettransport-api","title":"WebSocketTransport API","text":"Name Parameters Returns Description <code>__init__</code> <code>...</code> <code>None</code> Configure host/port and WebSocket settings for streaming connections. <code>send_task_stream</code> <code>...</code> <code>AsyncIterator[Task] \\| AsyncIterator[Message]</code> Send a <code>Task</code> and receive a stream of partial results or updates over a single WebSocket connection. (Exact type depends on implementation.) <code>start</code> / <code>stop</code> <code>self</code> <code>Awaitable[None]</code> Start or stop the WebSocket server."},{"location":"transports/#planned-transports","title":"Planned Transports","text":"<p>These transports are not implemented yet in the core library. The sections below describe the intended design so you can plan ahead, but there is currently no public API to import.</p> <p>Status: Design sketches only. Do not rely on these in production code.</p>"},{"location":"transports/#jsonrpctransport-planned","title":"JSONRPCTransport (planned)","text":"<ul> <li>JSON\u2011RPC 2.0 style envelope for structured requests and responses.</li> <li>Strong separation of methods, params, and results.</li> <li>Natural fit for RPC\u2011style integrations.</li> </ul>"},{"location":"transports/#jsonrpctransport-design","title":"JSONRPCTransport design","text":"Name Parameters Returns Description <code>send_request</code> <code>method: str</code>, <code>params: dict</code> <code>Awaitable[dict]</code> (Planned) Send a JSON\u2011RPC request and return the decoded result payload. <code>notify</code> <code>method: str</code>, <code>params: dict</code> <code>Awaitable[None]</code> (Planned) Fire\u2011and\u2011forget notification without a response. <code>start</code> / <code>stop</code> <code>self</code> <code>Awaitable[None]</code> (Planned) Start/stop the JSON\u2011RPC server."},{"location":"transports/#grpctransport-planned","title":"GRPCTransport (planned)","text":"<ul> <li>gRPC\u2011based transport with protobuf definitions for tasks and messages.</li> <li>High\u2011performance, strongly\u2011typed, streaming\u2011friendly.</li> </ul>"},{"location":"transports/#grpctransport-design","title":"GRPCTransport design","text":"Name Parameters Returns Description <code>send_task</code> <code>...</code> <code>Awaitable[...]</code> (Planned) Unary RPC for sending tasks. <code>send_task_stream</code> <code>...</code> <code>AsyncIterator[...]</code> (Planned) Streaming RPC for long\u2011running tasks and progress updates. <code>start</code> / <code>stop</code> <code>self</code> <code>Awaitable[None]</code> (Planned) Start/stop the gRPC server."}]}