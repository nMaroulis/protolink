{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p>A lightweight, production-ready framework for agent-to-agent communication, built on and extending Google's A2A protocol.</p> <p>Get Started View on GitHub</p> <p>Welcome to the Protolink documentation.</p> <p>This site provides an overview of the framework, its concepts, and how to use it in your projects.</p> <p>Current release: see protolink on PyPI. </p> <p> </p>"},{"location":"#contents","title":"Contents","text":"<ul> <li>Getting Started</li> <li>Agents</li> <li>Transports</li> <li>LLMs</li> <li>Tools</li> <li>Examples</li> </ul>"},{"location":"#what-is-protolink","title":"What is Protolink ?","text":"<p>ProtoLink is a lightweight, production-ready Python framework for building distributed multi-agent systems where AI agents communicate directly with each other.</p> <p>Each ProtoLink agent is a self-contained runtime that can embed an LLM, manage execution context, expose and consume tools (native or via MCP), and coordinate with other agents over a unified transport layer.</p> <p>ProtoLink implements and extends Google\u2019s Agent-to-Agent (A2A) specification for agent identity, capability declaration, and discovery, while going beyond A2A by enabling true agent-to-agent collaboration.</p> <p>The framework emphasizes minimal boilerplate, explicit control, and production-readiness, making it suitable for both research and real-world systems.</p>"},{"location":"#protolink-vs-google-a2a","title":"Protolink vs Google A2A \ud83d\udca1","text":"<p>ProtoLink implements Google\u2019s A2A protocol at the wire level, while providing a higher-level agent runtime that unifies client, server, transport, tools, and LLMs into a single composable abstraction the Agent.</p> Concept Google A2A ProtoLink Agent Protocol-level concept Runtime object Transport External server concern Agent-owned Client Separate Built-in LLM Out of scope First-class Tools Out of scope Native + MCP UX Enterprise infra Developer-first"},{"location":"#what-you-can-do-with-protolink","title":"What you can do with Protolink","text":"<ul> <li> <p>Build agents quickly   See Getting Started and Agents for the core concepts and basic setup.</p> </li> <li> <p>Choose your transport   Explore Transports to switch between HTTP, WebSocket, runtime, and future transports with minimal code changes.</p> </li> <li> <p>Plug in LLMs &amp; tools   Use LLMs and Tools to wire in language models and both native &amp; MCP tools as agent modules.</p> </li> </ul>"},{"location":"#key-ideas","title":"Key ideas:","text":"<ul> <li>Unified Agent model: a single autonomous <code>AI Agent</code> instance handles both client and server responsibilities, incorporating LLMs and tools.</li> <li>Flexible transports: HTTP, WebSocket, in\u2011process runtime, and planned JSON\u2011RPC / gRPC transports.</li> <li>LLM\u2011ready architecture: first\u2011class integration with API, local, and server\u2011hosted LLMs.</li> <li>Tools as modules: native Python tools and MCP tools plugged directly into agents.</li> </ul> <p>Use this documentation to:</p> <ul> <li>Install Protolink and run your first agent.</li> <li>Understand how agents, transports, LLMs, and tools fit together.</li> <li>Explore practical examples you can adapt to your own systems.</li> </ul> <p>Protolink is open source under the MIT license. Contributions are welcome \u2013 see the repository\u2019s Contributing section on GitHub.</p>"},{"location":"agents/","title":"Agents","text":"<p>Agents are the core building blocks in Protolink.</p>"},{"location":"agents/#concepts","title":"Concepts","text":"<p>An Agent in Protolink is a unified component that can act as both client and server. This is different from the original A2A spec, which separates client and server concerns.</p> <p>High\u2011level ideas:</p> <ul> <li>Unified model: a single <code>Agent</code> instance can send and receive messages.</li> <li>AgentCard: a small model describing the agent (name, description, metadata).</li> <li>Modules:<ul> <li>LLMs (e.g. <code>OpenAILLM</code>, <code>AnthropicLLM</code>, <code>LlamaCPPLLM</code>, <code>OllamaLLM</code>).</li> <li>Tools (native Python functions or MCP\u2011backed tools).</li> </ul> </li> <li>Transport abstraction: agents communicate over transports such as HTTP, WebSocket, gRPC, or the in\u2011process runtime transport.</li> </ul>"},{"location":"agents/#creating-an-agent","title":"Creating an Agent","text":"<p>A minimal agent consists of three pieces:</p> <ol> <li>An <code>AgentCard</code> describing the agent.</li> <li>A <code>Transport</code> implementation.</li> <li>An optional LLM and tools.</li> </ol> <p>Example:</p> <pre><code>from protolink.agents import Agent\nfrom protolink.models import AgentCard\nfrom protolink.transport import HTTPAgentTransport\nfrom protolink.llms.api import OpenAILLM\n\n# Agent card can be an AgentCard object or a dict for simplicity, both are handled the same way.\n# Option 1: Using AgentCard object\nagent_card = AgentCard(\n    name=\"example_agent\",\n    description=\"A dummy agent\",\n)\n\n# Option 2: Using dictionary (simpler)\ncard_dict = {\n    \"name\": \"example_agent\",\n    \"description\": \"A dummy agent\",\n    \"url\": \"http://localhost:8000\"\n}\n\ntransport = HTTPAgentTransport()\nllm = OpenAILLM(model=\"gpt-5.2\")\n\n# Both approaches work\nagent = Agent(agent_card, transport, llm)\n# OR\nagent = Agent(card_dict, transport, llm)\n</code></pre> <p>You can then attach tools and start the agent.</p>"},{"location":"agents/#agent-to-agent-communication","title":"Agent-to-Agent Communication","text":"<p>Agents communicate over a chosen transport.</p> <p>Common patterns:</p> <ul> <li>RuntimeAgentTransport: multiple agents in the same process share an in\u2011memory transport, which is ideal for local testing and composition.</li> <li>HTTPAgentTransport / WebSocketAgentTransport: agents expose HTTP or WebSocket endpoints so that other agents (or external clients) can send requests.</li> <li>gRPC / JSON\u2011RPC (planned): additional transports for more structured or high\u2011performance communication.</li> </ul> <p>From the framework\u2019s perspective, all of these are implementations of the same transport interface, so you can swap them with minimal code changes.</p>"},{"location":"agents/#agent-api-reference","title":"Agent API Reference","text":"<p>This section provides a detailed API reference for the <code>Agent</code> base class in <code>protolink.agents.base</code>. The <code>Agent</code> class is the core component for creating A2A-compatible agents, serving as both client and server.</p> <p>Unified Agent Model</p> <p>Unlike the original A2A specification, Protolink's <code>Agent</code> combines client and server functionality in a single class. You can send tasks/messages to other agents while also serving incoming requests.</p>"},{"location":"agents/#constructor","title":"Constructor","text":"Parameter Type Default Description <code>card</code> <code>AgentCard</code> \u2014 Required. The agent's metadata card containing name, description, and other identifying information. <code>llm</code> <code>LLM | None</code> <code>None</code> Optional language model instance for AI-powered task processing. <code>transport</code> <code>Transport | None</code> <code>None</code> Optional transport for communication. If not provided, you must set one later via <code>set_transport()</code>. <code>authenticator</code> <code>Authenticator | None</code> <code>None</code> Optional authentication provider for securing agent communications. <code>skills</code> <code>Literal[\"auto\", \"fixed\"]</code> <code>\"auto\"</code> Skills mode - <code>\"auto\"</code> to automatically detect and add skills, <code>\"fixed\"</code> to use only the skills defined by the user in the AgentCard. <pre><code>from protolink.agents import Agent\nfrom protolink.models import AgentCard\nfrom protolink.transport import HTTPAgentTransport\nfrom protolink.llms.api import OpenAILLM\n\nurl = \"http://localhost:8020\"\ncard = AgentCard(name=\"my_agent\", description=\"Example agent\", url=url)\nllm = OpenAILLM(model=\"gpt-4\")\ntransport = HTTPAgentTransport(url=url)\n\nagent = Agent(card=card, llm=llm, transport=transport)\n</code></pre>"},{"location":"agents/#lifecycle-methods","title":"Lifecycle Methods","text":"<p>These methods control the agent's server component lifecycle.</p> Name Parameters Returns Description <code>start()</code> \u2014 <code>None</code> Starts the agent's server component if a transport is configured. <code>stop()</code> \u2014 <code>None</code> Stops the agent's server component and cleans up resources. <p>Transport Required</p> <p>The <code>start()</code> method requires a transport to be configured. If no transport was provided during construction, call <code>set_transport()</code> first.</p>"},{"location":"agents/#transport-management","title":"Transport Management","text":"Name Parameters Returns Description <code>set_transport()</code> <code>transport: Transport</code> <code>None</code> Sets or updates the transport used by this agent. <code>client</code> (property) \u2014 <code>AgentClient</code> Returns the client instance for sending requests to other agents. <code>server</code> (property) \u2014 <code>Server \\| None</code> Returns the server instance if one is available via the transport."},{"location":"agents/#task-and-message-handling","title":"Task and Message Handling","text":""},{"location":"agents/#core-task-processing","title":"Core Task Processing","text":"Name Parameters Returns Description <code>handle_task()</code> <code>task: Task</code> <code>Task</code> Abstract method. Subclasses must implement this to define how tasks are processed. <code>handle_task_streaming()</code> <code>task: Task</code> <code>AsyncIterator[Task]</code> Optional streaming handler for real-time task updates. Default raises <code>NotImplementedError</code>."},{"location":"agents/#communication-methods","title":"Communication Methods","text":"Name Parameters Returns Description <code>send_task_to()</code> <code>agent_url: str</code>, <code>task: Task</code>, <code>skill: str \\| None = None</code> <code>Task</code> Sends a task to another agent and returns the processed result. <code>send_message_to()</code> <code>agent_url: str</code>, <code>message: Message</code> <code>Message</code> Sends a message to another agent and returns the response. <p>Authentication</p> <p>All outgoing requests are automatically signed if an <code>authenticator</code> is configured. Incoming requests are verified against the same provider.</p>"},{"location":"agents/#skills-management","title":"Skills Management","text":"<p>Skills represent the capabilities that an agent can perform. Skills are stored in the <code>AgentCard</code> and can be automatically detected or added.</p>"},{"location":"agents/#skills-modes","title":"Skills Modes","text":"Mode Description <code>\"auto\"</code> Automatically detects skills from tools and public methods, and adds them to the AgentCard <code>\"fixed\"</code> Uses only the skills explicitly defined in the AgentCard"},{"location":"agents/#skill-detection","title":"Skill Detection","text":"<p>When using <code>\"auto\"</code> mode, the agent detects skills from:</p> <ol> <li>Tools: Each registered tool becomes a skill</li> <li>Public Methods: Optional detection of public methods (controlled by <code>include_public_methods</code> parameter)</li> </ol> <pre><code># Auto-detect skills from tools only\nagent = Agent(card, skills=\"auto\")\n\n# Use only skills defined in AgentCard\nagent = Agent(card, skills=\"fixed\")\n</code></pre>"},{"location":"agents/#skills-in-agentcard","title":"Skills in AgentCard","text":"<p>Skills are persisted in the AgentCard and serialized when the card is exported to JSON:</p> <pre><code>from protolink.models import AgentCard, AgentSkill\n\n# Create skills manually in AgentCard\ncard = AgentCard(\n    name=\"weather_agent\",\n    description=\"Weather information agent\",\n    skills=[\n        AgentSkill(\n            id=\"get_weather\",\n            description=\"Get current weather for a location\",\n            tags=[\"weather\", \"forecast\"],\n            examples=[\"What's the weather in New York?\"]\n        )\n    ]\n)\n\n# Use fixed mode to only use these skills\nagent = Agent(card, skills=\"fixed\")\n</code></pre>"},{"location":"agents/#tool-management","title":"Tool Management","text":"<p>Tools allow agents to execute external functions and APIs.</p> Name Parameters Returns Description <code>add_tool()</code> <code>tool: BaseTool</code> <code>None</code> Registers a tool with the agent and automatically adds it as a skill to the AgentCard. <code>tool()</code> <code>name: str</code>, <code>description: str</code> <code>decorator</code> Decorator for registering Python functions as tools (automatically adds as skills). <code>call_tool()</code> <code>tool_name: str</code>, <code>**kwargs</code> <code>Any</code> Executes a registered tool by name with provided arguments. <pre><code># Using the decorator approach\n@agent.tool(\"calculate\", \"Performs basic calculations\")\ndef calculate(operation: str, a: float, b: float) -&gt; float:\n    if operation == \"add\":\n        return a + b\n    elif operation == \"multiply\":\n        return a * b\n    else:\n        raise ValueError(f\"Unsupported operation: {operation}\")\n\n# Direct tool registration\nfrom protolink.tools import BaseTool\n\nclass WeatherTool(BaseTool):\n    def call(self, location: str) -&gt; dict:\n        # Weather API logic here\n        return {\"temperature\": 72, \"conditions\": \"sunny\"}\n\nagent.add_tool(WeatherTool())\n</code></pre>"},{"location":"agents/#utility-methods","title":"Utility Methods","text":"Name Parameters Returns Description <code>get_agent_card()</code> \u2014 <code>AgentCard</code> Returns the agent's metadata card. <code>set_llm()</code> <code>llm: LLM</code> <code>None</code> Updates the agent's language model instance. <code>verify_auth()</code> <code>request: Request</code> <code>bool</code> Verifies authentication for incoming requests if an auth provider is configured."},{"location":"agents/#abstract-methods","title":"Abstract Methods","text":"<p>Subclasses of <code>Agent</code> must implement the following methods:</p> <ul> <li><code>handle_task(task: Task) -&gt; Task</code>: Defines the core logic for processing incoming tasks.</li> </ul> <p>Minimal Agent Implementation</p> <pre><code>from protolink.agents import Agent\nfrom protolink.models import AgentCard, Task, Message\n\nclass EchoAgent(Agent):\n    async def handle_task(self, task: Task) -&gt; Task:\n        # Echo back all messages\n        response_messages = []\n        for message in task.messages:\n            response_messages.append(\n                Message(\n                    content=f\"Echo: {message.content}\",\n                    role=\"assistant\"\n                )\n            )\n\n        return Task(\n            messages=response_messages,\n            parent_task_id=task.id\n        )\n</code></pre>"},{"location":"agents/#error-handling","title":"Error Handling","text":"<p>The <code>Agent</code> class includes several error handling patterns:</p> <ul> <li>Missing Transport: Raises <code>ValueError</code> if trying to start without a transport.</li> <li>Authentication Failures: Returns <code>401</code> or <code>403</code> responses for invalid auth.</li> <li>Tool Errors: Tool execution errors are propagated to the caller.</li> <li>Task Processing: Errors in <code>handle_task()</code> are caught and returned as error messages to the sender.</li> </ul>"},{"location":"agents/#authentication-integration","title":"Authentication Integration","text":"<p>When an <code>authenticator</code> is configured, the agent automatically:</p> <ol> <li>Signs outgoing requests with appropriate authentication headers</li> <li>Verifies incoming requests using the same auth mechanism</li> <li>Returns appropriate HTTP status codes for auth failures (401, 403)</li> </ol> <p>Supported auth providers include API key authentication, OAuth, and custom implementations.</p>"},{"location":"concept/","title":"Concept","text":"<p>After reading this page you should have a good understanding of the core concepts and architecture of Protolink.</p>"},{"location":"concept/#architecture-overview","title":"Architecture Overview","text":"<p>Protolink is designed around explicit separation of concerns, protocol agnosticism, and low boilerplate for agent authors. At a high level, Protolink models an agent as a logical actor that communicates with other agents via well-defined client/server interfaces, backed by pluggable transports.</p> <p>The core idea is simple:  </p> <p>Agents express intent. Clients and servers handle communication. Transports handle protocols.  </p> <p>This separation keeps agent logic clean, testable, and future-proof.</p>"},{"location":"concept/#core-concepts","title":"Core Concepts","text":"<p>Protolink is built from the following core components:</p> <ul> <li>Agent \u2014 business logic and orchestration  </li> <li>Client \u2014 outgoing communication  </li> <li>Server \u2014 incoming communication  </li> <li>Transport \u2014 protocol + runtime implementation  </li> <li>Registry \u2014 discovery and coordination  </li> </ul> <p>Each layer has a single responsibility and a clear dependency direction.</p>"},{"location":"concept/#agent","title":"Agent","text":"<p>The Agent is the central abstraction in Protolink.  </p> <p>It represents:</p> <ul> <li>Identity (via <code>AgentCard</code>)  </li> <li>Capabilities and skills  </li> <li>Task handling logic  </li> <li>Lifecycle orchestration  </li> </ul> <p>The agent does not perform networking and does not implement protocols.</p>"},{"location":"concept/#responsibilities","title":"Responsibilities","text":"<ul> <li>Define how tasks are handled (<code>handle_task</code>)  </li> <li>Manage tools, skills, and optional LLMs  </li> <li>Coordinate startup and shutdown  </li> <li>Orchestrate client/server components  </li> <li>Register and discover peers via the registry  </li> </ul>"},{"location":"concept/#what-the-agent-does-not-do","title":"What the Agent Does Not Do","text":"<ul> <li>Open sockets  </li> <li>Handle HTTP requests  </li> <li>Serialize messages  </li> <li>Know about protocols (HTTP, WS, local, etc.)  </li> </ul> <p>This is intentional and enforced by design.</p>"},{"location":"concept/#client-server-layer","title":"Client / Server Layer","text":"<p>Between the agent and the transport, Protolink introduces an explicit client/server layer. This layer removes boilerplate from agent implementations while keeping responsibilities clear.</p>"},{"location":"concept/#agentclient-outgoing","title":"AgentClient (Outgoing)","text":"<p>The <code>AgentClient</code> handles agent-to-agent outgoing communication.</p>"},{"location":"concept/#responsibilities_1","title":"Responsibilities","text":"<ul> <li>Sending tasks to other agents  </li> <li>Sending messages to other agents  </li> <li>Delegating all transport details  </li> </ul> <p>The client exposes intent-level methods only.</p> <p>Example interface (simplified):</p> <pre><code>send_task(agent_url, task)\nsend_message(agent_url, message)\n</code></pre> <p>Key point: The client knows what it wants to send but does not know how it is sent.</p>"},{"location":"concept/#agentserver-incoming","title":"AgentServer (Incoming)","text":"<p>The <code>AgentServer</code> handles incoming requests for an agent.</p>"},{"location":"concept/#responsibilities_2","title":"Responsibilities","text":"<ul> <li>Wiring the agent\u2019s task handler into the transport  </li> <li>Starting and stopping the server runtime  </li> <li>Enforcing lifecycle rules  </li> </ul> <p>The server:</p> <ul> <li>Receives tasks via the transport  </li> <li>Delegates task execution to the agent  </li> <li>Never contains business logic  </li> </ul>"},{"location":"concept/#transport-layer","title":"Transport Layer","text":"<p>The <code>AgentTransport</code> is the lowest layer in the system.  </p> <p>It encapsulates:</p> <ul> <li>Network protocol (HTTP, WS, local, etc.)  </li> <li>Runtime concerns (ASGI, threads, event loops)  </li> <li>Serialization and deserialization  </li> <li>Request routing  </li> </ul>"},{"location":"concept/#key-properties","title":"Key Properties","text":"<ul> <li>Protocol-agnostic  </li> <li>Swappable without touching agent logic  </li> <li>Reusable across agents  </li> <li>Shared by client and server  </li> </ul> <p>The transport is never accessed directly by the agent.</p>"},{"location":"concept/#dependency-direction-important","title":"Dependency Direction (Important)","text":"<p>The dependency graph is strictly one-way:</p> <p>Agent  \u251c\u2500\u2500 AgentClient  \u2502    \u2514\u2500\u2500 AgentTransport  \u2514\u2500\u2500 AgentServer       \u2514\u2500\u2500 AgentTransport</p> <p>Key points:</p> <ul> <li>The agent owns the client and server  </li> <li>The client and server own the transport  </li> <li>The agent never calls transport methods directly  </li> </ul> <p>This guarantees:</p> <ul> <li>Clean abstractions  </li> <li>Easy testing  </li> <li>No protocol leakage into business logic  </li> </ul>"},{"location":"concept/#registry","title":"Registry","text":"<p>The registry enables agent discovery and coordination. Architecturally, it mirrors the agent model.</p>"},{"location":"concept/#registry-components","title":"Registry Components","text":"<ul> <li>Registry \u2014 logical registry service  </li> <li>RegistryClient \u2014 outgoing discovery requests  </li> <li>RegistryServer \u2014 incoming registry API  </li> <li>RegistryTransport \u2014 protocol implementation  </li> </ul>"},{"location":"concept/#registry-dependency-graph","title":"Registry Dependency Graph","text":"<p>Registry  \u251c\u2500\u2500 RegistryClient  \u2502    \u2514\u2500\u2500 RegistryTransport  \u2514\u2500\u2500 RegistryServer       \u2514\u2500\u2500 RegistryTransport</p> <p>This symmetry is intentional and keeps the mental model consistent across the system.</p>"},{"location":"concept/#registry-responsibilities","title":"Registry Responsibilities","text":"<ul> <li>Agent registration  </li> <li>Agent discovery  </li> <li>Heartbeat and expiry (liveness)  </li> <li>Filtering and metadata queries  </li> </ul> <p>Agents interact with the registry only via the <code>RegistryClient</code>.</p>"},{"location":"concept/#agent-lifecycle","title":"Agent Lifecycle","text":"<p>A typical agent lifecycle looks like this:</p> <ol> <li>Instantiation with:  </li> <li><code>AgentCard</code> </li> <li>Transport  </li> <li> <p>Optional registry reference  </p> </li> <li> <p>Creation of:  </p> </li> <li><code>AgentClient</code> </li> <li> <p><code>AgentServer</code> </p> </li> <li> <p>Startup:  </p> </li> <li>Server runtime  </li> <li> <p>Registry registration  </p> </li> <li> <p>Runtime: Agent runs autonomously  </p> </li> <li> <p>Shutdown:  </p> </li> <li>Server stopped  </li> <li>Registry unregistration  </li> </ol> <p>All of this happens with minimal boilerplate for the user.</p>"},{"location":"concept/#autonomous-agents","title":"Autonomous Agents","text":"<p>Protolink supports autonomous behavior without external orchestration.</p> <p>Agents can:</p> <ul> <li>Discover peers  </li> <li>Schedule tasks  </li> <li>Send tasks to other agents  </li> <li>React to incoming tasks  </li> </ul> <p>This is done inside the agent, without manual wiring between agents. Agents behave like independent actors, not manually invoked objects.</p>"},{"location":"concept/#why-this-architecture","title":"Why This Architecture","text":"<p>This design is intentionally:</p> <ul> <li>Protocol-agnostic  </li> <li>Low boilerplate  </li> <li>Explicit  </li> <li>Composable  </li> <li>Testable  </li> </ul> <p>It draws inspiration from:</p> <ul> <li>Actor models  </li> <li>Ports &amp; adapters (hexagonal architecture)  </li> <li>Distributed systems design  </li> <li>Google A2A concepts (agent cards, tasks, discovery)  </li> </ul> <p>Most importantly:</p> <p>The agent stays simple, and complexity is pushed down into infrastructure layers.</p>"},{"location":"concept/#summary","title":"Summary","text":"<ul> <li>Agents express intent </li> <li>Clients and servers handle directionality </li> <li>Transports handle protocols </li> <li>Registries handle coordination </li> <li>Dependencies flow one way </li> <li>Boilerplate is minimized by design </li> </ul> <p>This architecture makes it easy to:</p> <ul> <li>Add new transports  </li> <li>Scale from local to distributed  </li> <li>Swap protocols  </li> <li>Keep agent logic clean and focused</li> </ul>"},{"location":"concept/#design-principles","title":"Design Principles","text":"<p>Protolink\u2019s architecture is guided by a small number of explicit design principles. These principles explain why the system looks the way it does and help contributors extend it coherently.</p>"},{"location":"concept/#1-intent-over-mechanism","title":"1. Intent Over Mechanism","text":"<p>Agents express what they want to do, never how it is done.</p> <ul> <li>Agents send tasks  </li> <li>Agents receive tasks  </li> <li>Agents discover peers  </li> </ul> <p>They do not: - Open sockets - Serialize payloads - Know transport details  </p> <p>This allows: - Clean agent logic - Easier testing - Transport substitution without rewrites  </p>"},{"location":"concept/#2-directional-communication-is-explicit","title":"2. Directional Communication Is Explicit","text":"<p>Outgoing and incoming communication are separate concerns.</p> <p>That is why Protolink has: - <code>AgentClient</code> for outgoing requests - <code>AgentServer</code> for incoming requests  </p> <p>This avoids: - Bidirectional \u201cgod objects\u201d - Hidden side effects - Transport leakage into agents  </p>"},{"location":"concept/#3-transport-is-a-boundary-not-a-feature","title":"3. Transport Is a Boundary, Not a Feature","text":"<p>Transports are infrastructure.</p> <p>They are: - Swappable - Replaceable - Reusable - Shared between client and server  </p> <p>Agents should never depend on: - HTTP - ASGI - WebSockets - Threads - Event loops  </p> <p>This keeps the system future-proof.</p>"},{"location":"concept/#4-registry-mirrors-agent-architecture","title":"4. Registry Mirrors Agent Architecture","text":"<p>The registry is not \u201cspecial\u201d.</p> <p>It follows the same architectural rules as agents:</p> <ul> <li>Logical registry object  </li> <li>Client for outgoing calls  </li> <li>Server for incoming calls  </li> <li>Transport underneath  </li> </ul> <p>This symmetry: - Reduces cognitive load - Improves maintainability - Makes distributed registries natural  </p>"},{"location":"concept/#5-minimal-boilerplate-explicit-control","title":"5. Minimal Boilerplate, Explicit Control","text":"<p>Protolink aims to reduce boilerplate without hiding control.</p> <ul> <li>Defaults are sensible  </li> <li>Explicit overrides are possible  </li> <li>No magic global state  </li> <li>No hidden background threads  </li> </ul> <p>You always know: - What is running - What is registered - What is communicating  </p>"},{"location":"concept/#agent-agent-sequence-diagram","title":"Agent \u2194 Agent Sequence Diagram","text":"<p>This section describes the runtime flow when one agent sends a task to another.</p>"},{"location":"concept/#scenario","title":"Scenario","text":"<p>Agent A wants to send a task to Agent B.</p> <p>Both agents are already running and registered.</p>"},{"location":"concept/#sequence","title":"Sequence","text":"<ol> <li>Agent A creates a <code>Task</code> </li> <li>Agent A calls <code>send_task_to(agent_b_url, task)</code> </li> <li><code>AgentClient</code> forwards the task to its transport  </li> <li>Transport sends the request to Agent B\u2019s server endpoint  </li> <li>Agent B\u2019s transport receives the request  </li> <li><code>AgentServer</code> invokes Agent B\u2019s <code>handle_task</code> </li> <li>Agent B processes the task  </li> <li>The result task is returned through the same path  </li> <li>Agent A receives the completed task  </li> </ol>"},{"location":"concept/#responsibility-breakdown","title":"Responsibility Breakdown","text":"<ul> <li>Agent: defines what to do  </li> <li>Client: defines direction </li> <li>Server: defines entry point </li> <li>Transport: defines mechanism </li> </ul> <p>No layer violates its responsibility.</p>"},{"location":"concept/#registry-interaction-sequence","title":"Registry Interaction Sequence","text":"<p>This section explains how discovery works at runtime.</p>"},{"location":"concept/#agent-startup","title":"Agent Startup","text":"<ol> <li>Agent starts its server  </li> <li>Agent creates a <code>RegistryClient</code> </li> <li>Agent registers its <code>AgentCard</code> </li> <li>Registry stores the agent metadata  </li> <li>Optional heartbeat begins  </li> </ol>"},{"location":"concept/#discovery","title":"Discovery","text":"<ol> <li>Agent requests discovery via <code>RegistryClient</code> </li> <li>Registry applies filters  </li> <li>Registry returns matching <code>AgentCard</code>s  </li> <li>Agent decides what to do next  </li> </ol> <p>The registry never pushes behavior to agents.</p>"},{"location":"concept/#comparison-with-raw-google-a2a","title":"Comparison With Raw Google A2A","text":"<p>Protolink is inspired by Google\u2019s A2A spec, but intentionally diverges in structure.</p>"},{"location":"concept/#what-is-preserved","title":"What Is Preserved","text":"<ul> <li>Agent Cards  </li> <li>Task-based communication  </li> <li>Explicit discovery  </li> <li>Stateless requests  </li> <li>Protocol neutrality  </li> </ul>"},{"location":"concept/#what-is-improved","title":"What Is Improved","text":""},{"location":"concept/#1-central-agent-abstraction","title":"1. Central Agent Abstraction","text":"<p>In Protolink, the agent is the primary unit, not a loose collection of endpoints.</p> <p>This: - Improves composability - Makes agents easier to reason about - Encourages reusable agent logic  </p>"},{"location":"concept/#2-explicit-client-server-split","title":"2. Explicit Client / Server Split","text":"<p>Google A2A often conflates: - Sending - Receiving - Hosting  </p> <p>Protolink separates them cleanly, which: - Improves testability - Clarifies ownership - Reduces hidden coupling  </p>"},{"location":"concept/#3-registry-as-a-first-class-component","title":"3. Registry as a First-Class Component","text":"<p>Instead of being an afterthought, the registry is: - Structured - Extensible - Transport-agnostic - Distributed-ready  </p>"},{"location":"concept/#4-lower-boilerplate-for-users","title":"4. Lower Boilerplate for Users","text":"<p>A typical Protolink agent requires: - One subclass - One <code>handle_task</code> method - One transport  </p> <p>Everything else is handled by composition.</p>"},{"location":"concept/#mental-model-summary","title":"Mental Model Summary","text":"<p>If you remember only one thing:</p> <p>Agents think. Clients talk. Servers listen. Transports move bytes. Registries coordinate.</p> <p>Each layer is small, focused, and replaceable.</p> <p>That is the entire philosophy.</p>"},{"location":"concept/#protolink-vs-google-a2a-concepts","title":"Protolink vs Google A2A Concepts","text":"<p>Protolink is inspired by Google\u2019s A2A (Agent-to-Agent) concepts, but adds practical layers and abstractions to make building autonomous agents easier and more maintainable.</p> Concept Google A2A Protolink Notes Agent Logical actor with tasks Logical actor with tasks, tools, skills, and optional LLMs In Protolink, agents can include AI capabilities, not just task orchestration. Communication Agent-to-Agent messages AgentClient / AgentServer with pluggable transports Explicit client/server layer reduces boilerplate and separates network logic from business logic. Discovery Registry / Service Directory Registry, RegistryClient, RegistryServer Symmetric design; agents never talk to the registry directly except through <code>RegistryClient</code>. Task Handling Internal message routing <code>handle_task</code> logic inside the Agent Agents remain autonomous; external orchestration is optional. Protocol Implicit (HTTP, WS, etc.) AgentTransport layer handles protocol, serialization, runtime Protocol-agnostic and swappable without touching agent logic. Extensibility Limited by A2A spec Tools, LLMs, and custom skills Agents can mix AI and deterministic tools seamlessly. Boilerplate Manual wiring, repetitive Minimal; agent owns clients and servers, which own transports Focuses on developer productivity and clarity. Autonomy Agents are actors, often invoked manually Agents run autonomously, discover peers, schedule and execute tasks Protolink pushes complexity down into infrastructure layers."},{"location":"concept/#key-differences","title":"Key Differences","text":"<ol> <li>LLM Integration </li> <li>In Protolink, an agent can include LLMs as part of its tools/skills, enabling advanced AI behavior.  </li> <li> <p>Google A2A does not define AI capabilities natively.</p> </li> <li> <p>Explicit Client/Server Layer </p> </li> <li>Protolink separates intent from transport, reducing boilerplate and making testing easier.  </li> <li> <p>Google A2A mixes communication concerns with agent logic in some implementations.</p> </li> <li> <p>Registry Symmetry </p> </li> <li>Registry, RegistryClient, and RegistryServer mirror the agent architecture for consistency.  </li> <li> <p>A2A often has ad-hoc discovery mechanisms.</p> </li> <li> <p>Protocol-Agnostic Transport </p> </li> <li>Protolink agents never handle HTTP, WS, or serialization directly.  </li> <li> <p>All networking is delegated to <code>AgentTransport</code> or <code>RegistryTransport</code>.</p> </li> <li> <p>Extensibility with Tools and Skills </p> </li> <li>Developers can define custom tools, attach LLMs, or integrate external APIs without touching transport logic.  </li> </ol>"},{"location":"concept/#developer-takeaways","title":"Developer Takeaways","text":"<ul> <li>Agents are rich actors: Tasks, tools, skills, LLMs  </li> <li>Clients/Servers handle communication, not the agent  </li> <li>Transport is pluggable, reusable, and protocol-agnostic  </li> <li>Registry abstracts discovery and coordination  </li> <li>Minimal boilerplate allows focusing on agent logic, not infrastructure</li> </ul> <p>Info</p> <p>By explicitly layering Client, Server, Transport, and Registry, Protolink provides a professional-grade framework for autonomous agent development, while keeping A2A concepts at its core.</p>"},{"location":"examples/","title":"Examples","text":"<p>This section links to example projects and code snippets in the repository.</p>"},{"location":"examples/#http-agents","title":"HTTP Agents","text":"<p>The repository includes several examples under the <code>examples/</code> directory. For HTTP\u2011based agents:</p> <ul> <li><code>examples/http_agents.py</code> \u2014 basic HTTP transport example showing how to spin up an HTTP\u2011enabled agent.</li> <li><code>examples/http_math_agents.py</code> \u2014 example of delegating between agents over HTTP (e.g. a question agent calling a math agent).</li> </ul>"},{"location":"examples/#other-examples","title":"Other Examples","text":"<p>Additional examples illustrate other capabilities:</p> <ul> <li><code>examples/basic_agent.py</code> \u2014 minimal agent setup focused on core concepts.</li> <li><code>examples/llms.py</code> \u2014 examples of wiring different LLM backends into agents.</li> <li><code>examples/runtime_agents.py</code> \u2014 demonstrates using <code>RuntimeTransport</code> for in\u2011process agent communication.</li> <li><code>examples/streaming_agent.py</code> \u2014 shows streaming behaviour (e.g. via WebSocket or other streaming\u2011capable transports).</li> <li><code>examples/oauth_agent.py</code> \u2014 demonstrates OAuth 2.0 and API\u2011key based security in front of agents.</li> </ul> <p>You can run and adapt these scripts as starting points for your own agent systems.</p> <p>New here?</p> <p>Start with <code>examples/basic_agent.py</code> to understand the core concepts, then move on to <code>examples/http_agents.py</code> for HTTP-based setups.</p>  \ud83c\udf89\ud83c\udf89 Congratulations, You made it! \ud83c\udf89\ud83c\udf89  <p> Want to see more? Stay tuned, as the project is actively maintained and everything is changing rapidly!   </p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide shows how to install and start using Protolink.</p>"},{"location":"getting-started/#installation","title":"Installation","text":"<p>Protolink is published on PyPI and can be installed with either <code>uv</code> (recommended) or <code>pip</code>.</p>"},{"location":"getting-started/#basic-installation","title":"Basic Installation","text":"<p>This installs the base package without optional extras:</p> <pre><code># Using uv (recommended)\nuv add protolink\n\n# Using pip\npip install protolink\n</code></pre>"},{"location":"getting-started/#optional-dependencies","title":"Optional Dependencies","text":"<p>Protolink exposes several extras to enable additional functionality:</p> uvpipuv &amp; pip <pre><code># Install with all optional dependencies\nuv add \"protolink[all]\"\n\n# HTTP support (for web-based agents)\nuv add \"protolink[http]\"\n\n# All supported LLM libraries\nuv add \"protolink[llms]\"\n\n# Development (all extras + testing tools)\nuv add \"protolink[dev]\"\n</code></pre> <pre><code># Install with all optional dependencies\npip install -e \"protolink[all]\"\n\n# HTTP support (for web-based agents)\npip install -e \"protolink[http]\"\n\n# All supported LLM libraries\npip install -e \"protolink[llms]\"\n\n# Development (all extras + testing tools)\npip install -e \"protolink[dev]\"\n</code></pre> <p>Because.. why not?</p> <pre><code># Install with all optional dependencies\nuv pip install -e \"protolink[all]\"\n\n# HTTP support (for web-based agents)\nuv pip install -e \"protolink[http]\"\n\n# All supported LLM libraries\nuv pip install -e \"protolink[llms]\"\n\n# Development (all extras + testing tools)\nuv pip install -e \"protolink[dev]\"\n</code></pre> <p>Optional extras</p> <p>You usually only need the extras that match your use case. The <code>protolink[llms]</code> will install all the supported LLM libraries (OpenAI, Anthropic, Ollama etc.) so it is advised to install manually the libraries that are needed for your project.</p> <p>For development from source:</p> <pre><code>git clone https://github.com/nmaroulis/protolink.git\ncd protolink\nuv pip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/#basic-example","title":"Basic Example","text":"<p>Below is a minimal example that wires together an agent, HTTP transport, an OpenAI LLM, and both native and MCP tools:</p> <pre><code>from protolink.agents import Agent\nfrom protolink.models import AgentCard\nfrom protolink.transport import HTTPAgentTransport\nfrom protolink.tools.adapters import MCPToolAdapter\nfrom protolink.llms.api import OpenAILLM\n\n\n# Define the agent card\nagent_card = AgentCard(\n    name=\"example_agent\",\n    description=\"A dummy agent\",\n)\n\n\n# Initialize the transport\ntransport = HTTPAgentTransport()\n\n\n# OpenAI API LLM\nllm = OpenAILLM(model=\"gpt-5.2\")\n\n\n# Initialize the agent\nagent = Agent(agent_card, transport, llm)\n\n\n# Add Native tool\n@agent.tool(name=\"add\", description=\"Add two numbers\")\nasync def add_numbers(a: int, b: int):\n    return a + b\n\n\n# Add MCP tool\nmcp_tool = MCPToolAdapter(mcp_client, \"multiply\")\nagent.add_tool(mcp_tool)\n\n\n# Start the agent\nagent.start()\n</code></pre> <p>This example demonstrates the core pieces of Protolink:</p> <ul> <li>AgentCard to describe the agent.</li> <li>Transport (here <code>HTTPAgentTransport</code>) to handle A2A (agent-to-agent) communication.</li> <li>LLM backend (<code>OpenAILLM</code>).</li> <li>Native tools (Python functions decorated with <code>@agent.tool</code>).</li> <li>MCP tools registered via <code>MCPToolAdapter</code>.</li> </ul>"},{"location":"llms/","title":"LLMs","text":"<p>Protolink integrates with various LLM backends.</p>"},{"location":"llms/#llm-types","title":"LLM Types","text":"<p>Protolink groups LLM backends into three broad categories:</p>    [ API ]   [ Server ]   [ Local ]  <ul> <li> <p>API \u2014 calls a remote API and requires an API key:</p> <ul> <li><code>OpenAILLM</code>: uses the OpenAI API for sync &amp; async requests.</li> <li><code>AnthropicLLM</code>: uses the Anthropic API for sync &amp; async requests.</li> </ul> </li> <li> <p>Server \u2014 connects to an LLM server, locally or remotely:</p> <ul> <li><code>OllamaLLM</code>: connects to an Ollama server for sync &amp; async requests.</li> </ul> </li> <li> <p>Local \u2014 runs the model directly in your runtime:</p> <ul> <li><code>LlamaCPPLLM</code>: uses a local llama.cpp runtime for sync &amp; async requests.</li> </ul> </li> </ul> <p>You can also use other LLM clients directly without going through Protolink\u2019s <code>LLM</code> wrappers if you prefer.</p>"},{"location":"llms/#configuration","title":"Configuration","text":"<p>Configuration depends on the specific backend, but the general pattern is:</p> <ol> <li>Install the relevant extras (from the README):</li> </ol> <pre><code># All supported LLM backends\nuv add \"protolink[llms]\"\n</code></pre> <p>Choosing LLM extras</p> <p>If you only need a subset of LLMs (e.g. OpenAI API), it is advised to install them manually instead of using the <code>llms</code> extra, which will intall all the supported libraries.</p> <ol> <li>Instantiate the LLM with the desired model and credentials:</li> </ol> <pre><code>from protolink.llms.api import OpenAILLM\n\n\nllm = OpenAILLM(\n    model=\"gpt-5.2\",\n    # api_key is typically read from the environment, e.g. OPENAI_API_KEY\n)\n</code></pre> <p>API keys</p> <p>Never commit API keys to version control. Read them from environment variables or a secure secrets manager.</p> <ol> <li>Pass the LLM to your Agent:</li> </ol> <pre><code>from protolink.agents import Agent\nfrom protolink.models import AgentCard\nfrom protolink.transport import HTTPAgentTransport\n\nurl = \"http://localhost:8020\"\nagent_card = AgentCard(name=\"llm_agent\", description=\"Agent backed by an LLM\", url=url)\ntransport = HTTPAgentTransport(url=url)\n\nagent = Agent(agent_card, transport, llm)\n</code></pre> <p>For local and server\u2011style LLMs (<code>LlamaCPPLLM</code>, <code>OllamaLLM</code>), configuration additionally includes paths to model files or server URLs. Refer to the corresponding example scripts in <code>examples/llms.py</code> for concrete usage patterns.</p>"},{"location":"llms/#llm-api-reference","title":"LLM API Reference","text":"<p>This section provides a detailed API reference for all LLM classes in Protolink. All LLM implementations inherit from the base <code>LLM</code> class and provide a consistent interface for generating responses.</p> <p>Unified LLM Interface</p> <p>Protolink provides a single, consistent API for all LLM providers. Whether you're using OpenAI, Anthropic, Ollama, or local models, you interact with them through the same methods: <code>generate_response()</code>, <code>generate_stream_response()</code>, and configuration helpers. This unified approach means you can swap LLM providers without changing your application code - just update the initialization and you're done!</p> <p>Why Use Protolink's LLM Wrappers?</p> <ul> <li>Provider Agnostic: Switch between OpenAI, Anthropic, Ollama, and future providers with minimal code changes</li> <li>Consistent Interface: Same method signatures and behavior across all implementations</li> <li>Built-in Features: Connection validation, parameter validation, and error handling out of the box</li> <li>Extensible: Easy to add new LLM providers while maintaining compatibility</li> <li>Production Ready: Robust error handling and logging for real-world applications</li> </ul> <p>Provider Switching in Action</p> <pre><code># The same code works with ANY LLM provider\n\n# Choose your provider - just change the import and initialization!\nfrom protolink.llms.api import OpenAILLM    # or AnthropicLLM\nfrom protolink.llms.server import OllamaLLM  # or any other provider\n\n# Initialize your chosen LLM\nllm = OpenAILLM(model=\"gpt-4\", temperature=0.7)\n# llm = AnthropicLLM(model=\"claude-3-sonnet\", temperature=0.7)\n# llm = OllamaLLM(model=\"llama3\", temperature=0.7)\n\n# The rest of your code stays EXACTLY the same!\nmessages = [Message(role=\"user\", content=\"Hello!\")]\nresponse = llm.generate_response(messages)\nprint(response.content)\n\n# Streaming also works identically\nfor chunk in llm.generate_stream_response(messages):\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <p>LLM Hierarchy</p> <ul> <li><code>LLM</code> - abstract base class</li> <li><code>APILLM</code> - base for API-based LLMs</li> <li><code>ServerLLM</code> - base for server-based LLMs</li> <li><code>LocalLLM</code> - base for local runtime LLMs</li> <li>Concrete implementations: <code>OpenAILLM</code>, <code>AnthropicLLM</code>, <code>OllamaLLM</code>, etc.</li> </ul>"},{"location":"llms/#base-llm-class","title":"Base LLM Class","text":"<p>The <code>LLM</code> class defines the common interface that all LLM implementations must follow.</p>"},{"location":"llms/#attributes","title":"Attributes","text":"Attribute Type Description <code>model_type</code> <code>LLMType</code> The type of LLM (<code>\"api\"</code>, <code>\"local\"</code>, or <code>\"server\"</code>). <code>provider</code> <code>LLMProvider</code> The provider name (<code>\"openai\"</code>, <code>\"anthropic\"</code>, <code>\"ollama\"</code>, etc.). <code>model</code> <code>str</code> The model name/identifier. <code>model_params</code> <code>dict[str, Any]</code> Model-specific parameters (temperature, max_tokens, etc.). <code>system_prompt</code> <code>str</code> Default system prompt for the model."},{"location":"llms/#core-methods","title":"Core Methods","text":"Name Parameters Returns Description <code>generate_response()</code> <code>messages: list[Message]</code> <code>Message</code> Generate a single response from the model. <code>generate_stream_response()</code> <code>messages: list[Message]</code> <code>Iterable[Message]</code> Generate a streaming response, yielding messages as they're generated. <code>set_model_params()</code> <code>model_params: dict[str, Any]</code> <code>None</code> Update model parameters. <code>set_system_prompt()</code> <code>system_prompt: str</code> <code>None</code> Set the system prompt for the model. <code>validate_connection()</code> \u2014 <code>bool</code> Validate that the LLM connection is working. <p>Abstract Methods</p> <p>The <code>LLM</code> base class is abstract. You should use one of the concrete implementations like <code>OpenAILLM</code> or <code>AnthropicLLM</code>.</p>"},{"location":"llms/#api-based-llms","title":"API-based LLMs","text":"<p>API-based LLMs connect to external services and require API keys or authentication.</p>"},{"location":"llms/#apillm-base-class","title":"APILLM Base Class","text":"<p>Base class for all API-based LLM implementations.</p> Name Parameters Returns Description <code>set_model_params()</code> <code>model_params: dict[str, Any]</code> <code>None</code> Update existing model parameters, ignoring invalid keys. <code>set_system_prompt()</code> <code>system_prompt: str</code> <code>None</code> Set the system prompt for the model. <code>validate_connection()</code> \u2014 <code>bool</code> Abstract. Validate API connection (implemented by subclasses)."},{"location":"llms/#openaillm","title":"OpenAILLM","text":"<p>OpenAI API implementation using the official OpenAI client.</p>"},{"location":"llms/#constructor","title":"Constructor","text":"Parameter Type Default Description <code>api_key</code> <code>str \\| None</code> <code>None</code> OpenAI API key. If not provided, uses <code>OPENAI_API_KEY</code> environment variable. <code>model</code> <code>str \\| None</code> <code>\"gpt-5\"</code> OpenAI model name. <code>model_params</code> <code>dict[str, Any] \\| None</code> <code>None</code> Model parameters (temperature, max_tokens, etc.). <code>base_url</code> <code>str \\| None</code> <code>None</code> Custom base URL for OpenAI-compatible APIs. <pre><code>from protolink.llms.api import OpenAILLM\n\n# Basic usage\nllm = OpenAILLM(model=\"gpt-4\")\n\n# With custom parameters\nllm = OpenAILLM(\n    model=\"gpt-4-turbo\",\n    model_params={\n        \"temperature\": 0.7,\n        \"max_tokens\": 1000,\n        \"top_p\": 0.9\n    }\n)\n\n# With custom base URL (for OpenAI-compatible APIs)\nllm = OpenAILLM(\n    model=\"custom-model\",\n    base_url=\"https://api.custom-provider.com/v1\",\n    api_key=\"your-api-key\"\n)\n</code></pre>"},{"location":"llms/#default-model-parameters","title":"Default Model Parameters","text":"Parameter Type Default Range/Description <code>temperature</code> <code>float</code> <code>1.0</code> <code>0.0</code> to <code>2.0</code> - Controls randomness <code>top_p</code> <code>float</code> <code>1.0</code> Nucleus sampling parameter <code>n</code> <code>int</code> <code>1</code> Number of completions to generate <code>stream</code> <code>bool</code> <code>False</code> Whether to stream responses <code>stop</code> <code>str \\| list[str] \\| None</code> <code>None</code> Stop sequences <code>max_tokens</code> <code>int \\| None</code> <code>None</code> Maximum tokens to generate <code>presence_penalty</code> <code>float</code> <code>0.0</code> <code>-2.0</code> to <code>2.0</code> - Presence penalty <code>frequency_penalty</code> <code>float</code> <code>0.0</code> <code>-2.0</code> to <code>2.0</code> - Frequency penalty <code>logit_bias</code> <code>dict \\| None</code> <code>None</code> Token bias dictionary"},{"location":"llms/#methods","title":"Methods","text":"Name Parameters Returns Description <code>generate_response()</code> <code>messages: list[Message]</code> <code>Message</code> Generate a single response using OpenAI's API. <code>generate_stream_response()</code> <code>messages: list[Message]</code> <code>Iterable[Message]</code> Generate streaming response, yielding partial messages. <code>validate_connection()</code> \u2014 <code>bool</code> Check if the model is available and API key is valid. <p>API Key Required</p> <p>OpenAI requires a valid API key. Set the <code>OPENAI_API_KEY</code> environment variable or pass the <code>api_key</code> parameter.</p>"},{"location":"llms/#anthropicllm","title":"AnthropicLLM","text":"<p>Anthropic Claude API implementation using the official Anthropic client.</p>"},{"location":"llms/#constructor_1","title":"Constructor","text":"Parameter Type Default Description <code>api_key</code> <code>str \\| None</code> <code>None</code> Anthropic API key. If not provided, uses <code>ANTHROPIC_API_KEY</code> environment variable. <code>model</code> <code>str \\| None</code> <code>\"claude-sonnet-4-20250514\"</code> Claude model name. <code>model_params</code> <code>dict[str, Any] \\| None</code> <code>None</code> Model parameters (temperature, max_tokens, etc.). <code>base_url</code> <code>str \\| None</code> <code>None</code> Custom base URL for Anthropic-compatible APIs. <pre><code>from protolink.llms.api import AnthropicLLM\n\n# Basic usage\nllm = AnthropicLLM(model=\"claude-3-5-sonnet-20241022\")\n\n# With custom parameters\nllm = AnthropicLLM(\n    model=\"claude-3-5-haiku-20241022\",\n    model_params={\n        \"temperature\": 0.5,\n        \"max_tokens\": 2000,\n        \"top_p\": 0.8\n    }\n)\n</code></pre>"},{"location":"llms/#default-model-parameters_1","title":"Default Model Parameters","text":"Parameter Type Default Range/Description <code>max_tokens</code> <code>int</code> <code>4096</code> Maximum tokens to generate <code>temperature</code> <code>float</code> <code>1.0</code> <code>0.0</code> to <code>1.0</code> - Controls randomness <code>top_p</code> <code>float</code> <code>1.0</code> Nucleus sampling parameter <code>top_k</code> <code>int \\| None</code> <code>None</code> Top-k sampling parameter <code>stop_sequences</code> <code>list[str] \\| None</code> <code>None</code> Stop sequences <code>metadata</code> <code>dict \\| None</code> <code>None</code> Additional metadata"},{"location":"llms/#methods_1","title":"Methods","text":"Name Parameters Returns Description <code>generate_response()</code> <code>messages: list[Message]</code> <code>Message</code> Generate a single response using Anthropic's API. <code>generate_stream_response()</code> <code>messages: list[Message]</code> <code>Iterable[Message]</code> Generate streaming response, yielding partial messages. <code>validate_connection()</code> \u2014 <code>bool</code> Check if the model is available and API key is valid. <p>API Key Required</p> <p>Anthropic requires a valid API key. Set the <code>ANTHROPIC_API_KEY</code> environment variable or pass the <code>api_key</code> parameter.</p>"},{"location":"llms/#server-based-llms","title":"Server-based LLMs","text":"<p>Server-based LLMs connect to local or remote LLM servers.</p>"},{"location":"llms/#serverllm-base-class","title":"ServerLLM Base Class","text":"<p>Base class for all server-based LLM implementations.</p>"},{"location":"llms/#constructor_2","title":"Constructor","text":"Parameter Type Default Description <code>base_url</code> <code>str</code> \u2014 Required. URL of the LLM server."},{"location":"llms/#methods_2","title":"Methods","text":"Name Parameters Returns Description <code>set_model_params()</code> <code>model_params: dict[str, Any]</code> <code>None</code> Update existing model parameters, ignoring invalid keys. <code>set_system_prompt()</code> <code>system_prompt: str</code> <code>None</code> Set the system prompt for the model. <code>validate_connection()</code> \u2014 <code>bool</code> Validate that the server is reachable."},{"location":"llms/#ollamallm","title":"OllamaLLM","text":"<p>Ollama server implementation for connecting to local or remote Ollama instances.</p>"},{"location":"llms/#constructor_3","title":"Constructor","text":"Parameter Type Default Description <code>base_url</code> <code>str \\| None</code> <code>None</code> Ollama server URL. If not provided, uses <code>OLLAMA_HOST</code> environment variable. <code>headers</code> <code>dict[str, str] \\| None</code> <code>None</code> Additional HTTP headers (including auth). <code>model</code> <code>str \\| None</code> <code>\"gemma3\"</code> Ollama model name. <code>model_params</code> <code>dict[str, Any] \\| None</code> <code>None</code> Model parameters (temperature, etc.). <pre><code>from protolink.llms.server import OllamaLLM\n\n# Local Ollama server\nllm = OllamaLLM(\n    base_url=\"http://localhost:11434\",\n    model=\"llama3\"\n)\n\n# Remote Ollama with authentication\nllm = OllamaLLM(\n    base_url=\"https://ollama.example.com\",\n    headers={\"Authorization\": \"Bearer your-token\"},\n    model=\"codellama\"\n)\n\n# Using environment variables\n# Set OLLAMA_HOST=http://localhost:11434\nllm = OllamaLLM(model=\"mistral\")\n</code></pre>"},{"location":"llms/#default-model-parameters_2","title":"Default Model Parameters","text":"Parameter Type Default Description <code>temperature</code> <code>float</code> <code>1.0</code> Controls randomness (range depends on model)."},{"location":"llms/#methods_3","title":"Methods","text":"Name Parameters Returns Description <code>generate_response()</code> <code>messages: list[Message]</code> <code>Message</code> Generate a single response using Ollama's API. <code>generate_stream_response()</code> <code>messages: list[Message]</code> <code>Iterable[Message]</code> Generate streaming response, yielding partial messages. <code>validate_connection()</code> \u2014 <code>bool</code> Check if Ollama server is reachable and has models available. <p>Ollama Server Required</p> <p>OllamaLLM requires a running Ollama server. Install Ollama and start it with <code>ollama serve</code>.</p>"},{"location":"llms/#usage-examples","title":"Usage Examples","text":""},{"location":"llms/#basic-llm-usage","title":"Basic LLM Usage","text":"<pre><code>from protolink.llms.api import OpenAILLM\nfrom protolink.models import Message\n\n# Initialize LLM\nllm = OpenAILLM(model=\"gpt-4\")\n\n# Create messages\nmessages = [\n    Message(role=\"user\", content=\"Hello, how are you?\")\n]\n\n# Generate response\nresponse = llm.generate_response(messages)\nprint(response.content)\n</code></pre>"},{"location":"llms/#streaming-responses","title":"Streaming Responses","text":"<pre><code># Generate streaming response\nfor chunk in llm.generate_stream_response(messages):\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"llms/#updating-parameters","title":"Updating Parameters","text":"<pre><code># Update model parameters\nllm.set_model_params({\n    \"temperature\": 0.7,\n    \"max_tokens\": 500\n})\n\n# Update system prompt\nllm.set_system_prompt(\"You are a helpful coding assistant.\")\n</code></pre>"},{"location":"llms/#connection-validation","title":"Connection Validation","text":"<pre><code># Validate connection before use\nif llm.validate_connection():\n    print(\"LLM is ready!\")\nelse:\n    print(\"LLM connection failed.\")\n</code></pre>"},{"location":"llms/#error-handling","title":"Error Handling","text":"<p>All LLM implementations include error handling for common issues:</p> <ul> <li>Authentication Errors: Missing or invalid API keys</li> <li>Connection Errors: Network issues or unavailable servers</li> <li>Model Errors: Invalid model names or unavailable models</li> <li>Parameter Errors: Invalid parameter values</li> </ul> <p>Connection Validation</p> <p>Always call <code>validate_connection()</code> before using an LLM to ensure it's properly configured and reachable.</p>"},{"location":"llms/#type-aliases","title":"Type Aliases","text":"<p>The LLM module defines several type aliases for clarity:</p> <pre><code>LLMType: TypeAlias = Literal[\"api\", \"local\", \"server\"]\nLLMProvider: TypeAlias = Literal[\"openai\", \"anthropic\", \"google\", \"llama.cpp\", \"ollama\"]\n</code></pre> <p>These are used throughout the LLM implementations to ensure type safety and clarity.</p>"},{"location":"models/","title":"Models API Reference","text":"<p>This section provides detailed API documentation for the core data models in Protolink. These models represent the fundamental data structures used throughout the framework for agent communication, task management, and data exchange.</p>"},{"location":"models/#table-of-contents","title":"Table of Contents","text":"<ul> <li>MimeType</li> <li>AgentCard</li> <li>AgentCapabilities</li> <li>AgentSkill</li> <li>Task</li> <li>TaskState</li> <li>Message</li> <li>Part</li> <li>Artifact</li> <li>Context</li> </ul>"},{"location":"models/#agentcard","title":"AgentCard","text":"<pre><code>@dataclass\nclass AgentCard:\n    name: str\n    description: str\n    url: str\n    version: str = \"1.0.0\"\n    protocol_version: str = protolink_version\n    capabilities: AgentCapabilities = field(default_factory=AgentCapabilities)\n    skills: list[AgentSkill] = field(default_factory=list)\n    input_formats: list[MimeType] = field(default_factory=lambda: [\"text/plain\"])\n    output_formats: list[MimeType] = field(default_factory=lambda: [\"text/plain\"])\n    security_schemes: dict[str, dict[str, Any]] | None = field(default_factory=dict)\n</code></pre> <p>Agent identity and capability declaration. This is the main metadata card that describes an agent's identity, capabilities, and security requirements.</p>"},{"location":"models/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>name</code> <code>str</code> \u2014 Required. Agent name <code>description</code> <code>str</code> \u2014 Required. Agent purpose/description <code>url</code> <code>str</code> \u2014 Required. Service endpoint URL <code>version</code> <code>str</code> <code>\"1.0.0\"</code> Agent version <code>protocol_version</code> <code>str</code> <code>protolink_version</code> Protolink Protocol version <code>capabilities</code> <code>AgentCapabilities</code> <code>AgentCapabilities()</code> Supported features <code>skills</code> <code>list[AgentSkill]</code> <code>[]</code> List of skills the agent can perform <code>input_formats</code> <code>list[MimeType]</code> <code>[\"text/plain\"]</code> Supported input MIME types <code>output_formats</code> <code>list[MimeType]</code> <code>[\"text/plain\"]</code> Supported output MIME types <code>security_schemes</code> <code>dict[SecuritySchemeType, dict[str, Any]] | None</code> <code>{}</code> Authentication schemes"},{"location":"models/#methods","title":"Methods","text":""},{"location":"models/#to_json-dictstr-any","title":"<code>to_json() -&gt; dict[str, Any]</code>","text":"<p>Convert the AgentCard to JSON format compatible with the A2A agent card specification.</p> <p>Returns: <pre><code>dict[str, Any]  # JSON dictionary representation\n</code></pre></p> <p>Example: <pre><code>card = AgentCard(name=\"weather_agent\", description=\"Weather service\")\njson_data = card.to_json()\nprint(json_data[\"name\"])  # \"weather_agent\"\n</code></pre></p>"},{"location":"models/#from_jsondata-dictstr-any-agentcard-classmethod","title":"<code>from_json(data: dict[str, Any]) -&gt; AgentCard</code> <code>classmethod</code>","text":"<p>Create an AgentCard from JSON data. This method can also handle regular Python dictionaries and includes basic field validation via <code>_validate_fields</code>. <pre><code>data: dict[str, Any]  # JSON dictionary or Python dict containing agent card data\n</code></pre></p> <p>Returns: <pre><code>AgentCard  # New AgentCard instance\n</code></pre></p> <p>Example: <pre><code>json_data = {\n    \"name\": \"weather_agent\",\n    \"description\": \"Weather service\",\n    \"url\": \"https://api.example.com/weather\"\n}\ncard = AgentCard.from_json(json_data)\n</code></pre></p>"},{"location":"models/#_validate_fieldsdata-dictstr-any-none","title":"<code>_validate_fields(data: dict[str, Any]) -&gt; None</code>","text":"<p>Validate the fields of the AgentCard.</p> <p>Returns: <pre><code>None\n</code></pre></p>"},{"location":"models/#example","title":"Example","text":"<pre><code>from protolink.models import AgentCard, AgentCapabilities\n\ncard = AgentCard(\n    name=\"weather_agent\",\n    description=\"Provides weather information\",\n    url=\"https://api.example.com/weather\",\n    version=\"1.2.0\",\n    input_formats=[\"text/plain\", \"application/json\"],\n    output_formats=[\"text/plain\", \"application/json\", \"text/markdown\"],\n    capabilities=AgentCapabilities(\n        streaming=True,\n        tool_calling=True,\n        max_concurrency=5\n    )\n)\n\n# Convert to JSON\njson_data = card.to_json()\n</code></pre>"},{"location":"models/#agentcapabilities","title":"AgentCapabilities","text":"<pre><code>@dataclass\nclass AgentCapabilities:\n    streaming: bool = False\n    push_notifications: bool = False\n    state_transition_history: bool = False\n    has_llm: bool = False\n    max_concurrency: int = 1\n    message_batching: bool = False\n    tool_calling: bool = False\n    multi_step_reasoning: bool = False\n    timeout_support: bool = False\n    delegation: bool = False\n    rag: bool = False\n    code_execution: bool = False\n</code></pre> <p>Defines the capabilities and limitations of an agent. This extends the A2A specification with additional capability flags.</p>"},{"location":"models/#parameters_1","title":"Parameters","text":"Parameter Type Default Description <code>streaming</code> <code>bool</code> <code>False</code> Supports Server-Sent Events (SSE) streaming <code>push_notifications</code> <code>bool</code> <code>False</code> Supports push notifications (webhooks) <code>state_transition_history</code> <code>bool</code> <code>False</code> Provides detailed task state history <code>has_llm</code> <code>bool</code> <code>False</code> Has an LLM component for AI processing <code>max_concurrency</code> <code>int</code> <code>1</code> Maximum concurrent tasks <code>message_batching</code> <code>bool</code> <code>False</code> Processes multiple messages per request <code>tool_calling</code> <code>bool</code> <code>False</code> Can call external tools/APIs <code>multi_step_reasoning</code> <code>bool</code> <code>False</code> Performs multi-step reasoning <code>timeout_support</code> <code>bool</code> <code>False</code> Respects operation timeouts <code>delegation</code> <code>bool</code> <code>False</code> Can delegate tasks to other agents <code>rag</code> <code>bool</code> <code>False</code> Supports Retrieval-Augmented Generation <code>code_execution</code> <code>bool</code> <code>False</code> Has access to safe execution sandbox"},{"location":"models/#agentskill","title":"AgentSkill","text":"<pre><code>@dataclass\nclass AgentSkill:\n    id: str\n    description: str = \"\"\n    tags: list[str] = field(default_factory=list)\n    examples: list[str] = field(default_factory=list)\n</code></pre> <p>Represents a task that an agent can perform. Skills are used to advertise specific capabilities to other agents.</p>"},{"location":"models/#parameters_2","title":"Parameters","text":"Parameter Type Default Description <code>id</code> <code>str</code> \u2014 Required. Unique Human-readable identifier for the task <code>description</code> <code>str</code> <code>\"\"</code> Optional Detailed description of what the task does <code>tags</code> <code>list[str]</code> <code>[]</code> Optional Tags for categorization <code>examples</code> <code>list[str]</code> <code>[]</code> Optional Example inputs or usage scenarios"},{"location":"models/#example_1","title":"Example","text":"<pre><code>skill = AgentSkill(\n    id=\"weather_forecast\",\n    description=\"Get weather forecast for any location\",\n    tags=[\"weather\", \"forecast\", \"location\"],\n    examples=[\n        \"What's the weather in New York?\",\n        \"Forecast for London tomorrow\",\n        \"Weather in 90210\"\n    ]\n)\n</code></pre>"},{"location":"models/#type-aliases-in-agentcard","title":"Type Aliases in AgentCard","text":""},{"location":"models/#mimetype","title":"MimeType","text":"<p>Type alias for supported MIME types in Protolink. These are used to specify the input and output formats that agents can handle.</p>"},{"location":"models/#supported-types","title":"Supported Types","text":"Category MIME Types Text <code>text/plain</code>, <code>text/markdown</code>, <code>text/html</code> Structured Data <code>application/json</code> Images <code>image/png</code>, <code>image/jpeg</code>, <code>image/webp</code> Audio <code>audio/wav</code>, <code>audio/mpeg</code>, <code>audio/ogg</code> Video <code>video/mp4</code>, <code>video/webm</code> Documents <code>application/pdf</code>"},{"location":"models/#securityschemetype","title":"SecuritySchemeType","text":"<p>Type alias for supported security schemes in Protolink. These are used to specify the authentication methods that agents can use.</p> Category Security Schemes API key <code>apiKey</code> HTTP (bearer/basic/digest) <code>http</code> full OAuth OAuth2 <code>oauth2</code> Certificates <code>mutualTLS</code> OIDC auto-discovery <code>openIdConnect</code>"},{"location":"models/#task","title":"Task","text":"<pre><code>@dataclass\nclass Task:\n    id: str = field(default_factory=lambda: str(uuid.uuid4()))\n    state: TaskState = TaskState.SUBMITTED\n    messages: list[Message] = field(default_factory=list)\n    artifacts: list[Artifact] = field(default_factory=list)\n    metadata: dict[str, Any] = field(default_factory=dict)\n    created_at: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat())\n</code></pre> <p>Unit of work exchanged between agents. Tasks encapsulate a complete unit of work including messages, state, and output artifacts.</p>"},{"location":"models/#parameters_3","title":"Parameters","text":"Parameter Type Default Description <code>id</code> <code>str</code> <code>uuid4()</code> Unique task identifier <code>state</code> <code>TaskState</code> <code>SUBMITTED</code> Current task state <code>messages</code> <code>list[Message]</code> <code>[]</code> Communication history <code>artifacts</code> <code>list[Artifact]</code> <code>[]</code> Output artifacts <code>metadata</code> <code>dict[str, Any]</code> <code>{}</code> Additional metadata <code>created_at</code> <code>str</code> <code>utc now</code> Creation timestamp"},{"location":"models/#methods_1","title":"Methods","text":""},{"location":"models/#task-add-message","title":"<code>add_message(message: Message) -&gt; Task</code>","text":"<p>Add a message to the task's communication history.</p> <p>Parameters: <pre><code>message: Message  # Message object to add to the task\n</code></pre></p> <p>Returns: <pre><code>Task  # Self for method chaining\n</code></pre></p> <p>Example: <pre><code>task = Task()\ntask.add_message(Message.user(\"What's the weather?\"))\ntask.add_message(Message.agent(\"It's sunny!\"))\n</code></pre></p>"},{"location":"models/#task-add-artifact","title":"<code>add_artifact(artifact: Artifact) -&gt; Task</code>","text":"<p>Add an output artifact to the task (v0.2.0+).</p> <p>Parameters: <pre><code>artifact: Artifact  # Artifact representing task output\n</code></pre></p> <p>Returns: <pre><code>Task  # Self for method chaining\n</code></pre></p> <p>Example: <pre><code>artifact = Artifact()\nartifact.add_text(\"Weather analysis complete\")\ntask.add_artifact(artifact)\n</code></pre></p>"},{"location":"models/#task-update-state","title":"<code>update_state(state: TaskState) -&gt; Task</code>","text":"<p>Update the task's current state.</p> <p>Parameters: <pre><code>state: TaskState  # New task state from TaskState enum\n</code></pre></p> <p>Returns: <pre><code>Task  # Self for method chaining\n</code></pre></p> <p>Example: <pre><code>task.update_state(TaskState.WORKING)\n# ... process task ...\ntask.update_state(TaskState.COMPLETED)\n</code></pre></p>"},{"location":"models/#task-complete","title":"<code>complete(response_text: str) -&gt; Task</code>","text":"<p>Mark the task as completed with a response message.</p> <p>Parameters: <pre><code>response_text: str  # Final response message text\n</code></pre></p> <p>Returns: <pre><code>Task  # Self for method chaining\n</code></pre></p> <p>Example: <pre><code>task.complete(\"The weather is sunny and 75\u00b0F.\")\nprint(task.state)  # TaskState.COMPLETED\n</code></pre></p>"},{"location":"models/#task-fail","title":"<code>fail(error_message: str) -&gt; Task</code>","text":"<p>Mark the task as failed with an error message.</p> <p>Parameters: <pre><code>error_message: str  # Description of the error\n</code></pre></p> <p>Returns: <pre><code>Task  # Self for method chaining\n</code></pre></p> <p>Example: <pre><code>task.fail(\"Weather API unavailable\")\nprint(task.state)  # TaskState.FAILED\nprint(task.metadata[\"error\"])  # \"Weather API unavailable\"\n</code></pre></p>"},{"location":"models/#task-to-dict","title":"<code>to_dict() -&gt; dict[str, Any]</code>","text":"<p>Convert the task to a dictionary for serialization.</p> <p>Returns: <pre><code>dict[str, Any]  # Dictionary representation of the task\n</code></pre></p> <p>Example: <pre><code>task_dict = task.to_dict()\nprint(task_dict[\"id\"])  # Task UUID\nprint(task_dict[\"state\"])  # Current state as string\n</code></pre></p>"},{"location":"models/#task-from-dict","title":"<code>from_dict(data: dict[str, Any]) -&gt; Task</code> <code>classmethod</code>","text":"<p>Create a task from a dictionary.</p> <p>Parameters: <pre><code>data: dict[str, Any]  # Dictionary containing task data\n</code></pre></p> <p>Returns: <pre><code>Task  # New Task instance\n</code></pre></p> <p>Example: <pre><code>task_dict = {\n    \"id\": \"123e4567-e89b-12d3-a456-426614174000\",\n    \"state\": \"completed\",\n    \"messages\": []\n}\ntask = Task.from_dict(task_dict)\n</code></pre></p>"},{"location":"models/#task-create","title":"<code>create(message: Message) -&gt; Task</code> <code>classmethod</code>","text":"<p>Create a new task with an initial message.</p> <p>Parameters: <pre><code>message: Message  # Initial message for the task\n</code></pre></p> <p>Returns: <pre><code>Task  # New Task instance with the message added\n</code></pre></p> <p>Example: <pre><code>task = Task.create(Message.user(\"Analyze this data\"))\nprint(len(task.messages))  # 1\nprint(task.state)  # TaskState.SUBMITTED\n</code></pre></p>"},{"location":"models/#example_2","title":"Example","text":"<pre><code>from protolink.models import Task, Message\n\n# Create task with initial message\ntask = Task.create(Message.user(\"What's the weather in New York?\"))\n\n# Add response and complete\ntask.add_message(Message.agent(\"It's 72\u00b0F and sunny in New York.\"))\ntask.complete(\"Weather forecast provided.\")\n\n# Or use convenience method\ntask = Task.create(Message.user(\"Hello\")).complete(\"Hi there!\")\n</code></pre>"},{"location":"models/#taskstate","title":"TaskState","text":"<pre><code>class TaskState(Enum):\n    SUBMITTED = \"submitted\"\n    WORKING = \"working\"\n    INPUT_REQUIRED = \"input-required\"\n    COMPLETED = \"completed\"\n    CANCELED = \"canceled\"\n    FAILED = \"failed\"\n    UNKNOWN = \"unknown\"\n</code></pre> <p>Enumeration of possible task states.</p>"},{"location":"models/#values","title":"Values","text":"Value Description <code>SUBMITTED</code> Task has been submitted to the agent <code>WORKING</code> Agent is actively processing the task <code>INPUT_REQUIRED</code> Agent needs additional input from user <code>COMPLETED</code> Task has been successfully completed <code>CANCELED</code> Task was canceled by user or agent <code>FAILED</code> Task failed due to an error <code>UNKNOWN</code> Task state is unknown"},{"location":"models/#message","title":"Message","text":"<pre><code>@dataclass\nclass Message:\n    id: str = field(default_factory=lambda: str(uuid.uuid4()))\n    role: RoleType = \"user\"\n    parts: list[Part] = field(default_factory=list)\n    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())\n</code></pre> <p>Single unit of communication between agents. Messages contain one or more parts and have a specific role.</p>"},{"location":"models/#parameters_4","title":"Parameters","text":"Parameter Type Default Description <code>id</code> <code>str</code> <code>uuid4()</code> Unique message identifier <code>role</code> <code>RoleType</code> <code>\"user\"</code> Sender role <code>parts</code> <code>list[Part]</code> <code>[]</code> Message content parts <code>timestamp</code> <code>str</code> <code>now</code> Creation timestamp"},{"location":"models/#role-types","title":"Role Types","text":"<ul> <li><code>\"user\"</code>: Message from a human user</li> <li><code>\"agent\"</code>: Message from an agent</li> <li><code>\"system\"</code>: System-level message</li> </ul>"},{"location":"models/#methods_2","title":"Methods","text":""},{"location":"models/#message-add-text","title":"<code>add_text(text: str) -&gt; Message</code>","text":"<p>Add a text part to the message.</p> <p>Parameters: <pre><code>text: str  # Text content to add\n</code></pre></p> <p>Returns: <pre><code>Message  # Self for method chaining\n</code></pre></p> <p>Example: <pre><code>msg = Message(role=\"user\")\nmsg.add_text(\"Hello, world!\")\nprint(len(msg.parts))  # 1\nprint(msg.parts[0].content)  # \"Hello, world!\"\n</code></pre></p>"},{"location":"models/#message-add-part","title":"<code>add_part(part: Part) -&gt; Message</code>","text":"<p>Add a content part to the message.</p> <p>Parameters: <pre><code>part: Part  # Part object to add (text, image, file, etc.)\n</code></pre></p> <p>Returns: <pre><code>Message  # Self for method chaining\n</code></pre></p> <p>Example: <pre><code>msg = Message(role=\"agent\")\nmsg.add_part(Part(\"text\", \"Here's the analysis:\"))\nmsg.add_part(Part(\"data\", {\"result\": \"success\"}))\n</code></pre></p>"},{"location":"models/#message-to-dict","title":"<code>to_dict() -&gt; dict[str, Any]</code>","text":"<p>Convert the message to a dictionary for serialization.</p> <p>Returns: <pre><code>dict[str, Any]  # Dictionary representation of the message\n</code></pre></p> <p>Example: <pre><code>msg = Message.user(\"Hello\")\nmsg_dict = msg.to_dict()\nprint(msg_dict[\"role\"])  # \"user\"\nprint(msg_dict[\"parts\"][0][\"content\"])  # \"Hello\"\n</code></pre></p>"},{"location":"models/#message-from-dict","title":"<code>from_dict(data: dict[str, Any]) -&gt; Message</code> <code>classmethod</code>","text":"<p>Create a message from a dictionary.</p> <p>Parameters: <pre><code>data: dict[str, Any]  # Dictionary containing message data\n</code></pre></p> <p>Returns: <pre><code>Message  # New Message instance\n</code></pre></p> <p>Example: <pre><code>msg_dict = {\n    \"id\": \"msg-123\",\n    \"role\": \"user\",\n    \"parts\": [{\"type\": \"text\", \"content\": \"Hello\"}],\n    \"timestamp\": \"2023-01-01T00:00:00Z\"\n}\nmsg = Message.from_dict(msg_dict)\n</code></pre></p>"},{"location":"models/#message-user","title":"<code>user(text: str) -&gt; Message</code> <code>classmethod</code>","text":"<p>Create a user message with text content (convenience method).</p> <p>Parameters: <pre><code>text: str  # Message text content\n</code></pre></p> <p>Returns: <pre><code>Message  # New Message instance with role \"user\"\n</code></pre></p> <p>Example: <pre><code>msg = Message.user(\"What's the weather?\")\nprint(msg.role)  # \"user\"\nprint(msg.parts[0].content)  # \"What's the weather?\"\n</code></pre></p>"},{"location":"models/#message-agent","title":"<code>agent(text: str) -&gt; Message</code> <code>classmethod</code>","text":"<p>Create an agent message with text content (convenience method).</p> <p>Parameters: <pre><code>text: str  # Message text content\n</code></pre></p> <p>Returns: <pre><code>Message  # New Message instance with role \"agent\"\n</code></pre></p> <p>Example: <pre><code>msg = Message.agent(\"It's sunny and 75\u00b0F.\")\nprint(msg.role)  # \"agent\"\nprint(msg.parts[0].content)  # \"It's sunny and 75\u00b0F.\"\n</code></pre></p>"},{"location":"models/#example_3","title":"Example","text":"<pre><code>from protolink.models import Message, Part\n\n# Create messages using convenience methods\nuser_msg = Message.user(\"What's the weather?\")\nagent_msg = Message.agent(\"It's sunny and 75\u00b0F.\")\n\n# Create message with multiple parts\nmsg = Message(role=\"user\")\nmsg.add_text(\"Here's an image:\")\nmsg.add_part(Part(\"image\", image_data))\n</code></pre>"},{"location":"models/#part","title":"Part","text":"<pre><code>@dataclass\nclass Part:\n    type: str\n    content: Any\n</code></pre> <p>Atomic content unit within a message. Parts represent individual pieces of content like text, images, or files.</p>"},{"location":"models/#parameters_5","title":"Parameters","text":"Parameter Type Description <code>type</code> <code>str</code> Content type (e.g., 'text', 'image', 'file') <code>content</code> <code>Any</code> The actual content data"},{"location":"models/#methods_3","title":"Methods","text":""},{"location":"models/#to_dict-dictstr-any","title":"<code>to_dict() -&gt; dict[str, Any]</code>","text":"<p>Convert to dictionary.</p> <p>Returns: Dictionary representation</p>"},{"location":"models/#from_dictdata-dictstr-any-part","title":"<code>from_dict(data: dict[str, Any]) -&gt; Part</code>","text":"<p>Create from dictionary.</p> <p>Parameters: - <code>data</code>: Dictionary data</p> <p>Returns: New Part instance</p>"},{"location":"models/#textcontent-str-part-classmethod","title":"<code>text(content: str) -&gt; Part</code> (classmethod)","text":"<p>Create a text part.</p> <p>Parameters: - <code>content</code>: Text content</p> <p>Returns: New Part instance with type \"text\"</p>"},{"location":"models/#example_4","title":"Example","text":"<pre><code>from protolink.models import Part\n\n# Create different types of parts\ntext_part = Part.text(\"Hello, world!\")\nimage_part = Part(\"image\", binary_image_data)\nfile_part = Part(\"file\", {\"filename\": \"report.pdf\", \"data\": pdf_data})\n</code></pre>"},{"location":"models/#artifact","title":"Artifact","text":"<pre><code>@dataclass\nclass Artifact:\n    artifact_id: str = field(default_factory=lambda: str(uuid.uuid4()))\n    parts: list[Part] = field(default_factory=list)\n    metadata: dict[str, Any] = field(default_factory=dict)\n    created_at: str = field(default_factory=lambda: datetime.utcnow().isoformat())\n</code></pre> <p>Output produced by a task (v0.2.0+). Artifacts represent results from task execution - files, structured data, analysis results, etc.</p>"},{"location":"models/#parameters_6","title":"Parameters","text":"Parameter Type Default Description <code>artifact_id</code> <code>str</code> <code>uuid4()</code> Unique artifact identifier <code>parts</code> <code>list[Part]</code> <code>[]</code> Content parts of the artifact <code>metadata</code> <code>dict[str, Any]</code> <code>{}</code> Artifact metadata <code>created_at</code> <code>str</code> <code>utc now</code> Creation timestamp"},{"location":"models/#methods_4","title":"Methods","text":""},{"location":"models/#add_partpart-part-artifact","title":"<code>add_part(part: Part) -&gt; Artifact</code>","text":"<p>Add content part to artifact.</p> <p>Parameters: - <code>part</code>: Part to add</p> <p>Returns: Self for method chaining</p>"},{"location":"models/#add_texttext-str-artifact","title":"<code>add_text(text: str) -&gt; Artifact</code>","text":"<p>Add text content (convenience method).</p> <p>Parameters: - <code>text</code>: Text content</p> <p>Returns: Self for method chaining</p>"},{"location":"models/#to_dict-dictstr-any_1","title":"<code>to_dict() -&gt; dict[str, Any]</code>","text":"<p>Convert to dictionary.</p> <p>Returns: Dictionary representation</p>"},{"location":"models/#from_dictdata-dictstr-any-artifact","title":"<code>from_dict(data: dict[str, Any]) -&gt; Artifact</code>","text":"<p>Create from dictionary.</p> <p>Parameters: - <code>data</code>: Dictionary data</p> <p>Returns: New Artifact instance</p>"},{"location":"models/#example_5","title":"Example","text":"<pre><code>from protolink.models import Artifact, Part\n\n# Create artifact with multiple parts\nartifact = Artifact()\nartifact.add_text(\"Analysis Results:\")\nartifact.add_part(Part(\"data\", {\"results\": [1, 2, 3]}))\nartifact.add_part(Part(\"chart\", chart_image_data))\n\n# Set metadata\nartifact.metadata[\"type\"] = \"analysis_report\"\nartifact.metadata[\"version\"] = \"1.0\"\n</code></pre>"},{"location":"models/#context","title":"Context","text":"<pre><code>@dataclass\nclass Context:\n    context_id: str = field(default_factory=lambda: str(uuid.uuid4()))\n    messages: list = field(default_factory=list)  # List[Message]\n    metadata: dict[str, Any] = field(default_factory=dict)\n    created_at: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat())\n    last_accessed: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat())\n</code></pre> <p>Represents a conversation context (session). Contexts group messages across multiple turns, enabling long-running conversations and session persistence.</p>"},{"location":"models/#parameters_7","title":"Parameters","text":"Parameter Type Default Description <code>context_id</code> <code>str</code> <code>uuid4()</code> Unique context identifier <code>messages</code> <code>list</code> <code>[]</code> All messages in this context <code>metadata</code> <code>dict[str, Any]</code> <code>{}</code> Custom context data <code>created_at</code> <code>str</code> <code>utc now</code> Creation timestamp <code>last_accessed</code> <code>str</code> <code>utc now</code> Last activity timestamp"},{"location":"models/#methods_5","title":"Methods","text":""},{"location":"models/#add_messagemessage-context","title":"<code>add_message(message) -&gt; Context</code>","text":"<p>Add a message to this context.</p> <p>Parameters: - <code>message</code>: Message object to add</p> <p>Returns: Self for method chaining</p>"},{"location":"models/#to_dict-dict","title":"<code>to_dict() -&gt; dict</code>","text":"<p>Convert context to dictionary.</p> <p>Returns: Dictionary representation</p>"},{"location":"models/#from_dictdata-dict-context","title":"<code>from_dict(data: dict) -&gt; Context</code>","text":"<p>Create context from dictionary.</p> <p>Parameters: - <code>data</code>: Dictionary data</p> <p>Returns: New Context instance</p>"},{"location":"models/#example_6","title":"Example","text":"<pre><code>from protolink.models import Context, Message\n\n# Create new context\ncontext = Context()\ncontext.metadata[\"user_id\"] = \"user123\"\ncontext.metadata[\"session_type\"] = \"weather_chat\"\n\n# Add messages\ncontext.add_message(Message.user(\"What's the weather?\"))\ncontext.add_message(Message.agent(\"It's sunny!\"))\n\n# Context maintains conversation history\nfor msg in context.messages:\n    print(f\"{msg.role}: {msg.parts[0].content}\")\n</code></pre>"},{"location":"models/#usage-patterns","title":"Usage Patterns","text":""},{"location":"models/#task-workflow","title":"Task Workflow","text":"<pre><code>from protolink.models import Task, Message, Artifact, TaskState\n\n# Create and submit task\ntask = Task.create(Message.user(\"Analyze this data\"))\n\n# Process task\ntask.update_state(TaskState.WORKING)\n\n# Add results\nartifact = Artifact()\nartifact.add_text(\"Analysis complete\")\nartifact.add_part(Part(\"data\", {\"result\": \"success\"}))\n\ntask.add_artifact(artifact)\ntask.add_message(Message.agent(\"Analysis complete\"))\ntask.update_state(TaskState.COMPLETED)\n</code></pre>"},{"location":"models/#context-management","title":"Context Management","text":"<pre><code>from protolink.models import Context, Message\n\n# Long-running conversation\ncontext = Context()\ncontext.add_message(Message.user(\"Hello\"))\ncontext.add_message(Message.agent(\"Hi! How can I help?\"))\n\n# Continue conversation later\ncontext.add_message(Message.user(\"What did we discuss?\"))\n# Context maintains full history\n</code></pre>"},{"location":"models/#serialization","title":"Serialization","text":"<p>All models support JSON/dict serialization:</p> <pre><code># Convert to dict\ntask_dict = task.to_dict()\n\n# Restore from dict\nrestored_task = Task.from_dict(task_dict)\n\n# Works for all models\ncontext_dict = context.to_dict()\nrestored_context = Context.from_dict(context_dict)\n</code></pre>"},{"location":"registry/","title":"Registry","text":"<p>The Registry is the discovery and coordination layer of Protolink. It allows agents to register themselves, discover other agents, and interact autonomously over a shared transport.</p> <p>At a high level:</p> <ul> <li>Agents announce their existence to a Registry</li> <li>Other agents can discover agents dynamically</li> <li>The Registry runs as a server and exposes a transport-backed API</li> <li>Agents can operate autonomously, without manual function calls</li> </ul> <p>Why a Registry?</p> <p>The Registry enables dynamic, decentralized agent systems. Agents don\u2019t need hard-coded references to each other \u2014 they discover peers at runtime and coordinate through messages.</p> <p>Inspiration</p> <p>The Registry is inspired by Google's A2A paper, which introduces a similar concept for agent-to-agent discovery.</p>"},{"location":"registry/#core-concepts","title":"Core Concepts","text":""},{"location":"registry/#registry_1","title":"Registry","text":"<p>The Registry is a long-running service responsible for:</p> <ul> <li>Tracking active agents</li> <li>Storing their <code>AgentCard</code> metadata</li> <li>Exposing discovery endpoints via a transport</li> <li>Acting as the entry point for multi-agent systems</li> </ul> <p>The Registry does not execute agent logic. It only provides discovery, metadata, and routing primitives.</p>"},{"location":"registry/#agent-registration","title":"Agent Registration","text":"<p>Each agent registers itself by sending its <code>AgentCard</code> to the Registry when it starts.</p> <p>An <code>AgentCard</code> typically includes:</p> <ul> <li>Agent name</li> <li>Description</li> <li>Capabilities / skills</li> <li>Transport URL</li> <li>Optional metadata</li> </ul> <p>Once registered, the agent becomes discoverable by other agents.</p> <p>Dynamic Systems</p> <p>Agents may join or leave at any time. The Registry reflects the current state of the system dynamically.</p>"},{"location":"registry/#agent-discovery","title":"Agent Discovery","text":"<p>Agents query the Registry to discover other agents.</p> <p>Discovery supports optional filtering, for example:</p> <ul> <li>By name</li> <li>By capability</li> <li>By arbitrary metadata</li> </ul> <pre><code>agents = await registry.discover(filters={\n    \"capability\": \"search\",\n    \"domain\": \"finance\"\n})\n</code></pre>"},{"location":"registry/#registry_2","title":"Registry","text":"<p>The Registry is the discovery and coordination layer of Protolink. It allows agents to register themselves, discover other agents, and interact autonomously over a shared transport.</p> <p>At a high level:</p> <ul> <li>Agents announce their existence to a Registry</li> <li>Other agents can discover agents dynamically</li> <li>The Registry runs as a server and exposes a transport-backed API</li> <li>Agents can operate autonomously, without manual function calls</li> </ul> <p>Why a Registry?</p> <p>The Registry enables dynamic, decentralized agent systems. Agents don\u2019t need hard-coded references to each other \u2014 they discover peers at runtime and coordinate through messages.</p>"},{"location":"registry/#core-concepts_1","title":"Core Concepts","text":""},{"location":"registry/#registry_3","title":"Registry","text":"<p>The Registry is a long-running service responsible for:</p> <ul> <li>Tracking active agents</li> <li>Storing their <code>AgentCard</code> metadata</li> <li>Exposing discovery endpoints via a transport</li> <li>Acting as the entry point for multi-agent systems</li> </ul> <p>The Registry does not execute agent logic. It only provides discovery, metadata, and routing primitives.</p>"},{"location":"registry/#agent-registration_1","title":"Agent Registration","text":"<p>Each agent registers itself by sending its <code>AgentCard</code> to the Registry when it starts.</p> <p>An <code>AgentCard</code> typically includes:</p> <ul> <li>Agent name</li> <li>Description</li> <li>Capabilities / skills</li> <li>Transport URL</li> <li>Optional metadata</li> </ul> <p>Once registered, the agent becomes discoverable by other agents.</p> <p>Dynamic Systems</p> <p>Agents may join or leave at any time. The Registry reflects the current state of the system dynamically.</p>"},{"location":"registry/#agent-discovery_1","title":"Agent Discovery","text":"<p>Agents query the Registry to discover other agents.</p> <p>Discovery supports optional filtering, for example:</p> <ul> <li>By name</li> <li>By capability</li> <li>By arbitrary metadata</li> </ul> <pre><code>agents = await registry.discover(filters={\n    \"capability\": \"search\",\n    \"domain\": \"finance\"\n})\n</code></pre> <p>Filters are Optional</p> <p>Calling discover() with no filters returns all registered agents.</p>"},{"location":"registry/#transport-integration","title":"Transport Integration","text":"<p>The Registry is transport-agnostic. It relies on a Transport implementation to expose its API.</p> <p>Currently supported:</p> <ul> <li>HTTPRegistryTransport</li> </ul> <p>The transport is responsible for:</p> <ul> <li>Binding to a host and port</li> <li>Exposing registry endpoints</li> <li>Handling request/response lifecycle</li> </ul>"},{"location":"registry/#starting-the-registry","title":"Starting the Registry","text":"<p>The Registry is started via its transport</p> <pre><code>from protolink.registry import Registry\nfrom protolink.transport import HTTPRegistryTransport\n\ntransport = HTTPRegistryTransport(url=\"http://localhost:9020\")\nregistry = Registry(transport)\n\nawait registry.start()\n</code></pre> <p>This starts a registry server that agents can connect to.</p> <p>Single Source of Truth</p> <p>The Registry\u2019s public URL is derived from the transport and used by agents for registration and discovery.</p>"},{"location":"registry/#url-handling","title":"URL Handling","text":"<p>Both Agents and the Registry expose a url property.</p> <ul> <li>The Transport owns host and port</li> <li>The url is a derived, canonical representation</li> </ul> <p>To avoid duplication, transports provide helpers to derive host and port from a URL.</p> <pre><code>transport = HTTPRegistryTransport.from_url(\"http://localhost:8000\")\n</code></pre> <p>This ensures consistent configuration across agents and registry instances.</p>"},{"location":"tools/","title":"Tools","text":"<p>Tools extend agent capabilities with additional functions. The tools can be used by the LLM or directly by the agent.</p>"},{"location":"tools/#module-structure","title":"Module Structure","text":"<p>The tools module is organized as follows:</p> <pre><code># Core tool interfaces\nfrom protolink.tools import BaseTool, Tool\n\n# Tool adapters for external integrations  \nfrom protolink.tools.adapters import MCPToolAdapter\n</code></pre> <ul> <li><code>protolink.tools</code>: Core tool interfaces and native tool implementation</li> <li><code>protolink.tools.adapters</code>: Adapters for integrating external tool systems</li> </ul>"},{"location":"tools/#native-tools","title":"Native Tools","text":"<p>Native tools are regular Python callables that you register on an agent. They are exposed over the transport so that other agents (or clients) can invoke them.</p> <p>To register a native tool, decorate an async function with <code>@agent.tool</code>:</p> <pre><code>from protolink.agents import Agent\n\n\n@agent.tool(name=\"add\", description=\"Add two numbers\")\nasync def add_numbers(a: int, b: int) -&gt; int:\n    return a + b\n</code></pre> <p>Native tools are ideal for business logic, data access, or any capability you want your agents to expose.</p>"},{"location":"tools/#mcp-tools","title":"MCP Tools","text":"<p>Protolink can also expose tools from an MCP (Model Context Protocol) server using the <code>MCPToolAdapter</code>.</p> <p>High\u2011level flow:</p> <ol> <li>Connect to an MCP server using an MCP client (not shown here).</li> <li>Wrap an MCP tool with <code>MCPToolAdapter</code>.</li> <li>Register it on the agent.</li> </ol> <p>Example pattern:</p> <pre><code>from protolink.tools.adapters import MCPToolAdapter\n\n\nmcp_tool = MCPToolAdapter(mcp_client, \"multiply\")\nagent.add_tool(mcp_tool)\n</code></pre> <p>The MCP adapter lets you reuse existing MCP tools as if they were native tools, keeping a consistent interface on the agent side.</p>"},{"location":"tools/#tool-tags","title":"Tool Tags","text":"<p>Tools can be categorized using tags for better organization and discovery:</p> <pre><code>@agent.tool(name=\"calculate\", description=\"Performs calculations\", tags=[\"math\", \"utility\"])\nasync def calculate(operation: str, a: float, b: float) -&gt; float:\n    if operation == \"add\":\n        return a + b\n    elif operation == \"multiply\":\n        return a * b\n    else:\n        raise ValueError(f\"Unsupported operation: {operation}\")\n</code></pre> <p>Tags are automatically propagated to the agent's skills and can be used for filtering and categorization.</p>"},{"location":"transports/","title":"Transports","text":"<p>Protolink supports multiple transports for agent-to-agent and agent-to-registry communication. A transport is responsible for delivering <code>Task</code> and <code>Message</code> objects between components and for exposing an API surface (HTTP, WebSocket, in\u2011memory, etc.).</p> <p>At a high level, all transports implement the same conceptual operations:</p> <ul> <li>Send work: send a <code>Task</code> or <code>Message</code> to another agent or registry.</li> <li>Receive work: expose an endpoint / callback to handle incoming requests.</li> <li>Lifecycle: start and stop the underlying server or runtime.</li> </ul>"},{"location":"transports/#transport-categories","title":"Transport Categories","text":"<p>Protolink separates transports into two distinct categories:</p>"},{"location":"transports/#agent-transports-agenttransport","title":"Agent Transports (<code>AgentTransport</code>)","text":"<p>Handle agent-to-agent communication for task execution and messaging.</p>"},{"location":"transports/#registry-transports-registrytransport","title":"Registry Transports (<code>RegistryTransport</code>)","text":"<p>Handle agent-to-registry communication for discovery and coordination.</p> <p>This separation ensures clean boundaries and allows different transports to be optimized for their specific use cases.</p>"},{"location":"transports/#supported-transports","title":"Supported Transports","text":""},{"location":"transports/#agent-to-agent-transports","title":"Agent-to-Agent Transports","text":"<ul> <li>HTTPAgentTransport</li> <li>Uses HTTP/HTTPS for synchronous request/response.</li> <li>Backed by ASGI frameworks:<ul> <li><code>Starlette</code> + <code>httpx</code> + <code>uvicorn</code> (lightweight default backend).</li> <li><code>FastAPI</code> + <code>pydantic</code> + <code>uvicorn</code> (with optional request validation).</li> </ul> </li> <li> <p>Great default choice for web\u2011based agents, simple deployments, and interoperable APIs.</p> </li> <li> <p>WebSocketAgentTransport</p> </li> <li>Uses WebSocket for streaming requests and responses.</li> <li>Built on top of libraries like <code>websockets</code> (and <code>httpx</code> for HTTP parts where applicable).</li> <li> <p>Useful for real\u2011time, bidirectional communication or token\u2011level streaming.</p> </li> <li> <p>RuntimeAgentTransport</p> </li> <li>Simple in\u2011process, in\u2011memory transport.</li> <li>Allows multiple agents to communicate within the same Python process.</li> <li>Ideal for local development, testing, and tightly\u2011coupled agent systems.</li> </ul>"},{"location":"transports/#agent-to-registry-transports","title":"Agent-to-Registry Transports","text":"<ul> <li>HTTPRegistryTransport</li> <li>Uses HTTP/HTTPS for registry operations (registration, discovery, heartbeat).</li> <li>Backed by ASGI frameworks similar to <code>HTTPAgentTransport</code>.</li> <li>Handles registry-specific endpoints and protocols.</li> </ul>"},{"location":"transports/#planned-transports","title":"Planned Transports","text":"<ul> <li>JSONRPCAgentTransport (TBD)</li> <li>Planned JSON\u2011RPC based transport for agents.</li> <li> <p>Intended for structured, RPC\u2011style interactions.</p> </li> <li> <p>GRPCAgentTransport (TBD)</p> </li> <li>Planned gRPC transport for agents.</li> <li>Intended for high\u2011performance, strongly\u2011typed communication.</li> </ul>"},{"location":"transports/#choosing-a-transport","title":"Choosing a Transport","text":"<p>Some rough guidelines:</p> <ul> <li>Use RuntimeAgentTransport for local experiments, tests, or when all agents live in the same process.</li> <li>Use HTTPAgentTransport when you want a simple, interoperable API surface (e.g. calling agents from other services or frontends).</li> <li>Use WebSocketAgentTransport when you need streaming and interactive sessions.</li> <li>Use HTTPRegistryTransport for registry communication (the primary choice currently).</li> <li>Plan for JSONRPCAgentTransport or GRPCAgentTransport if you need stricter schemas or higher performance across services.</li> </ul> <p>The rest of this page dives into the API of each transport in more detail.</p>"},{"location":"transports/#httpagenttransport","title":"HTTPAgentTransport","text":"<p><code>HTTPAgentTransport</code> is the main network transport for agent-to-agent communication in Protolink. It exposes a simple JSON HTTP API compatible with the rest of the framework.</p>"},{"location":"transports/#overview","title":"Overview","text":"<ul> <li>Client side</li> <li>Uses <code>httpx.AsyncClient</code> to send JSON requests to other agents.</li> <li> <p>Provides helpers to send a full <code>Task</code> or a single <code>Message</code>.</p> </li> <li> <p>Server side</p> </li> <li>Uses an ASGI app (Starlette or FastAPI) to expose:<ul> <li><code>POST /tasks/</code> \u2014 submit a <code>Task</code> to the agent.</li> <li><code>GET /.well-known/agent.json</code> \u2014 served by the agent itself, not the transport, but typically used together.</li> </ul> </li> <li>Uses a backend implementation of <code>BackendInterface</code> to manage the ASGI app and <code>uvicorn</code> server.</li> </ul>"},{"location":"transports/#backends-starlette-vs-fastapi","title":"Backends: Starlette vs FastAPI","text":"<p><code>HTTPAgentTransport</code> delegates server behavior to a backend implementing <code>BackendInterface</code>:</p> <ul> <li>StarletteBackend (default)</li> <li>Minimal Starlette app with a single <code>POST /tasks/</code> route.</li> <li>No extra request validation beyond what <code>Task.from_dict()</code> does.</li> <li> <p>Best when you want low overhead and trust callers to send valid payloads.</p> </li> <li> <p>FastAPIBackend</p> </li> <li>FastAPI app with optional Pydantic models mirroring the <code>Task</code>/<code>Message</code>/<code>Artifact</code> structures.</li> <li>When <code>validate_schema=True</code>, incoming requests are validated against these models before being converted with <code>Task.from_dict()</code>.</li> <li>Best when you want schema validation and better generated OpenAPI / docs.</li> </ul> <p>Backend and validation are selected via the <code>HTTPAgentTransport</code> constructor:</p> <pre><code>from protolink.transport import HTTPAgentTransport\n\n# Starlette backend (default)\ntransport = HTTPAgentTransport()\n\n# Explicit Starlette backend\ntransport = HTTPAgentTransport(backend=\"starlette\")\n\n# FastAPI backend without schema validation\ntransport = HTTPAgentTransport(backend=\"fastapi\", validate_schema=False)\n\n# FastAPI backend with full schema validation\ntransport = HTTPAgentTransport(backend=\"fastapi\", validate_schema=True)\n</code></pre>"},{"location":"transports/#wire-format","title":"Wire Format","text":"<p><code>HTTPAgentTransport</code> sends and receives JSON payloads that match the core models' <code>to_dict()</code> methods. A typical <code>Task</code> request body looks like this:</p> <pre><code>{\n  \"id\": \"8c1e93b3-9f72-4a37-8c4c-3d2d8a9c4f7c\",\n  \"state\": \"submitted\",\n  \"messages\": [\n    {\n      \"id\": \"f0e4c2f7-5d3b-4b0a-b6e0-6a7f2d9c1b2a\",\n      \"role\": \"user\",\n      \"parts\": [\n        {\"type\": \"text\", \"content\": \"Hi Bob, how are you?\"}\n      ],\n      \"timestamp\": \"2025-01-01T12:00:00Z\"\n    }\n  ],\n  \"artifacts\": [],\n  \"metadata\": {},\n  \"created_at\": \"2025-01-01T12:00:00Z\"\n}\n</code></pre> <p>The tables below document each object type.</p>"},{"location":"transports/#task","title":"Task","text":"Field Type Description <code>id</code> <code>str</code> Unique task identifier. <code>state</code> <code>str</code> One of <code>\"submitted\"</code>, <code>\"working\"</code>, <code>\"completed\"</code>, etc. <code>messages</code> <code>list[Message]</code> Conversation history for this task. <code>artifacts</code> <code>list[Artifact]</code> Outputs produced by the task. <code>metadata</code> <code>dict[str, Any]</code> Arbitrary metadata attached to the task. <code>created_at</code> <code>str</code> ISO\u20118601 timestamp (UTC)."},{"location":"transports/#message","title":"Message","text":"<pre><code>{\n  \"id\": \"f0e4c2f7-5d3b-4b0a-b6e0-6a7f2d9c1b2a\",\n  \"role\": \"user\",\n  \"parts\": [\n    {\"type\": \"text\", \"content\": \"Hi Bob, how are you?\"}\n  ],\n  \"timestamp\": \"2025-01-01T12:00:00Z\"\n}\n</code></pre> Field Type Description <code>id</code> <code>str</code> Unique message identifier. <code>role</code> <code>\"user\" \\| \"agent\" \\| \"system\"</code> Sender role. <code>parts</code> <code>list[Part]</code> Content payloads. <code>timestamp</code> <code>str</code> ISO\u20118601 timestamp."},{"location":"transports/#part","title":"Part","text":"<pre><code>{\"type\": \"text\", \"content\": \"Hi Bob, how are you?\"}\n</code></pre> Field Type Description <code>type</code> <code>str</code> Content type (e.g. <code>\"text\"</code>). <code>content</code> <code>Any</code> The actual content payload."},{"location":"transports/#artifact","title":"Artifact","text":"<pre><code>{\n  \"artifact_id\": \"a1b2c3\",\n  \"parts\": [\n    {\"type\": \"text\", \"content\": \"final report\"}\n  ],\n  \"metadata\": {\"kind\": \"report\"},\n  \"created_at\": \"2025-01-01T12:00:00Z\"\n}\n</code></pre> Field Type Description <code>artifact_id</code> <code>str</code> Unique artifact identifier. <code>parts</code> <code>list[Part]</code> Artifact content. <code>metadata</code> <code>dict[str, Any]</code> Artifact metadata. <code>created_at</code> <code>str</code> ISO\u20118601 timestamp."},{"location":"transports/#typical-usage","title":"Typical Usage","text":""},{"location":"transports/#exposing-an-agent-over-http","title":"Exposing an agent over HTTP","text":"<pre><code>from protolink.agents import Agent\nfrom protolink.models import AgentCard, Task, Message\nfrom protolink.transport import HTTPAgentTransport\n\n\nclass EchoAgent(Agent):\n    def __init__(self, port: int) -&gt; None:\n        url = f\"http://127.0.0.1:{port}\"\n        card = AgentCard(\n            name=\"echo\", \n            description=\"Echoes back the last user message\", \n            url=url,\n        )\n        transport = HTTPAgentTransport(url=url)\n        super().__init__(card, transport=transport)\n\n    async def handle_task(self, task: Task) -&gt; Task:\n        last_msg = task.messages[-1]\n        reply = Message.agent(f\"echo: {last_msg.parts[0].content}\")\n        return Task(id=task.id, messages=task.messages + [reply])\n</code></pre> <p>Then run the agent and call it from another agent or client using <code>send_task_to</code> or <code>send_message_to</code>.</p>"},{"location":"transports/#calling-a-remote-agent","title":"Calling a remote agent","text":"<pre><code>from protolink.agents import Agent\nfrom protolink.models import AgentCard, Task, Message\nfrom protolink.transport import HTTPAgentTransport\n\n\n# Agent that calls other agents\nclass CallerAgent(Agent):\n    def __init__(self, target_url: str) -&gt; None:\n        url = \"http://localhost:8021\"\n        card = AgentCard(name=\"caller\", description=\"Calls other agents\", url=url)\n        transport = HTTPAgentTransport(url=url)\n        super().__init__(card, transport=transport)\n        self.target_url = target_url\n\n    async def handle_task(self, task: Task) -&gt; Task:\n        # Forward the task to another agent\n        result = await self.send_task_to(self.target_url, task)\n        return result\n\n\nasync def call_remote(url: str) -&gt; None:\n    hello = Task.create(Message.user(\"Hello over HTTP!\"))\n    result = await caller_agent.send_task_to(url, hello)\n    print(\"Response:\", result.messages[-1].parts[0].content)\n</code></pre>"},{"location":"transports/#httpagenttransport-api-reference","title":"HTTPAgentTransport API Reference","text":"<p>The most important public methods on <code>HTTPAgentTransport</code> are summarized below.</p>"},{"location":"transports/#constructor-lifecycle","title":"Constructor &amp; lifecycle","text":"Name Parameters Returns Description <code>__init__</code> <code>url: str</code>, <code>timeout: float = 30.0</code>, <code>authenticator: Authenticator \\| None = None</code>, <code>backend: Literal[\"starlette\", \"fastapi\"] = \"starlette\"</code>, <code>validate_schema: bool = False</code> <code>None</code> Configure URL, request timeout, optional authentication provider, backend implementation, and whether to enable FastAPI/Pydantic schema validation. <code>start</code> <code>self</code> <code>Awaitable[None]</code> Start the selected backend, register the <code>/tasks/</code> route and create the internal <code>httpx.AsyncClient</code>. Must be awaited before serving HTTP traffic. <code>stop</code> <code>self</code> <code>Awaitable[None]</code> Stop the backend server and close the internal HTTP client. Safe to call multiple times."},{"location":"transports/#sending-receiving","title":"Sending &amp; receiving","text":"Name Parameters Returns Description <code>on_task_received</code> <code>handler: Callable[[Task], Awaitable[Task]]</code> <code>None</code> Register the callback that will handle incoming tasks on <code>POST /tasks/</code>. This must be set before <code>start()</code> when running as a server. <code>send_task</code> <code>agent_url: str</code>, <code>task: Task</code>, <code>skill: str \\| None = None</code> <code>Awaitable[Task]</code> Send a <code>Task</code> to <code>POST {agent_url}/tasks/</code> and return the resulting <code>Task</code> from the remote agent. The optional <code>skill</code> is passed via headers and can be used by agents to route work. <code>send_message</code> <code>agent_url: str</code>, <code>message: Message</code> <code>Awaitable[Message]</code> Convenience wrapper that wraps a single <code>Message</code> in a new <code>Task</code>, calls <code>send_task</code>, and returns the last response message. Ideal for simple request/response interactions. <code>get_agent_card</code> <code>agent_url: str</code> <code>Awaitable[AgentCard]</code> Fetch the remote agent's <code>AgentCard</code> description from <code>GET {agent_url}/.well-known/agent.json</code>. Useful for discovery and capability inspection."},{"location":"transports/#auth-utilities","title":"Auth &amp; utilities","text":"Name Parameters Returns Description <code>authenticate</code> <code>credentials: str</code> <code>Awaitable[None]</code> Use the configured <code>Authenticator</code> to obtain an auth context (for example, exchanging an API key for a bearer token). The resulting context is automatically injected into outgoing HTTP headers. <code>_build_headers</code> <code>skill: str \\| None = None</code> <code>dict[str, str]</code> Internal helper that constructs HTTP headers (including <code>Authorization</code> when an auth context is present). Exposed here for completeness; you normally do not need to call it directly. <code>validate_agent_url</code> <code>agent_url: str</code> <code>bool</code> Return <code>True</code> if the URL is considered local to this transport's host/port (e.g. for allow\u2011listing), <code>False</code> otherwise."},{"location":"transports/#httpregistrytransport","title":"HTTPRegistryTransport","text":"<p><code>HTTPRegistryTransport</code> is the main network transport for agent-to-registry communication in Protolink. It handles registry operations like agent registration, discovery, and heartbeat.</p>"},{"location":"transports/#overview_1","title":"Overview","text":"<ul> <li>Client side</li> <li>Uses <code>httpx.AsyncClient</code> to communicate with the registry server.</li> <li> <p>Provides helpers for registration, discovery, and heartbeat operations.</p> </li> <li> <p>Server side</p> </li> <li>Uses an ASGI app (Starlette or FastAPI) to expose registry endpoints:<ul> <li><code>POST /agents/</code> \u2014 register an agent</li> <li><code>GET /agents/</code> \u2014 discover agents</li> <li><code>POST /agents/{agent_id}/heartbeat</code> \u2014 send heartbeat</li> </ul> </li> <li>Uses a backend implementation of <code>BackendInterface</code> to manage the ASGI app and <code>uvicorn</code> server.</li> </ul>"},{"location":"transports/#typical-usage_1","title":"Typical Usage","text":"<pre><code>from protolink.registry import Registry\nfrom protolink.transport import HTTPRegistryTransport\n\n# Initialize registry transport\ntransport = HTTPRegistryTransport(url=\"http://localhost:9020\")\nregistry = Registry(transport)\n\n# Start the registry server\nawait registry.start()\n\n# Agents can now register and discover\n</code></pre>"},{"location":"transports/#httpregistrytransport-api-reference","title":"HTTPRegistryTransport API Reference","text":"Name Parameters Returns Description <code>__init__</code> <code>url: str</code>, <code>timeout: float = 30.0</code>, <code>backend: Literal[\"starlette\", \"fastapi\"] = \"starlette\"</code> <code>None</code> Configure URL, request timeout, and backend implementation. <code>start</code> <code>self</code> <code>Awaitable[None]</code> Start the registry server with the selected backend. <code>stop</code> <code>self</code> <code>Awaitable[None]</code> Stop the registry server and clean up resources."},{"location":"transports/#runtimeagenttransport","title":"RuntimeAgentTransport","text":"<p><code>RuntimeAgentTransport</code> is an in\u2011process, in\u2011memory transport used primarily for tests, local experimentation, and tightly\u2011coupled multi\u2011agent systems.</p> <p>Characteristics:</p> <ul> <li>No network hops, very low latency.</li> <li>Multiple agents share the same runtime transport instance.</li> <li>Ideal for composition and unit tests (see <code>tests/test_agent.py</code>).</li> </ul>"},{"location":"transports/#runtimeagenttransport-api","title":"RuntimeAgentTransport API","text":"Name Parameters Returns Description <code>__init__</code> <code>...</code> <code>None</code> Create an in\u2011memory transport registry for agents that live in the same Python process. <code>register</code> <code>agent</code> <code>None</code> Add an agent to the runtime transport so it can receive tasks from others. <code>unregister</code> <code>agent</code> <code>None</code> Remove an agent from the runtime transport. <code>send_task</code> <code>agent_id_or_url</code>, <code>task: Task</code> <code>Task \\| Awaitable[Task]</code> Dispatch a <code>Task</code> to another agent registered on the same runtime transport instance. <code>start</code> / <code>stop</code> <code>self</code> <code>None</code> Often no\u2011op or light\u2011weight setup/teardown. Provided for a consistent lifecycle API with other transports."},{"location":"transports/#websocketagenttransport","title":"WebSocketAgentTransport","text":"<p><code>WebSocketAgentTransport</code> (when available) provides streaming, bidirectional communication between agents or between agents and external clients.</p> <p>Use it when:</p> <ul> <li>You need token\u2011level or chunk\u2011level streaming.</li> <li>You want long\u2011lived interactive sessions (chat UIs, dashboards, tools that stream output).</li> </ul>"},{"location":"transports/#websocketagenttransport-api","title":"WebSocketAgentTransport API","text":"Name Parameters Returns Description <code>__init__</code> <code>...</code> <code>None</code> Configure host/port and WebSocket settings for streaming connections. <code>send_task_stream</code> <code>...</code> <code>AsyncIterator[Task] \\| AsyncIterator[Message]</code> Send a <code>Task</code> and receive a stream of partial results or updates over a single WebSocket connection. <code>start</code> / <code>stop</code> <code>self</code> <code>Awaitable[None]</code> Start or stop the WebSocket server."},{"location":"transports/#planned-transports_1","title":"Planned Transports","text":"<p>These transports are not implemented yet in the core library. The sections below describe the intended design so you can plan ahead, but there is currently no public API to import.</p> <p>Status: Design sketches only. Do not rely on these in production code.</p>"},{"location":"transports/#jsonrpcagenttransport-planned","title":"JSONRPCAgentTransport (planned)","text":"<ul> <li>JSON\u2011RPC 2.0 style envelope for structured requests and responses.</li> <li>Strong separation of methods, params, and results.</li> <li>Natural fit for RPC\u2011style integrations.</li> </ul>"},{"location":"transports/#jsonrpcagenttransport-design","title":"JSONRPCAgentTransport design","text":"Name Parameters Returns Description <code>send_request</code> <code>method: str</code>, <code>params: dict</code> <code>Awaitable[dict]</code> (Planned) Send a JSON\u2011RPC request and return the decoded result payload. <code>notify</code> <code>method: str</code>, <code>params: dict</code> <code>Awaitable[None]</code> (Planned) Fire\u2011and\u2011forget notification without a response. <code>start</code> / <code>stop</code> <code>self</code> <code>Awaitable[None]</code> (Planned) Start/stop the JSON\u2011RPC server."},{"location":"transports/#grpcagenttransport-planned","title":"GRPCAgentTransport (planned)","text":"<ul> <li>gRPC\u2011based transport with protobuf definitions for tasks and messages.</li> <li>High\u2011performance, strongly\u2011typed, streaming\u2011friendly.</li> </ul>"},{"location":"transports/#grpcagenttransport-design","title":"GRPCAgentTransport design","text":"Name Parameters Returns Description <code>send_task</code> <code>...</code> <code>Awaitable[...]</code> (Planned) Unary RPC for sending tasks. <code>send_task_stream</code> <code>...</code> <code>AsyncIterator[...]</code> (Planned) Streaming RPC for long\u2011running tasks and progress updates. <code>start</code> / <code>stop</code> <code>self</code> <code>Awaitable[None]</code> (Planned) Start/stop the gRPC server."},{"location":"types/","title":"Type Aliases API Reference","text":"<p>This section provides detailed documentation for the type aliases used throughout the Protolink framework. Type aliases improve code readability, provide type safety, and make the API more consistent across different modules.</p>"},{"location":"types/#table-of-contents","title":"Table of Contents","text":"<ul> <li>BackendType</li> <li>LLMProvider</li> <li>LLMType</li> <li>MimeType</li> <li>RoleType</li> <li>SecuritySchemeType</li> </ul>"},{"location":"types/#backendtype","title":"BackendType","text":"<pre><code>BackendType: TypeAlias = Literal[\"starlette\", \"fastapi\"]\n</code></pre> <p>Type alias for supported HTTP backend implementations in Protolink transports.</p>"},{"location":"types/#supported-backends","title":"Supported Backends","text":"Backend Description starlette Lightweight ASGI framework (default) fastapi Full-featured API framework with automatic validation"},{"location":"types/#usage-example","title":"Usage Example","text":"<pre><code>from protolink.types import BackendType\nfrom protolink.transport import HTTPAgentTransport\n\n# Use Starlette backend (default)\ntransport = HTTPAgentTransport(backend=\"starlette\")\n\n# Use FastAPI backend for automatic validation\ntransport = HTTPAgentTransport(backend=\"fastapi\", validate_schema=True)\n</code></pre>"},{"location":"types/#llmprovider","title":"LLMProvider","text":"<pre><code>LLMProvider: TypeAlias = Literal[\"openai\", \"anthropic\", \"google\", \"llama.cpp\", \"ollama\"]\n</code></pre> <p>Type alias for supported Large Language Model providers in Protolink.</p>"},{"location":"types/#supported-providers","title":"Supported Providers","text":"Provider Description openai OpenAI API (GPT models) anthropic Anthropic Claude API google Google AI models llama.cpp Local LLaMA models ollama Ollama local models"},{"location":"types/#usage-example_1","title":"Usage Example","text":"<pre><code>from protolink.types import LLMProvider\nfrom protolink.llms.api import OpenAILLM\n\n# Specify provider when creating LLM\nllm = OpenAILLM(model=\"gpt-4\", provider=\"openai\")\n</code></pre>"},{"location":"types/#llmtype","title":"LLMType","text":"<pre><code>LLMType: TypeAlias = Literal[\"api\", \"local\", \"server\"]\n</code></pre> <p>Type alias for different types of LLM deployment methods.</p>"},{"location":"types/#llm-types","title":"LLM Types","text":"Type Description api Cloud-based API models local Local model execution server Self-hosted server models"},{"location":"types/#usage-example_2","title":"Usage Example","text":"<pre><code>from protolink.types import LLMType\n\n# Different LLM deployment types\napi_llm = OpenAILLM(model=\"gpt-4\")  # API type\nlocal_llm = LocalLLM(model_path=\"./model.gguf\")  # Local type\nserver_llm = ServerLLM(endpoint=\"http://localhost:8080\")  # Server type\n</code></pre>"},{"location":"types/#mimetype","title":"MimeType","text":"<p>Type alias for supported MIME types in Protolink. These are used to specify the input and output formats that agents can handle.</p>"},{"location":"types/#supported-types","title":"Supported Types","text":"Category MIME Types Text <code>text/plain</code>, <code>text/markdown</code>, <code>text/html</code> Structured Data <code>application/json</code> Images <code>image/png</code>, <code>image/jpeg</code>, <code>image/webp</code> Audio <code>audio/wav</code>, <code>audio/mpeg</code>, <code>audio/ogg</code> Video <code>video/mp4</code>, <code>video/webm</code> Documents <code>application/pdf</code>"},{"location":"types/#usage-example_3","title":"Usage Example","text":"<pre><code>from protolink.types import MimeType\nfrom protolink.models import AgentCard\n\n# Specify supported formats in AgentCard\ncard = AgentCard(\n    name=\"multimedia-agent\",\n    description=\"Agent that handles various media formats\",\n    url=\"http://localhost:8000\",\n    input_formats=[\"text/plain\", \"application/json\", \"image/png\"],\n    output_formats=[\"text/plain\", \"application/json\", \"image/jpeg\"]\n)\n</code></pre>"},{"location":"types/#roletype","title":"RoleType","text":"<pre><code>RoleType: TypeAlias = Literal[\"user\", \"agent\", \"system\"]\n</code></pre> <p>Type alias for supported message roles in agent communication.</p>"},{"location":"types/#message-roles","title":"Message Roles","text":"Role Description user Human user messages agent Agent responses system System instructions"},{"location":"types/#usage-example_4","title":"Usage Example","text":"<pre><code>from protolink.types import RoleType\nfrom protolink.models import Message\n\n# Create messages with different roles\nuser_msg = Message(role=\"user\", content=\"Hello, how are you?\")\nagent_msg = Message(role=\"agent\", content=\"I'm doing well, thank you!\")\nsystem_msg = Message(role=\"system\", content=\"You are a helpful assistant.\")\n</code></pre>"},{"location":"types/#securityschemetype","title":"SecuritySchemeType","text":"<p>Type alias for supported security schemes in Protolink. These are used to specify the authentication methods that agents can use.</p>"},{"location":"types/#supported-schemes","title":"Supported Schemes","text":"Category Security Schemes API key <code>apiKey</code> HTTP (bearer/basic/digest) <code>http</code> full OAuth OAuth2 <code>oauth2</code> Certificates <code>mutualTLS</code> OIDC auto-discovery <code>openIdConnect</code>"},{"location":"types/#usage-example_5","title":"Usage Example","text":"<pre><code>from protolink.types import SecuritySchemeType\nfrom protolink.models import AgentCard\n\n# Define security schemes in AgentCard\ncard = AgentCard(\n    name=\"secure-agent\",\n    description=\"Agent with multiple auth methods\",\n    url=\"http://localhost:8000\",\n    security_schemes={\n        \"bearer\": {\"type\": \"http\", \"description\": \"Bearer token authentication\"},\n        \"api_key\": {\"type\": \"apiKey\", \"description\": \"API key authentication\"},\n        \"oauth2\": {\"type\": \"oauth2\", \"description\": \"OAuth 2.0 authentication\"}\n    }\n)\n</code></pre>"},{"location":"types/#benefits-of-type-aliases","title":"Benefits of Type Aliases","text":"<p>Using type aliases in Protolink provides several advantages:</p>"},{"location":"types/#1-type-safety","title":"1. Type Safety","text":"<pre><code># Compiler catches invalid values\nbackend: BackendType = \"invalid\"  # Type error!\n</code></pre>"},{"location":"types/#2-ide-support","title":"2. IDE Support","text":"<pre><code># Autocomplete shows valid options\ntransport = HTTPAgentTransport(backend=\"\")  # IDE shows: \"starlette\" | \"fastapi\"\n</code></pre>"},{"location":"types/#3-documentation","title":"3. Documentation","text":"<pre><code># Clear intent in function signatures\ndef create_llm(provider: LLMProvider, model: str) -&gt; LLM:\n    # Implementation\n</code></pre>"},{"location":"types/#4-refactoring","title":"4. Refactoring","text":"<pre><code># Easy to update across the entire codebase\nBackendType = Literal[\"starlette\", \"fastapi\", \"new_backend\"]\n</code></pre>"},{"location":"types/#5-consistency","title":"5. Consistency","text":"<pre><code># Same type used across multiple modules\nfrom protolink.types import MimeType\nfrom protolink.models import AgentCard\nfrom protolink.transport import HTTPAgentTransport\n</code></pre>"},{"location":"types/#importing-types","title":"Importing Types","text":"<p>All type aliases are available from the central types module:</p> <pre><code># Import individual types\nfrom protolink.types import MimeType, SecuritySchemeType, RoleType\n\n# Import all types\nfrom protolink.types import BackendType, LLMProvider, LLMType\n</code></pre> <p>The types module is organized to provide a single source of truth for all shared type definitions in the Protolink framework, making the codebase more maintainable and easier to understand.</p>"}]}