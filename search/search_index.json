{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Protolink","text":"<p>A lightweight, production-ready framework for agent-to-agent communication, built on and extending Google's A2A protocol.</p> <p>Get Started View on GitHub</p> <p>Welcome to the Protolink documentation.</p> <p>This site provides an overview of the framework, its concepts, and how to use it in your projects.</p> <p>Current release: see protolink on PyPI. </p>"},{"location":"#contents","title":"Contents","text":"<ul> <li>Getting Started</li> <li>Agents</li> <li>Transports</li> <li>LLMs</li> <li>Tools</li> <li>Examples</li> </ul>"},{"location":"#what-you-can-do-with-protolink","title":"What you can do with Protolink","text":"<ul> <li> <p>Build agents quickly   See Getting Started and Agents for the core concepts and basic setup.</p> </li> <li> <p>Choose your transport   Explore Transports to switch between HTTP, WebSocket, runtime, and future transports with minimal code changes.</p> </li> <li> <p>Plug in LLMs &amp; tools   Use LLMs and Tools to wire in language models and both native &amp; MCP tools as agent modules.</p> </li> </ul>"},{"location":"#overview","title":"Overview","text":"<p>Protolink is a lightweight, production\u2011ready framework for agent\u2011to\u2011agent communication, built around and extending Google\u2019s A2A protocol.</p> <p>Key ideas:</p> <ul> <li>Unified Agent model: a single <code>Agent</code> instance handles both client and server responsibilities.</li> <li>Flexible transports: HTTP, WebSocket, in\u2011process runtime, and planned JSON\u2011RPC / gRPC transports.</li> <li>LLM\u2011ready architecture: first\u2011class integration with API, local, and server\u2011hosted LLMs.</li> <li>Tools as modules: native Python tools and MCP tools plugged directly into agents.</li> </ul> <p>Use this documentation to:</p> <ul> <li>Install Protolink and run your first agent.</li> <li>Understand how agents, transports, LLMs, and tools fit together.</li> <li>Explore practical examples you can adapt to your own systems.</li> </ul> <p>Protolink is open source under the MIT license. Contributions are welcome \u2013 see the repository\u2019s Contributing section on GitHub.</p>"},{"location":"agents/","title":"Agents","text":"<p>Agents are the core building blocks in Protolink.</p>"},{"location":"agents/#concepts","title":"Concepts","text":"<p>An Agent in Protolink is a unified component that can act as both client and server. This is different from the original A2A spec, which separates client and server concerns.</p> <p>High\u2011level ideas:</p> <ul> <li>Unified model: a single <code>Agent</code> instance can send and receive messages.</li> <li>AgentCard: a small model describing the agent (name, description, metadata).</li> <li>Modules:</li> <li>LLMs (e.g. <code>OpenAILLM</code>, <code>AnthropicLLM</code>, <code>LlamaCPPLLM</code>, <code>OllamaLLM</code>).</li> <li>Tools (native Python functions or MCP\u2011backed tools).</li> <li>Transport abstraction: agents communicate over transports such as HTTP, WebSocket, gRPC, or the in\u2011process runtime transport.</li> </ul>"},{"location":"agents/#creating-an-agent","title":"Creating an Agent","text":"<p>A minimal agent consists of three pieces:</p> <ol> <li>An <code>AgentCard</code> describing the agent.</li> <li>A <code>Transport</code> implementation.</li> <li>An optional LLM and tools.</li> </ol> <p>Example:</p> <pre><code>from protolink.agents import Agent\nfrom protolink.models import AgentCard\nfrom protolink.transport import HTTPTransport\nfrom protolink.llms.api import OpenAILLM\n\n\nagent_card = AgentCard(\n    name=\"example_agent\",\n    description=\"A dummy agent\",\n)\n\ntransport = HTTPTransport()\nllm = OpenAILLM(model=\"gpt-5.1\")\n\nagent = Agent(agent_card, transport, llm)\n</code></pre> <p>You can then attach tools and start the agent.</p>"},{"location":"agents/#agent-to-agent-communication","title":"Agent-to-Agent Communication","text":"<p>Agents communicate over a chosen transport.</p> <p>Common patterns:</p> <ul> <li>RuntimeTransport: multiple agents in the same process share an in\u2011memory transport, which is ideal for local testing and composition.</li> <li>HTTPTransport / WebSocketTransport: agents expose HTTP or WebSocket endpoints so that other agents (or external clients) can send requests.</li> <li>gRPC / JSON\u2011RPC (planned): additional transports for more structured or high\u2011performance communication.</li> </ul> <p>From the framework\u2019s perspective, all of these are implementations of the same transport interface, so you can swap them with minimal code changes.</p>"},{"location":"examples/","title":"Examples","text":"<p>This section links to example projects and code snippets in the repository.</p>"},{"location":"examples/#http-agents","title":"HTTP Agents","text":"<p>The repository includes several examples under the <code>examples/</code> directory. For HTTP\u2011based agents:</p> <ul> <li><code>examples/http_agents.py</code> \u2014 basic HTTP transport example showing how to spin up an HTTP\u2011enabled agent.</li> <li><code>examples/http_math_agents.py</code> \u2014 example of delegating between agents over HTTP (e.g. a question agent calling a math agent).</li> </ul>"},{"location":"examples/#other-examples","title":"Other Examples","text":"<p>Additional examples illustrate other capabilities:</p> <ul> <li><code>examples/basic_agent.py</code> \u2014 minimal agent setup focused on core concepts.</li> <li><code>examples/llms.py</code> \u2014 examples of wiring different LLM backends into agents.</li> <li><code>examples/runtime_agents.py</code> \u2014 demonstrates using <code>RuntimeTransport</code> for in\u2011process agent communication.</li> <li><code>examples/streaming_agent.py</code> \u2014 shows streaming behaviour (e.g. via WebSocket or other streaming\u2011capable transports).</li> <li><code>examples/oauth_agent.py</code> \u2014 demonstrates OAuth 2.0 and API\u2011key based security in front of agents.</li> </ul> <p>You can run and adapt these scripts as starting points for your own agent systems.</p> <p>New here?</p> <p>Start with <code>examples/basic_agent.py</code> to understand the core concepts, then move on to <code>examples/http_agents.py</code> for HTTP-based setups.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide shows how to install and start using Protolink.</p>"},{"location":"getting-started/#installation","title":"Installation","text":"<p>Protolink is published on PyPI and can be installed with either <code>uv</code> (recommended) or <code>pip</code>.</p>"},{"location":"getting-started/#basic-installation","title":"Basic Installation","text":"<p>This installs the base package without optional extras:</p> <pre><code># Using uv (recommended)\nuv add protolink\n\n# Using pip\npip install protolink\n</code></pre>"},{"location":"getting-started/#optional-dependencies","title":"Optional Dependencies","text":"<p>Protolink exposes several extras to enable additional functionality:</p> <pre><code># Install with all optional dependencies\nuv add \"protolink[all]\"\n\n# HTTP support (for web-based agents)\nuv add \"protolink[http]\"\n\n# All supported LLM libraries\nuv add \"protolink[llms]\"\n\n# Development (all extras + testing tools)\nuv add \"protolink[dev]\"\n</code></pre> <p>Optional extras</p> <p>You usually only need the extras that match your use case. For example, <code>protolink[http]</code> for web transports or <code>protolink[llms]</code> when integrating LLM backends.</p> <p>For development from source:</p> <pre><code>git clone https://github.com/nmaroulis/protolink.git\ncd protolink\nuv pip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/#basic-example","title":"Basic Example","text":"<p>Below is a minimal example that wires together an agent, HTTP transport, an OpenAI LLM, and both native and MCP tools:</p> <pre><code>from protolink.agents import Agent\nfrom protolink.models import AgentCard\nfrom protolink.transport import HTTPTransport\nfrom protolink.tools import MCPToolAdapter\nfrom protolink.llms.api import OpenAILLM\n\n\n# Define the agent card\nagent_card = AgentCard(\n    name=\"example_agent\",\n    description=\"A dummy agent\",\n)\n\n\n# Initialize the transport\ntransport = HTTPTransport()\n\n\n# OpenAI API LLM\nllm = OpenAILLM(model=\"gpt-5.1\")\n\n\n# Initialize the agent\nagent = Agent(agent_card, transport, llm)\n\n\n# Add Native tool\n@agent.tool(name=\"add\", description=\"Add two numbers\")\nasync def add_numbers(a: int, b: int):\n    return a + b\n\n\n# Add MCP tool\nmcp_tool = MCPToolAdapter(mcp_client, \"multiply\")\nagent.add_tool(mcp_tool)\n\n\n# Start the agent\nagent.start()\n</code></pre> <p>This example demonstrates the core pieces of Protolink:</p> <ul> <li>AgentCard to describe the agent.</li> <li>Transport (here <code>HTTPTransport</code>) to handle communication.</li> <li>LLM backend (<code>OpenAILLM</code>).</li> <li>Native tools (Python functions decorated with <code>@agent.tool</code>).</li> <li>MCP tools registered via <code>MCPToolAdapter</code>.</li> </ul>"},{"location":"llms/","title":"LLMs","text":"<p>Protolink integrates with various LLM backends.</p>"},{"location":"llms/#llm-types","title":"LLM Types","text":"<p>Protolink groups LLM backends into three broad categories:</p> <ul> <li>API \u2014 calls a remote API and requires an API key:</li> <li><code>OpenAILLM</code>: uses the OpenAI API for sync &amp; async requests.</li> <li> <p><code>AnthropicLLM</code>: uses the Anthropic API for sync &amp; async requests.</p> </li> <li> <p>Local \u2014 runs the model directly in your runtime:</p> </li> <li> <p><code>LlamaCPPLLM</code>: uses a local llama.cpp runtime for sync &amp; async requests.</p> </li> <li> <p>Server \u2014 connects to an LLM server, locally or remotely:</p> </li> <li><code>OllamaLLM</code>: connects to an Ollama server for sync &amp; async requests.</li> </ul> <p>You can also use other LLM clients directly without going through Protolink\u2019s <code>LLM</code> wrappers if you prefer.</p>"},{"location":"llms/#configuration","title":"Configuration","text":"<p>Configuration depends on the specific backend, but the general pattern is:</p> <ol> <li>Install the relevant extras (from the README):</li> </ol> <pre><code># All supported LLM backends\nuv add \"protolink[llms]\"\n</code></pre> <p>!!! info \"Choosing LLM extras\"        If you only need a subset of backends, you can install more targeted extras once they are exposed (for example, only OpenAI or only local backends).</p> <ol> <li>Instantiate the LLM with the desired model and credentials:</li> </ol> <pre><code>from protolink.llms.api import OpenAILLM\n\n\nllm = OpenAILLM(\n    model=\"gpt-5.1\",\n    # api_key is typically read from the environment, e.g. OPENAI_API_KEY\n)\n</code></pre> <p>!!! warning \"API keys\"        Never commit API keys to version control. Read them from environment variables or a secure secrets manager.</p> <ol> <li>Pass the LLM to your Agent:</li> </ol> <pre><code>from protolink.agents import Agent\nfrom protolink.models import AgentCard\nfrom protolink.transport import HTTPTransport\n\n\nagent_card = AgentCard(name=\"llm_agent\", description=\"Agent backed by an LLM\")\ntransport = HTTPTransport()\n\nagent = Agent(agent_card, transport, llm)\n</code></pre> <p>For local and server\u2011style LLMs (<code>LlamaCPPLLM</code>, <code>OllamaLLM</code>), configuration additionally includes paths to model files or server URLs. Refer to the corresponding example scripts in <code>examples/llms.py</code> for concrete usage patterns.</p>"},{"location":"tools/","title":"Tools","text":"<p>Tools extend agent capabilities with additional functions.</p>"},{"location":"tools/#native-tools","title":"Native Tools","text":"<p>Native tools are regular Python callables that you register on an agent. They are exposed over the transport so that other agents (or clients) can invoke them.</p> <p>To register a native tool, decorate an async function with <code>@agent.tool</code>:</p> <pre><code>from protolink.agents import Agent\n\n\n@agent.tool(name=\"add\", description=\"Add two numbers\")\nasync def add_numbers(a: int, b: int) -&gt; int:\n    return a + b\n</code></pre> <p>Native tools are ideal for business logic, data access, or any capability you want your agents to expose.</p>"},{"location":"tools/#mcp-tools","title":"MCP Tools","text":"<p>Protolink can also expose tools from an MCP (Model Context Protocol) server using the <code>MCPToolAdapter</code>.</p> <p>High\u2011level flow:</p> <ol> <li>Connect to an MCP server using an MCP client (not shown here).</li> <li>Wrap an MCP tool with <code>MCPToolAdapter</code>.</li> <li>Register it on the agent.</li> </ol> <p>Example pattern:</p> <pre><code>from protolink.tools import MCPToolAdapter\n\n\nmcp_tool = MCPToolAdapter(mcp_client, \"multiply\")\nagent.add_tool(mcp_tool)\n</code></pre> <p>The MCP adapter lets you reuse existing MCP tools as if they were native tools, keeping a consistent interface on the agent side.</p>"},{"location":"transports/","title":"Transports","text":"<p>Protolink supports multiple transports for agent communication.</p>"},{"location":"transports/#supported-transports","title":"Supported Transports","text":"<ul> <li>HTTPTransport</li> <li>Uses HTTP/HTTPS for synchronous requests.</li> <li>Backed by ASGI frameworks such as <code>starlette</code>/<code>httpx</code>/<code>uvicorn</code> or <code>fastapi</code>/<code>pydantic</code>/<code>uvicorn</code>.</li> <li> <p>Good default choice for web\u2011based agents and simple deployments.</p> </li> <li> <p>WebSocketTransport</p> </li> <li>Uses WebSocket for streaming requests and responses.</li> <li>Built on top of libraries like <code>websockets</code> (and <code>httpx</code> for HTTP parts where applicable).</li> <li> <p>Useful when you need real\u2011time, bidirectional communication or token\u2011level streaming.</p> </li> <li> <p>JSONRPCTransport (TBD)</p> </li> <li>Planned JSON\u2011RPC based transport.</li> <li> <p>Intended for structured, RPC\u2011style interactions.</p> </li> <li> <p>GRPCTransport (TBD)</p> </li> <li>Planned gRPC transport.</li> <li> <p>Intended for high\u2011performance, strongly\u2011typed communication.</p> </li> <li> <p>RuntimeTransport</p> </li> <li>Simple in\u2011process, in\u2011memory transport.</li> <li>Allows multiple agents to communicate within the same Python process.</li> <li>Ideal for local development, testing, and tightly\u2011coupled agent systems.</li> </ul>"},{"location":"transports/#choosing-a-transport","title":"Choosing a Transport","text":"<p>Some rough guidelines:</p> <ul> <li>Use RuntimeTransport for local experiments, tests, or when all agents live in the same process.</li> <li>Use HTTPTransport when you want a simple, interoperable API surface (e.g. calling agents from other services or frontends).</li> <li>Use WebSocketTransport when you need streaming and interactive sessions.</li> <li>Plan for JSONRPCTransport or GRPCTransport if you need stricter schemas or higher performance across services.</li> </ul>"}]}